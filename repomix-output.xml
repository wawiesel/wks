This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.cursor/
  rules/
    important.mdc
.github/
  workflows/
    test.yml
guides/
  CLAUDE.md
  links-design.md
  patterns-design.md
  related-design.md
scripts/
  deploy_wks0.sh
tests/
  test_cli_basic.py
  test_cli_db_info.py
  test_cli_related.py
  test_config_validator.py
  test_daemon_maintenance.py
  test_dbmeta.py
  test_error_messages.py
  test_extractor.py
  test_mongo_retry.py
  test_mongoctl.py
  test_monitor_controller.py
  test_monitor.py
  test_phase1.py
  test_smoke.py
  test_space_db.py
wks/
  cli/
    commands/
      __init__.py
      config.py
      index.py
      monitor.py
      related.py
      service.py
    __init__.py
    constants.py
    dataclasses.py
    display_strategies.py
    helpers.py
    main.py
  display/
    __init__.py
    base.py
    cli.py
    context.py
    mcp.py
  __init__.py
  __main__.py
  activity.py
  cli_db.py
  cli.py
  cli.py.backup
  config_schema.py
  config_validator.py
  config.py
  constants.py
  daemon.py
  db_helpers.py
  dbmeta.py
  error_messages.py
  extractor.py
  links.py
  logging_config.py
  mcp_server.py
  mongo_retry.py
  mongoctl.py
  monitor_controller.py
  monitor.py
  obsidian.py
  priority.py
  service_controller.py
  similarity.py
  status.py
  templating.py
  uri_utils.py
  utils.py
.gitignore
CHANGELOG.md
CONTRIBUTING.md
README.md
ROADMAP.md
setup.py
SPEC.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Read(//Users/ww5/Desktop/**)",
      "Read(//Users/ww5/Downloads/**)",
      "Read(//Users/ww5/**)",
      "Bash(find ~/Downloads -type f -name '~$*')",
      "Bash(find ~/Downloads -type f -name '~$*' -delete)",
      "Bash(cat ~/.wks/config.json)",
      "Bash(for dir in */)",
      "Bash(find \"$dir\" -type f -print0)",
      "Bash(xargs -0 stat -f \"%m\")",
      "Bash(xargs -I {} date -r {} \"+%Y-%m-%d\")",
      "Bash(done)",
      "Bash(find ~/Desktop -name \"*2025-07-03*\" -delete)",
      "Bash(find ~/Downloads -name \"ChatGPT Image*\" -type f -exec rm {} ;)",
      "Bash(find ~/Downloads -type f ( -name \"IMG-*\" -o -name \"PXL_*\" -o -name \"2025????_*.jpg\" -o -name \"2024????_*.jpg\" ))",
      "Bash(find ~/Downloads -type f ( -name \"IMG-*\" -o -name \"PXL_*\" -o -name \"2025????_*.jpg\" -o -name \"2024????_*.jpg\" ) -exec mv {} ~/Documents/2025-Trip-Receipts/ ;)",
      "Bash(find ~/Downloads/\"NSED ModSim Strategic Plan\" -type f -name \"*.pptx\" -exec ls -lt {} +)",
      "Bash(find \"/Users/ww5/Downloads/NSED ModSim Strategic Plan\" -type f -name \"*.pptx\" -exec ls -lt {} +)",
      "Bash(find ~/Downloads -maxdepth 1 -name \"1-s2.0-*.pdf\" -type f -exec mv {} ~/Documents/2025-Research-Papers/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*NRC*.pdf\" -o -name \"*SOW*.pdf\" -o -name \"*Agenda*.pdf\" -o -name \"00_*.pdf\" -o -name \"*TPR*.pdf\" ) -exec mv {} ~/Documents/2025-NRC-Documents/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*insurance*.pdf\" -o -name \"*Recommendation*.pdf\" -o -name \"*Radaideh*.pdf\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"2025-*.pdf\" -o -name \"2024-*.pdf\" ) -exec mv {} ~/Documents/2025-Technical-Reports/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*SCALE*.pdf\" -o -name \"*scale*.pdf\" -o -name \"*ORIGEN*.pdf\" -o -name \"*Polaris*.pdf\" ) -exec mv {} ~/Documents/2025-SCALE-Documentation/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*DNCSH*.pdf\" -o -name \"*SPARC*.pdf\" -o -name \"*HALEU*.pdf\" -o -name \"*Charter*.pdf\" -o -name \"31310025S0003*.pdf\" ) -exec mv {} ~/Documents/2025-Program-Documents/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*precursor*.pdf\" -o -name \"*TSL*.pdf\" -o -name \"Eu152.pdf\" -o -name \"regra*.pdf\" -o -name \"*trans_v*.pdf\" -o -name \"*epjconf*.pdf\" -o -name \"dn_*.pdf\" ) -exec mv {} ~/Documents/2025-Nuclear-Data-Papers/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*paper.pdf\" -o -name \"*PARFUME*.pdf\" -o -name \"*THETA*.pdf\" -o -name \"*Best_Practices*.pdf\" -o -name \"NCSD*.pdf\" -o -name \"*WIESELQUIST*.pdf\" -o -name \"6.3-*.pdf\" ) -exec mv {} ~/Documents/2025-Misc-PDFs/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"myAir*.pdf\" -o -name \"*Ltr_Report*.pdf\" -o -name \"slides.pdf\" -o -name \"today.pdf\" -o -name \"snip-*.pdf\" -o -name \"*uuid*.pdf\" -o -name \"Re-*.pdf\" ) -exec mv {} ~/Documents/2025-Misc-PDFs/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"Confirmation*.pdf\" -o -name \"*Registration*.pdf\" -o -name \"*WANDA*.pdf\" -o -name \"*M&C*.pdf\" ) -exec mv {} ~/Documents/2025-Conference-Registrations/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*Benchmark*.pdf\" -o -name \"*AGR*.pdf\" -o -name \"*Criticality*.pdf\" ) -exec mv {} ~/Documents/2025-Benchmark-Documents/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"31310019N0008*.pdf\" -exec mv {} ~/Documents/2025-Monthly-Reports/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*Holcomb*.pdf\" -o -name \"*CV*.pdf\" -o -name \"*_Form*.pdf\" -o -name \"*Screening*.pdf\" -o -name \"*conflict*.pdf\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"ANE_*.pdf\" -o -name \"*SLICE*.pdf\" -o -name \"Appendix*.pdf\" -o -name \"*Compendium*.pdf\" -o -name \"202504_mc*.pdf\" -o -name \"2509.*.pdf\" ) -exec mv {} ~/Documents/2025-Research-Papers/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*uuid*.pdf\" -o -name \"*[0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f]*.pdf\" -o -name \"001DD*.pdf\" ) -exec mv {} ~/Documents/2025-Misc-PDFs/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -name \"*.pdf\" -type f -exec mv {} ~/Documents/2025-Misc-PDFs/ ;)",
      "Bash(awk '{print $6, $7, $8, $9}')",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"polaris*\" -o -name \"triton*\" ) -exec mv {} ~/2025-polaris-triton-depletion-grid-study/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"origen*\" -exec mv {} ~/2025-origami-test/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*TSUNAMI*\" -o -name \"*godiva*\" -o -name \"CE-*\" -o -name \"*sampler*\" ) -exec mv {} ~/2025-SCALE-SQA/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"csas*\" -o -name \"*shift*\" -o -name \"burst*\" -o -name \"dodec*\" ) -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"PWR_*\" -o -name \"ge14*\" -o -name \"full_eq*\" ) -exec mv {} ~/2025-NRC/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*msre*\" -o -name \"slice*\" -o -name \"SFR_*\" -o -name \"atr-*\" -o -name \"CER_*\" -o -name \"Facility*\" ) -exec mv {} ~/2025-DNCSH/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"lu177*\" -o -name \"u238*\" -o -name \"decayp*\" ) -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 ( -name \"*.inp\" -o -name \"*.out\" -o -name \"*.plt\" -o -name \"*.t16\" ) -type f -exec mv {} ~/2025-SCALE/scratch-tests/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"core*.png\" -o -name \"flux*.png\" -o -name \"power.jpg\" -o -name \"*triso*.png\" -o -name \"out-*.png\" -o -name \"*weak-scaling*.png\" -o -name \"*weak-scaling*.jpeg\" -o -name \"gFHR*.png\" ) -exec mv {} ~/2025-SCALE/figures/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -name \"*.docx\" -type f -exec basename {})",
      "Bash(awk '{print $9}')",
      "Bash(source venv/bin/activate)",
      "Bash(wks sim stats)",
      "Bash(wks sim route --path ~/Downloads/2025-Q3-PMPDNCSH_rev30.docx --mode chunk --top 30 --max-targets 5 --json)",
      "Bash(wks sim route --path ~/Downloads/\"31310025S0003 FEB25.docx\" --mode chunk --top 30 --max-targets 5 --json)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*PMPDNCSH*\" -o -name \"1.03.01.02*\" -o -name \"1.03.1.02*\" -o -name \"DNCSH*\" -o -name \"*HST*\" -o -name \"plan-hst-dncsh.docx\" ) -exec mv {} ~/2025-DNCSH/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"31310*\" -o -name \"1886-Z720-24*\" -o -name \"Agenda_NRC*\" -o -name \"*_pwr_*\" -o -name \"*NUREG*\" -o -name \"*Nuclide*Ranking*\" -o -name \"PWP_FY23*\" -o -name \"*NRC*SOW*\" -o -name \"*SCALE*Dev*SOW*\" -o -name \"NRCNon-LWR*\" ) -exec mv {} ~/2025-NRC/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*SCALE*QAP*\" -o -name \"*SCALE*Validation*\" -o -name \"*Harmon*\" -o -name \"*SQAB*\" -o -name \"Industry-Standard*\" -o -name \"FFESD-PRO*\" -o -name \"*Polaris*Rom*\" ) -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*Recommendation*\" -o -name \"*-Holcomb*\" -o -name \"Performance*Eval*\" -o -name \"Wieselquist_FY26*\" -o -name \"Records*FAQ*\" -o -name \"Trip_Report.docx\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*Criticality*Benchmark*\" -o -name \"*HALEU*Benchmark*\" -o -name \"LEU-COMP-THERM*\" -o -name \"*Increased*Enrichment*\" -o -name \"*TRISO_Production*\" -o -name \"NE41_Nuclear*\" ) -exec mv {} ~/Documents/2025-Benchmark-Documents/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*Tracking*Every*Pebble*\" -o -name \"Report 4_ww5.docx\" -o -name \"*Polaris-PARCS*\" -o -name \"*graphite*elements*IPENMB*\" -o -name \"*NonLWR_FuelCycleImpact*\" ) -exec mv {} ~/Documents/2025-Technical-Reports/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*WANDA*\" -o -name \"*ATF-HE-HBU*Workshop*\" -o -name \"SPARC*Review*Agenda*\" ) -exec mv {} ~/Documents/2025-Workshop_Slides_Collection/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"EAW2_Communication*\" -o -name \"*NCERC*fuel*\" -o -name \"*Deimos*THETA*\" ) -exec mv {} ~/2025-DNCSH/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*ORIGEN*\" -o -name \"*Fuel*Cycle*Estimator*\" -o -name \"SCALE6.3_physics*\" -o -name \"1d_vs_2d.docx\" -o -name \"CRITGUIDE*\" ) -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*NSC*bodies*\" -o -name \"SG-8_NEA*\" -o -name \"NCSD_2025*\" ) -exec mv {} ~/Documents/2025-Committee-Work/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -name \"*.docx\" -type f -exec basename {} ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*Volume2_NCS_631*\" -o -name \"Draft_Volume2_NCS*\" ) -exec mv {} ~/Documents/2025-NCS-Standards/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"FY25_Q3_NCSP*\" -o -name \"FY25_Activities*\" -o -name \"SOW_QC_2025*\" -o -name \"NEFCD*MasterWorkScopes*\" ) -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"Plan_Task3.2*\" -o -name \"Task2_SU-Guidance*\" -o -name \"*Non-LWR*Development*Plan*\" ) -exec mv {} ~/2025-NRC/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"*MOU-IPEN-ORNL*\" -exec mv {} ~/Documents/2025-International-Collaboration/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*TRISO*Next*Talk*\" -o -name \"2024-hpr_inl.docx\" -o -name \"2025-Lang_TP4.docx\" ) -exec mv {} ~/2025-DNCSH/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -name \"*.docx\" -type f -exec mv {} ~/Documents/2025-Misc-Documents/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -name \"*.xlsx\" -type f -exec basename {})",
      "Bash(wks sim route --path ~/Downloads/2025-nrc-budgets.xlsx --mode chunk --top 30 --max-targets 5 --json)",
      "Bash(python3 -c \"\nimport openpyxl\nimport sys\n\ntry:\n    wb = openpyxl.load_workbook(''$HOME/Downloads/LA Updates.xlsx'', data_only=True)\n    print(f''Sheets: {wb.sheetnames}'')\n    for sheet_name in wb.sheetnames[:3]:  # First 3 sheets\n        ws = wb[sheet_name]\n        print(f''\\n=== Sheet: {sheet_name} ==='')\n        # Print first few rows\n        for i, row in enumerate(ws.iter_rows(max_row=5, values_only=True)):\n            print(row)\n            if i >= 4:\n                break\nexcept Exception as e:\n    print(f''Error: {e}'', file=sys.stderr)\n\")",
      "Bash(wks sim query --path ~/Downloads/\"LA Updates.xlsx\" --mode file --top 5 --min 0.3 --json)",
      "Bash(python3 -m json.tool)",
      "Bash(for f in ~/Downloads/*.xlsx ~/Downloads/*.xlsm)",
      "Bash(do if [ -f \"$f\" ])",
      "Bash(find ~/Downloads -maxdepth 1 ( -name \"*.xlsx\" -o -name \"*.xlsm\" ) -type f -exec sh -c 'echo \"\"=== $(basename \"\"$1\"\") ===\"\"; mdls \"\"$1\"\" | grep -E \"\"kMDItemAuthors|kMDItemTitle|kMDItemContentCreation\"\"; echo' _ {} ;)",
      "Bash(awk '{print $5, $9}')",
      "Bash(find ~/Downloads -maxdepth 1 ( -name \"*.xlsx\" -o -name \"*.xlsm\" ) -type f -exec mv {} ~/2025-SCALE/scratch-data/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -name \"*.zip\" -type f -exec ls -lh {} +)",
      "Bash(find ~/Downloads -maxdepth 1 -name \"*.zip\" -type f -exec basename {})",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"DMG_*.zip\" -o -name \"EXE_*.zip\" -o -name \"linux-gcc-bundle-*.zip\" -o -name \"centos7-mpi-bundle-*.zip\" -o -name \"mac-clang-bundle-*.zip\" -o -name \"rpm-centos7-gcc-mpi-*.zip\" -o -name \"TGZ_*.zip\" -o -name \"SCALE-6.3.2-setup*.zip\" -o -name \"linux-speedy-*.zip\" ) -exec mv {} ~/2025-SCALE/installers/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"Manual_*.zip\" -o -name \"SCALE-MANUAL-*.zip\" -o -name \"rst2html5slides-*.zip\" ) -exec mv {} ~/2025-SCALE/docs/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"Unit_*.zip\" -o -name \"artifacts*.zip\" ) -exec mv {} ~/2025-SCALE/builds/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"iTermBrowserPlugin-*.zip\" -o -name \"clock_animation_demo.zip\" ) -exec mv {} ~/Documents/2025-Software-Tools/ ;)",
      "Bash(unzip -l ~/Downloads/2024-reports.zip)",
      "Bash(unzip -l ~/Downloads/ATFDocs.zip)",
      "Bash(unzip -l ~/Downloads/Slides.zip)",
      "Bash(unzip -l ~/Downloads/AI-ASCR-NE.zip)",
      "Bash(unzip -l ~/Downloads/dncb.zip)",
      "Bash(unzip -l ~/Downloads/code.zip)",
      "Bash(unzip -l ~/Downloads/c379e51b-7ee1-4fdd-9613-582121561ea2_Export-bcb95b81-4676-4da1-88ca-b05b8ed8c025.zip)",
      "Bash(unzip -l ~/Documents/2025-Software-Tools/code.zip)",
      "Bash(unzip -l ~/Documents/2025-Software-Tools/clock_animation_demo.zip)",
      "Bash(wks sim route --path ~/Documents/2025-Misc-Documents/\"2025-HALEU_Manufacturing_Benchmak_Gap_Assessment.docx\" --mode chunk --top 20 --max-targets 3 --json)",
      "Bash(find ~/Documents/2025-Misc-Documents -type f -name \"*.docx\" -exec basename {} ;)",
      "Bash(wks sim route --path ~/Documents/2025-Misc-Documents/\"QC_ShortReport_2pages_updated.docx\" --mode chunk --top 20 --max-targets 3 --json)",
      "Bash(wks sim route --path ~/Documents/2025-Misc-Documents/\"publication-list.docx\" --mode chunk --top 20 --max-targets 3 --json)",
      "Bash(python3 -c \"import json, sys; c=json.load(sys.stdin); print(c.get(''''vault_path'''', ''''~/obsidian'''')); print(c.get(''''obsidian'''', {}).get(''''base_dir'''', ''''''''))\")",
      "WebSearch",
      "Bash(find ~/Documents -maxdepth 3 -type d ( -name \"*.docx\" -o -name \"*.pptx\" -o -name \"*.xlsx\" ))",
      "Bash(find ~/Documents -maxdepth 1 -name \"*.pptx\" -type f -exec basename {} ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -name \"*paper.pdf\" -exec mv {} ~/Documents/2025-Research-Papers/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"*flux*\" -o -name \"*power*\" -o -name \"*dens_*\" -o -name \"*core*\" ) -exec mv {} ~/Documents/2025-Technical-Figures/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -exec basename {})",
      "Bash(xargs -I {} sh -c 'echo \"\"\"\"$(basename \"\"\"\"{}\"\"\"\")\"\"\"\"')",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"*.dmg\" -exec basename {} ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"*.dmg\" -exec mv {} ~/Documents/2025-Software-Installers/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*.f71\" -o -name \"*.f33\" -o -name \"*.msg\" -o -name \"*assm*\" -o -name \"*.ps\" ) -exec mv {} ~/2025-SCALE/scratch-tests/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"DALL*\" -exec mv {} ~/Documents/2025-AI-Generated-Images/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*.py\" -o -name \"*.ipynb\" -o -name \"*.h5\" -o -name \"*.npy\" ) -exec mv {} ~/2025-SCALE/scratch-tests/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"*.webp\" -exec mv {} ~/Documents/2025-AI-Generated-Images/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"*.doc\" -exec basename {} ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*Template*.doc\" -o -name \"SQA*.doc\" ) -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"*.xpi\" -exec mv {} ~/Documents/2025-Software-Installers/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*.mp4\" -o -name \"VIDEO*\" ) -exec basename {} ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"Project Charter for HALEU*\" -exec mv {} ~/Documents/2025_07-HALEU_Consortium/_drafts/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"CM Plan*\" -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"Q2+PR*\" -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"*.mp4\" -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"*.tar.gz\" -exec mv {} ~/2025-SCALE/installers/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"SDL-*\" -exec basename {})",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"SDL-*\" -exec mv {} ~/2025-SCALE/bug-tracking/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*.aux\" -o -name \"*.log\" -o -name \".DS_Store\" -o -name \".localized\" ) -exec rm {} ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"*.epub\" -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*endf*\" -o -name \"*Ensdf*\" -o -name \"quadriso*\" -o -name \"*.volumes\" -o -name \"*_compBlock\" ) -exec mv {} ~/2025-SCALE/scratch-tests/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"*.html\" -exec mv {} ~/2025-SCALE/scratch-tests/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*.txt\" -o -name \"*.csv\" -o -name \"*.json\" -o -name \"*.svg\" ) -exec mv {} ~/2025-SCALE/scratch-tests/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -exec basename {} ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"SCALE-*.exe\" -exec mv {} ~/2025-SCALE/installers/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*.tex\" -o -name \"*.bib\" -o -name \"*.rst\" ) -exec mv {} ~/2025-SCALE/scratch-tests/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*.cpp\" -o -name \"*.ts\" -o -name \"*.mermaid\" -o -name \"*.idc\" -o -name \"*.err\" ) -exec mv {} ~/2025-SCALE/scratch-tests/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -name \"Discharge_burnup*\" -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*.ics\" -o -name \"Invoice-*.PDF\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*.pem\" -o -name \"*.atomsvc\" ) -exec mv {} ~/2025-SCALE/scratch-tests/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f ( -name \"*.PNG\" -o -name \"*.xltx\" -o -name \"xf33\" -o -name \"z\" -o -name \"reactor-tree\" -o -name \"NOTE*\" ) -exec mv {} ~/2025-SCALE/scratch-tests/ ;)",
      "Bash(find ~/Downloads -maxdepth 1 -type f -exec ls -lh {} ;)",
      "Bash(wks sim query --path ~/Documents/2025-Misc-PDFs/THETA critical measurement summary.pdf --mode file --top 5 --min 0.3 --json)",
      "Bash(for file in \"Best_Practices_for_Nuclear_Experiment_Data_Preservation_at_INL_draft-010325.pdf\" \"TSUNAMI_Product_Owner_Responsibilities_and_Qualifications.pdf\" \"nvidia-h100-datasheet-2430615.pdf\")",
      "Bash(do echo \"=== $file ===\" wks sim route --path ~/Documents/2025-Misc-PDFs/\"$file\" --mode chunk --top 10 --max-targets 2 --json)",
      "Bash(python3 -c \"import json,sys; data=json.load(sys.stdin); print(data[''''suggestions''''][0][''''target''''] if data[''''suggestions''''] else ''''No match'''')\" done)",
      "Bash(wks sim route --path ~/Documents/2025-Misc-Documents/\"2025-Ideas-AmericanScienceCloud_NP.docx\" --mode chunk --top 20 --max-targets 3 --json)",
      "Bash(wks sim route --path ~/Documents/2025-Misc-Documents/\"2025-options.docx\" --mode chunk --top 20 --max-targets 3 --json)",
      "Bash(wks sim route --path ~/Documents/2025-Misc-Documents/\"Nuclear Energy Renaissance 5.8.docx\" --mode chunk --top 20 --max-targets 3 --json)",
      "Bash(python3 -c \"\nfrom docx import Document\nimport sys\n\ndoc = Document(''/Users/ww5/Documents/2025-Misc-Documents/2025-Ideas-AmericanScienceCloud_NP.docx'')\ntext = ''\\n''.join([para.text for para in doc.paragraphs[:20]])\nprint(text[:1000])\n\")",
      "Bash(python3 -c \"\nfrom docx import Document\nimport sys\n\ndoc = Document(''/Users/ww5/Documents/2025-Misc-Documents/2025-options.docx'')\ntext = ''\\n''.join([para.text for para in doc.paragraphs[:20]])\nprint(text[:1000])\n\")",
      "Bash(python3 -c \"\nfrom docx import Document\nimport sys\n\ndoc = Document(''/Users/ww5/Documents/2025-Misc-Documents/Nuclear Energy Renaissance 5.8.docx'')\ntext = ''\\n''.join([para.text for para in doc.paragraphs[:20]])\nprint(text[:1000])\n\")",
      "Bash(find ~/Documents/2025-Misc-Documents -type f -name \"*.docx\" -exec ls -lh {} ;)",
      "Bash(python3 -c \"\nfrom docx import Document\n\ndoc = Document(''/Users/ww5/Documents/2025-Misc-Documents/Wills_Walkthrough_rev1.docx'')\ntext = ''\\n''.join([para.text for para in doc.paragraphs[:20]])\nprint(text[:1000])\n\")",
      "Bash(python3 -c \"\nfrom docx import Document\n\ndoc = Document(''/Users/ww5/Documents/2025-Misc-Documents/response.docx'')\ntext = ''\\n''.join([para.text for para in doc.paragraphs[:20]])\nprint(text[:1000])\n\")",
      "Bash(find ~/Documents/2025-Misc-Documents -type f ( -name \"*AmericanScienceCloud*\" -o -name \"Nuclear Energy Renaissance*\" ) -exec mv {} ~/Documents/2025-Ideas/ ;)",
      "Bash(find ~/Documents/2025-Misc-Documents -type f -name \"2025-options.docx\" -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Documents/2025-Misc-Documents -type f -name \"Wills_Walkthrough*.docx\" -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Documents/2025-Misc-Documents -type f ( -name \"response.docx\" -o -name \"*absorption reaction rate*\" ) -exec mv {} ~/Documents/2025-Technical-Reports/ ;)",
      "Bash(find ~/Documents/2025-Misc-Documents -type f -name \"ORNL_AGENDA*.docx\" -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(python3 -c \"\nfrom docx import Document\n\ndoc = Document(''/Users/ww5/Documents/2025-Misc-Documents/LPCF-business-plan-draft 2 1-080613-rev(gff comments).docx'')\ntext = ''\\n''.join([para.text for para in doc.paragraphs[:20]])\nprint(text[:1000])\n\")",
      "Bash(python3 -c \"\nfrom docx import Document\n\ndoc = Document(''/Users/ww5/Documents/2025-Misc-Documents/TemplateDRDCP.docx'')\ntext = ''\\n''.join([para.text for para in doc.paragraphs[:10]])\nprint(text[:500])\n\")",
      "Bash(find ~/Documents/2025-Misc-Documents -type f -name \"LPCF-business-plan*.docx\" -exec mv {} ~/_old/2013/ ;)",
      "Bash(find ~/Documents/2025-Misc-Documents -type f -name \"TemplateDRDCP*.docx\" -exec mv {} ~/Documents/2025-Templates/ ;)",
      "Bash(find ~/Documents/2025-Misc-Documents -type f -exec basename {} ;)",
      "Bash(python3 -c \"\nimport openpyxl\nwb = openpyxl.load_workbook(''/Users/ww5/Documents/2025-Misc-Documents/report.xlsx'', data_only=True)\nprint(f''Sheets: {wb.sheetnames}'')\nws = wb.active\nfor i, row in enumerate(ws.iter_rows(max_row=10, values_only=True)):\n    print(row)\n    if i >= 4:\n        break\n\")",
      "Bash(find ~/Documents/2025-Misc-Documents -type f -name \"report*.xlsx\" -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -type f -name \"*.pdf\" -exec basename {})",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"*paper.pdf\" -o -name \"*Delayed_Neutron*.pdf\" -o -name \"jne-*.pdf\" -o -name \"*Canadian*.pdf\" -o -name \"sustainability-*.pdf\" -o -name \"PhD_*.pdf\" -o -name \"let_*.pdf\" ) -exec mv {} ~/Documents/2025-Research-Papers/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"*THETA*.pdf\" -o -name \"*Best_Practices*.pdf\" -o -name \"ORNL-479*.pdf\" -o -name \"*CritFacility*.pdf\" ) -exec mv {} ~/Documents/2025-Benchmark-Documents/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"NFCSC*Monthly*.pdf\" -o -name \"*Ltr_Report*.pdf\" -o -name \"MilestoneStatus.pdf\" -o -name \"wanda_*.pdf\" -o -name \"ES Main Report.pdf\" ) -exec mv {} ~/Documents/2025-Technical-Reports/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"myAir-*.pdf\" -o -name \"slack_invoice*.pdf\" -o -name \"WIESELQUIST_*.pdf\" -o -name \"FOLIO*.pdf\" -o -name \"waw_cv*.pdf\" -o -name \"ORNL-839*.pdf\" -o -name \"signed_page.pdf\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"today.pdf\" -o -name \"my.pdf\" -o -name \"slides.pdf\" -o -name \"snip-*.pdf\" -o -name \"*uuid*.pdf\" -o -name \"*[0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f]-*.pdf\" -o -name \"001DD*.pdf\" -o -name \"output*.pdf\" -o -name \"Re-*.pdf\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"NUREG*.pdf\" -o -name \"Task*SU-Guidance*.pdf\" -o -name \"ML*.pdf\" -o -name \"Proposal_*.pdf\" -o -name \"*Westinghouse*.pdf\" -o -name \"Y-12 LEU*.pdf\" -o -name \"*Technical Evaluation*.pdf\" ) -exec mv {} ~/2025-NRC/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"*vera-plan*.pdf\" -o -name \"*racklife*.pdf\" -o -name \"*Quadratic Depletion*.pdf\" -o -name \"orange-g4*.pdf\" -o -name \"qc_ICAPP*.pdf\" -o -name \"*thermal_scattering*.pdf\" -o -name \"Pub*.pdf\" ) -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"*EAW*.pdf\" -o -name \"LANL_Fuel*.pdf\" -o -name \"*IPEN*.pdf\" -o -name \"HMKP-*.pdf\" -o -name \"UVS2D1M2*.pdf\" -o -name \"SPC-*.pdf\" -o -name \"3_Peter*.pdf\" ) -exec mv {} ~/2025-DNCSH/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"TTD-*.pdf\" -o -name \"*SBMS*.pdf\" -o -name \"*Safety and General Software*.pdf\" -o -name \"ORNL Band Template*.pdf\" -o -name \"*EDRM*.pdf\" ) -exec mv {} ~/Documents/2025-Templates/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"*nvidia*.pdf\" -o -name \"*l40s*.pdf\" -o -name \"*m3-ultra*.pdf\" -o -name \"*datasheet*.pdf\" ) -exec mv {} ~/Documents/2025-Software-Installers/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"*nvidia*.pdf\" -o -name \"*l40s*.pdf\" -o -name \"*m3-ultra*.pdf\" -o -name \"*datasheet*.pdf\" ) -exec mv {} ~/Documents/2025-Hardware-Datasheets/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"*AmSC*.pdf\" -o -name \"prop_ascnp.pdf\" -o -name \"*A.I. and Mathematical*.pdf\" -o -name \"NP AmSC*.pdf\" ) -exec mv {} ~/Documents/2025-Ideas/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"*subk-completion*.pdf\" -o -name \"flow.pdf\" -o -name \"nda_venus*.pdf\" -o -name \"WorkPackage.pdf\" -o -name \"*Planning*.pdf\" -o -name \"NEFCD Org Chart*.pdf\" -o -name \"email-*.pdf\" -o -name \"M3IR-*.pdf\" -o -name \"L2025*.pdf\" -o -name \"nn_2025*.pdf\" -o -name \"ww5-excerpt.pdf\" -o -name \"mermaid-diagram*.pdf\" -o -name \"HLMQ*.pdf\" -o -name \"*Members.pdf\" -o -name \"2025 Members.pdf\" -o -name \"PB2000031*.pdf\" -o -name \"*SSL Server*.pdf\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -name \"*.pdf\" -type f -exec basename {} ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"FY 2026 PROGRAM*.pdf\" -o -name \"ORNLChargeBalances.pdf\" -o -name \"SessionChairGuidance.pdf\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"ORNLBandTemplate*.pdf\" ) -exec mv {} ~/Documents/2025-Templates/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"*lessons_learned*.pdf\" -o -name \"*nonlwr_inventory*.pdf\" ) -exec mv {} ~/Documents/2025-Technical-Reports/ ;)",
      "Bash(find ~/Documents/2025-Misc-PDFs -maxdepth 1 -type f ( -name \"Advance Nuclear*.pdf\" -o -name \"5404b97d*.pdf\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"*flux*\" -o -name \"*power*\" -o -name \"*dens_*\" -o -name \"*core*\" -o -name \"Figure_*.png\" -o -name \"*plot*.png\" -o -name \"memory_usage*.png\" ) -exec mv {} ~/Documents/2025-Technical-Figures/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"Screenshot*.jpg\" -o -name \"Screenshot*.png\" -o -name \"screen*.jpg\" -o -name \"screen*.png\" -o -name \"slack-thread*.png\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"SCALE_NCS*.jpg\" -o -name \"*slide*.png\" -o -name \"paper-page*.png\" -o -name \"opener.png\" ) -exec mv {} ~/Documents/2025-Technical-Figures/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"*.jpeg\" -o -name \"download.png\" -o -name \"NotebookLM*.png\" -o -name \"GateMatrix.png\" -o -name \"contents.png\" -o -name \"acc.png\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"eqn*.jpg\" -o -name \"fp*.jpg\" -o -name \"t*.jpg\" -o -name \"lc.jpg\" -o -name \"hard-text.jpeg\" -o -name \"bebound*.png\" ) -exec mv {} ~/Documents/2025-Technical-Figures/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"x*.png\" -o -name \"z.png\" -o -name \"a*.png\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f -exec basename {})",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"*Framework*.png\" -o -name \"*flowchart*.png\" -o -name \"grid.png\" -o -name \"equation.png\" -o -name \"eqn.png\" -o -name \"table-d.png\" -o -name \"valid-icon.png\" -o -name \"Picture*.png\" -o -name \"detected_circles.png\" -o -name \"binary.png\" ) -exec mv {} ~/Documents/2025-Technical-Figures/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"SCALE-maintainer*.png\" -o -name \"Studio-Project.png\" -o -name \"events.jpg\" -o -name \"countries.png\" -o -name \"2025-poseidons*.jpg\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"download-*.png\" -o -name \"output.*\" -o -name \"input.png\" -o -name \"show.png\" -o -name \"debug.png\" -o -name \"selected.png\" -o -name \"contents*.png\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f ( -name \"p*.png\" -o -name \"p*.jpg\" -o -name \"c*.png\" -o -name \"c*.jpg\" -o -name \"s*.png\" -o -name \"l*.jpg\" -o -name \"ferran*.png\" -o -name \"doublep.jpg\" -o -name \"pz.png\" -o -name \"uir.png\" -o -name \"coh.png\" -o -name \"g1-soundwave.png\" -o -name \"b*.jpg\" -o -name \"A.png\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-Misc-Images -type f -name \"file_*.png\" -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents -maxdepth 1 -type d -name \"2025-*\" -exec basename {})",
      "Bash(find ~/Documents/2025-Administrative -type f -exec basename {})",
      "Bash(find ~/Documents/2025-AI-Research -type f -exec basename {})",
      "Bash(find ~/Documents/2025-SCALE-Work -type f -exec basename {})",
      "Bash(find ~/Documents/2025-GIS-Data -type f -exec basename {})",
      "Bash(find ~/Documents/2025-Administrative -type f -name \"*.xlsx\" -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(unzip -l ~/Documents/2025-AI-Research/AI-ASCR-NE.zip)",
      "Bash(find ~/Documents/2025-GIS-Data -type f -name \"*.zip\" -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Documents/2025-AI-Research -type f -name \"*.zip\" -exec mv {} ~/Documents/2025-Ideas/ ;)",
      "Bash(find ~/Documents/2025-SCALE-Work -type f -exec basename {} ;)",
      "Bash(find ~/Documents/2025-DNCSH_Workshops -type f -exec basename {} ;)",
      "Bash(find ~/Documents/2025-Workshop_Slides_Collection -type f -exec basename {})",
      "Bash(find ~/Documents/2025-SCALE-Work -type f -exec ls -lh {} ;)",
      "Bash(for dir in MSR Sensitivity-Analysis Testing Scratch Medical-Isotopes PWR SFR)",
      "Bash(do echo \"=== $dir ===\")",
      "Bash(find ~/Documents/2025-SCALE-Work/\"$dir\" -type f)",
      "Bash(find ~/Documents/2025_02-EPRI_Presentation -type f -exec basename {} ;)",
      "Bash(find ~/Documents/2025_02-EPRI_Presentation -maxdepth 1 -type f ( -name \"*rev2.pptx\" -o -name \"*rev3.pptx\" -o -name \"*_Wieselquist.pptx\" ) -exec mv {} ~/Documents/2025_02-EPRI_Presentation/_drafts/ ;)",
      "Bash(find ~/Documents/2025_02-EPRI_Presentation -maxdepth 1 -type f -exec basename {} ;)",
      "Bash(find ~/Documents/2025_02-WANDA_Conference -type f -exec ls -lh {} ;)",
      "Bash(find ~/Documents/2025_02-WANDA_Conference -maxdepth 1 -type f ( -name \"*_02_ww5.pptx\" -o -name \"*_03_ww5.pptx\" -o -name \"*WANDAPlenary_ww5.pptx\" ) -exec mv {} ~/Documents/2025_02-WANDA_Conference/_drafts/ ;)",
      "Bash(find ~/Documents/2025_02-WANDA_Conference -maxdepth 1 -type f ( -name \"WANDA_2025_JO*.pptx\" ) -exec mv {} ~/Documents/2025_02-WANDA_Conference/other-presentations/ ;)",
      "Bash(find ~/Documents/2025_02-WANDA_Conference -maxdepth 1 -type f -name \"WANDA_23*.pptx\" -exec mv {} ~/Documents/2024-WANDA_Conference/ ;)",
      "Bash(find ~/Documents/2025_02-WANDA_Conference -maxdepth 1 -type f -exec basename {} ;)",
      "Bash(find ~/2025-OECD_NEA_SG8 -type f -exec ls -lh {} ;)",
      "Bash(cat ~/obsidian/WKS/Index.md)",
      "Bash(find ~ -maxdepth 3 -type d ( -name \"*sdrm*\" -o -name \"*SDRM*\" ))",
      "Bash(find ~ -maxdepth 2 -type f ( -name \"*sdrm*.py\" -o -name \"*SDRM*.py\" ))",
      "Bash(find ~/Desktop -maxdepth 1 -type f -exec basename {})",
      "Bash(find ~/Desktop -type f ( -name \"Screenshot*.png\" -o -name \".DS_Store\" -o -name \".localized\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Desktop -type f -name \"1886-Z720-24*.pdf\" -exec mv {} ~/2025-NRC/ ;)",
      "Bash(find ~/Desktop -type f -name \"2024-05-14_SCALE.pptx\" -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Desktop -type f -name \"qc_ICAPP*.pdf\" -exec mv {} ~/Documents/2025-Technical-Reports/ ;)",
      "Bash(find ~/Desktop -type f -name \"SCALE-QAP*.pdf\" -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Desktop -type f -name \"SCALE-SQAB*.pptx\" -exec mv {} ~/2025-SCALE/ ;)",
      "Bash(find ~/Desktop -type f -name \"SDL-2024-065.html\" -exec mv {} ~/2025-SCALE/bug-tracking/ ;)",
      "Bash(find ~/Desktop -type f -name \"SPARC*.pptx\" -exec mv {} ~/2025-DNCSH/ ;)",
      "Bash(find ~/Desktop -type f ( -name \"*.md\" ) -exec basename {} ;)",
      "Bash(ln -s ~/2025-SCALE ~/Desktop/2025-SCALE)",
      "Bash(ln -s ~/2025-BudgetFix ~/Desktop/2025-BudgetFix)",
      "Bash(git -C ~/scale-dist log --pretty=format:\"%ai\")",
      "Bash(ln -s ~/2025-scale_dist ~/Desktop/2025-scale_dist)",
      "Bash(find ~/Desktop/stacks/records/self -type f ( -name \"*.docx\" -o -name \"*.pdf\" -o -name \"*.jpg\" -o -name \"*.png\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Desktop/stacks/records/others -type f ( -name \"*.docx\" -o -name \"*.pdf\" ) -exec mv {} ~/Documents/2025-Personal-Records/ ;)",
      "Bash(find ~/Desktop/stacks/science -type f ( -name \"*.pdf\" -o -name \"*.docx\" -o -name \"*.pptx\" ) -exec mv {} ~/Documents/2025-Research-Papers/ ;)",
      "Bash(find ~/Desktop/stacks/science -type f ( -name \"*.jpg\" -o -name \"*.png\" ) -exec mv {} ~/Documents/2025-Technical-Figures/ ;)",
      "Bash(find ~/Desktop/stacks/science -type f -name \"*.tex\" -exec mv {} ~/Documents/2025-Research-Papers/ ;)",
      "Bash(for dir in ~/Desktop/stacks/projects/personal/*/)",
      "Bash(find \"$dir\" -type f ! -name \"*.node.md\" ! -name \".DS_Store\" ! -path \"*/.ipynb_checkpoints/*\")",
      "Bash(find ~ -maxdepth 3 -type f ( -name \"*SCALE*.png\" -o -name \"*SCALE*.jpg\" -o -name \"*SCALE*.jpeg\" -o -name \"*scale*.png\" -o -name \"*scale*.jpg\" ) ! -path \"*/Library/*\" ! -path \"*/.Trash/*\")",
      "Bash(find ~/Documents/2025-AI-Generated-Images -type f ( -name \"*scale*\" -o -name \"*SCALE*\" -o -name \"*reactor*\" -o -name \"*nuclear*\" ) -iname \"*.png\" -o -iname \"*.jpg\" -o -iname \"*.webp\")",
      "Bash(ls -lh ~/2025-SCALE_Icons/*/*.{png,webp,jpg})",
      "Bash(find ~ -maxdepth 1 -type d ( -name \"*ensdf*\" -o -name \"*ENSDF*\" -o -name \"*nuclear*data*\" ))",
      "Bash(ln -sf ~/2024-ensdf_reader/README.md ~/obsidian/_links/2024-ensdf_reader/README.md)",
      "Bash(find ~/Desktop -maxdepth 1 -type f -exec basename {} ;)",
      "Bash(find ~/Documents/2025-Misc-Documents -type f -exec basename {})",
      "Bash(python -m pytest:*)",
      "Bash(mkdir:*)",
      "Bash(git rm:*)",
      "Bash(git add:*)",
      "Bash(python3 -m pytest:*)",
      "Bash(echo:*)",
      "Bash(/usr/bin/python3 -m pytest:*)",
      "Bash(pip3 install:*)",
      "Bash(git commit -m \"$(cat <<''EOF''\nAdd comprehensive robustness improvements (Phase 3)\n\n**Config Validation:**\n- Add wks/config_validator.py with comprehensive validation\n- Validates required keys, types, ranges, and path existence\n- Integrated into daemon startup for early failure detection\n- Tests in tests/test_config_validator.py (8 tests)\n\n**MongoDB Retry Logic:**\n- Add wks/mongo_retry.py with retry decorator and wrapper\n- Exponential backoff for transient failures\n- Read operations only (no writes to avoid duplicates)\n- Tests in tests/test_mongo_retry.py (7 tests)\n\n**Error Messages:**\n- Add wks/error_messages.py with helpful error messages\n- MongoDB connection failures with troubleshooting steps\n- Missing dependency errors with install instructions\n- Model download errors with proxy configuration\n- File permission errors with chmod examples\n- Integrated into similarity.py and cli.py\n- Tests in tests/test_error_messages.py (5 tests)\n\n**Integration:**\n- daemon.py: Replace manual validation with validator\n- similarity.py: Add connection testing and error handling\n- cli.py: Better dependency import error messages\n\n**Testing:**\n- All 58 tests pass (including 20 new tests)\n- Config validation, retry logic, and error messages tested\n- Smoke tests from first pass still passing\n\nü§ñ Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\nEOF\n)\")",
      "Bash(git commit:*)",
      "Bash(launchctl list:*)",
      "Bash(cat:*)",
      "Bash(wks0 service restart)",
      "Bash(pip install:*)",
      "Bash(wks0 service install:*)",
      "Bash(while read line)",
      "Bash(wks0 related:*)",
      "Bash(mongo --eval:*)",
      "Bash(python3:*)",
      "Bash(wks0 db info:*)",
      "Bash(wks0 service:*)",
      "Bash(/Users/ww5/2025-SCALE_Runtime_Estimator/.venv/bin/python3.14:*)",
      "Bash(find:*)",
      "Bash(wks0:*)",
      "Bash(/Users/ww5/2025-SCALE_Runtime_Estimator/.venv/bin/pip install:*)",
      "Bash(source .venv/bin/activate)",
      "Bash(/dev/null)",
      "Bash(pipx list:*)",
      "Bash(pipx uninstall:*)",
      "Bash(timeout 3 python3:*)",
      "Bash(python scripts/migrate_wks0.py:*)",
      "Bash(python -m wks.cli:*)",
      "Bash(pip3 list:*)",
      "Bash(git checkout:*)",
      "Bash(python -c:*)",
      "Bash(python test_mcp_server.py:*)",
      "Bash(timeout 2 wks0 mcp run:*)",
      "Bash(/Users/ww5/2025-WKS/.venv/bin/wks0 mcp run:*)",
      "Bash({ echo '{\"\"\"\"jsonrpc\"\"\"\":\"\"\"\"2.0\"\"\"\",\"\"\"\"id\"\"\"\":0,\"\"\"\"method\"\"\"\":\"\"\"\"initialize\"\"\"\",\"\"\"\"params\"\"\"\":{\"\"\"\"protocolVersion\"\"\"\":\"\"\"\"2024-11-05\"\"\"\",\"\"\"\"capabilities\"\"\"\":{},\"\"\"\"clientInfo\"\"\"\":{\"\"\"\"name\"\"\"\":\"\"\"\"test\"\"\"\",\"\"\"\"version\"\"\"\":\"\"\"\"1.0\"\"\"\"}}}')",
      "Bash(})"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path=".cursor/rules/important.mdc">
# Directives

* Use the virtual environment. If it doesn't exist create one in .venv. You can install whatever you want there.
* Eliminate unnecessary features and legacy code wherever possible‚Äîremove complexity, redundant state, backward compatibility, and unused paths.
* Support **only CLI and MCP** interfaces; drop all other modes.
* Consolidate configuration to minimal, essential parameters.
* Centralize configuration access through **dataclasses** instead of dictionaries. Validate strictly on load (`__post_init__`) and fail immediately if expected data or formats are missing.
* Remove fallback logic (‚Äúhedging‚Äù); no silent defaults or implicit substitutions.
* Represent databases as `<collection>.<database>` and fail hard if not found.
* Include clear validation and precise error reporting with explicit paths, found values, and expected values.
* For user-facing commands such as `wks0 service status`, support a **live-updating display** with the `--live` flag.
* Use colorized output‚Äîred for false or failed states‚Äîand show OK/FAIL status before the last error.
* Keep tables simple and grouped by logical sections when appropriate.
* Apply **DRY**, **KISS**, and no-hedge principles throughout.
* Use `lizard` to measure code metrics (**CCN** and **NLOC**) and split any function with **CCN > 10** or **NLOC > 100**.
* Apply clear design patterns, especially the **Strategy Pattern** for display modes.
* Remove or disable obsolete tests tied to deprecated functionality and ensure remaining tests pass after refactoring.
* Replace ad-hoc error handling with structured aggregation‚Äîcollect all errors, then raise them together for full diagnostics.
* Fail fast, fail visibly, and keep system behavior deterministic.
* Avoid optional or hidden recovery logic.
* Centralize validation and configuration handling at the lowest level.
* Favor strong typing, dataclasses, and explicit structure over dynamic or dictionary-based access.
* If a single file is more than 900 lines, break it up. This includes tests.
* Use a logger for all informational/debug and warning/error conditions. Output the informational debug content to logs only. In MCP mode, send warning/errors in the returned JSON packet. In CLI mode, emit warnings/errors to STDERR. Information/debug should not be emitted to CLI (only logs). CLI STDOUT should always just be the expected content. If the error is so bad no content can be rendered, then STDOUT should be empty. 
* Every CLI command needs to do 4 things: 1) immediately say what you are doing on STDERR, 2) on STDERR start a progress bar for doing it, 3) on STDERR say what you did and if there were problems, 4) display the output on STDOUT
</file>

<file path=".github/workflows/test.yml">
name: Tests

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]

jobs:
  test:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .

      - name: Run tests
        run: |
          python -m pytest tests/ -v --tb=short

      - name: Check imports
        run: |
          python -c "import wks; print('‚úì Package imports successfully')"
</file>

<file path="guides/links-design.md">
# Link Maintenance Design

**Status:** Design phase (Phase 6)
**Prerequisites:** Phase 5 (Related Documents) completion

## Overview

Automated maintenance of wiki links in Obsidian vault to keep references accurate as files move.

## Existing Infrastructure

Already implemented in `wks/obsidian.py`:
- `update_vault_links_on_move(old_path, new_path)` - Rewrites `[[links]]` when files move
- `mark_reference_deleted(path)` - Annotates notes referencing deleted files
- `find_broken_links()` - Finds broken symlinks in `_links/`
- `cleanup_broken_links()` - Removes broken symlinks

## New Commands

### `wks0 links audit`
Comprehensive link health check:
- Scan all markdown files in vault
- Find broken wiki links `[[path/to/file]]`
- Find broken wikilink aliases `[[path|alias]]`
- Find references to deleted files
- Generate report of issues

### `wks0 links fix`
Automated repair:
- Fix references to moved files (using file operations log)
- Update legacy `[[links/...]]` to `[[_links/...]]`
- Remove references to permanently deleted files
- Interactive confirmation mode

### `wks0 links report`
Health dashboard:
- Total links in vault
- Broken link count
- Recently updated links
- Most-linked files
- Orphaned files (no incoming links)

## Implementation Approach

### New CLI Module: `wks/cli_links.py`

```python
def cmd_links_audit(args):
    """Audit all wiki links in vault."""
    # 1. Scan vault markdown files
    # 2. Extract all [[wikilinks]]
    # 3. Verify targets exist
    # 4. Report issues

def cmd_links_fix(args):
    """Fix broken links automatically."""
    # 1. Run audit
    # 2. For each broken link:
    #    - Check file_ops.jsonl for moves
    #    - Suggest/apply fix
    # 3. Confirm with user if interactive

def cmd_links_report(args):
    """Generate link health report."""
    # 1. Count all links
    # 2. Compute statistics
    # 3. Format output
```

### Link Scanner

```python
def scan_vault_links(vault_path: Path) -> List[LinkRef]:
    """Extract all wiki links from vault markdown files."""
    # Parse [[target]] and [[target|alias]]
    # Return list of (source_file, link_text, target_path)
```

### Link Validator

```python
def validate_link(link: LinkRef, vault: ObsidianVault) -> LinkStatus:
    """Check if a link target exists and is accessible."""
    # Return: Valid, Broken, MovedTo(new_path), Deleted
```

## Integration with Daemon

The daemon already updates links on move via:
- `daemon.py:211` - `vault.update_vault_links_on_move(src, dest)`

Enhancement: Add periodic audit task
- Run `links audit` weekly
- Surface issues in Health.md
- Suggest running `links fix`

## Output Examples

### Audit Output
```
Link Audit Report
=================

Scanned: 453 markdown files
Total links: 1,247
Broken links: 12

Issues:
  ~/obsidian/Projects/2025-NRC.md:15
    [[_links/old/path/file.pdf]] ‚Üí Target not found

  ~/obsidian/Topics/Nuclear_Data.md:42
    [[links/legacy/document.pdf]] ‚Üí Legacy path (use _links)

Run 'wks0 links fix' to repair automatically.
```

### Fix Output
```
Fixing broken links...

~/obsidian/Projects/2025-NRC.md:15
  [[_links/old/path/file.pdf]]
  ‚Üí File moved to: ~/Documents/2025-Archive/file.pdf
  Fix: [[_links/Documents/2025-Archive/file.pdf]]
  Apply? [Y/n]

Fixed 8 of 12 issues.
4 require manual review.
```

## Testing Strategy

### Unit Tests
- Link extraction regex
- Link validation logic
- Fix application

### Integration Tests
- Full vault scan
- Repair broken links
- Report generation

### Manual Tests
1. Create test vault with known broken links
2. Run audit - verify all issues found
3. Run fix - verify repairs applied
4. Run report - verify statistics correct

## Design Principles

1. **Non-destructive** - Always confirm before changes
2. **Traceable** - Log all link fixes
3. **Reversible** - Maintain backup of originals
4. **Informative** - Clear explanations of issues
5. **Automated** - Minimize manual intervention
</file>

<file path="guides/patterns-design.md">
# Pattern System Design

**Status:** Design phase (Phase 4)
**Prerequisites:** Phase 3 (Robustness) completion

## Overview

Simple script-based organizational patterns with MCP server for AI access.
Zero duplication: scripts are the source of truth for both code and documentation.

## Architecture

### Pattern Structure
```
~/.wks/patterns/
  picofday              # Executable script
  projfile
  deadfile
```

### Documentation Extraction

Scripts document themselves via header comments:
```bash
#!/usr/bin/env bash
# Short description (becomes pattern.description)
#
# Longer documentation (becomes pattern.documentation)
# Usage: command <args>
```

### Configuration

Add to `~/.wks/config.json`:
```json
{
  "patterns": {
    "scripts_dir": "~/.wks/patterns"
  }
}
```

## Implementation Files

### 1. Core Module: `wks/patterns.py`

Pattern discovery and execution logic.

```python
"""Pattern system: discover and execute organizational scripts."""

from __future__ import annotations

import os
import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional


@dataclass
class Pattern:
    """A discovered pattern script."""
    name: str
    path: Path
    description: str
    documentation: str
    executable: bool

    @classmethod
    def from_path(cls, path: Path) -> Optional[Pattern]:
        """Create Pattern from script path."""
        if not path.is_file():
            return None

        name = path.name
        executable = os.access(path, os.X_OK)

        # Extract documentation from script comments
        description = ""
        doc_lines = []

        try:
            with open(path, 'r', encoding='utf-8') as f:
                in_header = False
                for line in f:
                    stripped = line.strip()

                    # Look for comment lines after shebang
                    if stripped.startswith('#!'):
                        in_header = True
                        continue

                    if in_header and stripped.startswith('#'):
                        # Remove leading '# '
                        content = stripped[1:].strip()
                        doc_lines.append(content)

                        # First non-empty line is the description
                        if not description and content:
                            description = content
                    elif in_header and not stripped:
                        # Empty line continues doc block
                        doc_lines.append("")
                    elif in_header:
                        # Non-comment line ends header
                        break
        except Exception:
            pass

        if not description:
            description = f"Pattern: {name}"

        documentation = '\n'.join(doc_lines)

        return cls(
            name=name,
            path=path,
            description=description,
            documentation=documentation,
            executable=executable
        )


def discover_patterns(patterns_dir: Optional[Path] = None) -> List[Pattern]:
    """Find all pattern scripts in the patterns directory."""
    if patterns_dir is None:
        from .constants import WKS_HOME_EXT
        patterns_dir = Path.home() / WKS_HOME_EXT / "patterns"

    patterns_dir = patterns_dir.expanduser()
    if not patterns_dir.exists():
        return []

    patterns = []
    for item in patterns_dir.iterdir():
        # Skip markdown docs, hidden files
        if item.name.startswith('.') or item.name.endswith('.md'):
            continue

        pattern = Pattern.from_path(item)
        if pattern:
            patterns.append(pattern)

    return sorted(patterns, key=lambda p: p.name)


def run_pattern(pattern: Pattern, args: List[str], interactive: bool = True) -> int:
    """Execute a pattern script with given arguments."""
    if not pattern.executable:
        print(f"Error: {pattern.path} is not executable")
        print(f"Run: chmod +x {pattern.path}")
        return 1

    cmd = [str(pattern.path)] + args

    try:
        if interactive:
            # Run with inherited stdin/stdout/stderr for interactive prompts
            result = subprocess.run(cmd, check=False)
            return result.returncode
        else:
            # Capture output for programmatic use
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=False
            )
            if result.stdout:
                print(result.stdout, end='')
            if result.stderr:
                print(result.stderr, end='')
            return result.returncode
    except Exception as e:
        print(f"Error executing pattern: {e}")
        return 1


def get_patterns_dir(config: dict) -> Path:
    """Get patterns directory from config with fallback."""
    from .constants import WKS_HOME_EXT

    patterns_cfg = config.get("patterns", {})
    scripts_dir = patterns_cfg.get("scripts_dir", f"~/{WKS_HOME_EXT}/patterns")
    return Path(scripts_dir).expanduser()
```

### 2. CLI Commands: `wks/cli_pattern.py`

```python
"""CLI commands for pattern management."""

from pathlib import Path

from .config import load_user_config
from .patterns import discover_patterns, get_patterns_dir, run_pattern


def cmd_pattern_list(args) -> int:
    """List all available patterns."""
    cfg = load_user_config()
    patterns_dir = get_patterns_dir(cfg)

    patterns = discover_patterns(patterns_dir)

    if not patterns:
        print(f"No patterns found in {patterns_dir}")
        print(f"\nCreate executable scripts in this directory to define patterns.")
        return 0

    print(f"Patterns directory: {patterns_dir}\n")

    if args.format == 'json':
        import json
        output = [
            {
                "name": p.name,
                "path": str(p.path),
                "description": p.description,
                "executable": p.executable
            }
            for p in patterns
        ]
        print(json.dumps(output, indent=2))
    else:
        print(f"{'Name':<20} {'Description':<50} {'Path'}")
        print("-" * 100)
        for p in patterns:
            status = "‚úì" if p.executable else "‚úó"
            print(f"{status} {p.name:<18} {p.description:<50} {p.path}")

    return 0


def cmd_pattern_show(args) -> int:
    """Show documentation for a pattern."""
    cfg = load_user_config()
    patterns_dir = get_patterns_dir(cfg)

    patterns = discover_patterns(patterns_dir)
    pattern = next((p for p in patterns if p.name == args.name), None)

    if not pattern:
        print(f"Pattern not found: {args.name}")
        return 1

    print(f"Pattern: {pattern.name}")
    print(f"Path: {pattern.path}")
    print(f"Executable: {'Yes' if pattern.executable else 'No'}")
    print()

    # Show extracted documentation
    if pattern.documentation:
        print(pattern.documentation)
    else:
        print("No documentation available.")
        print("\nAdd comments after the shebang line to document this pattern:")
        print("  #!/usr/bin/env bash")
        print("  # Short description")
        print("  # ")
        print("  # Longer documentation...")

    return 0


def cmd_pattern_run(args) -> int:
    """Run a pattern with arguments."""
    cfg = load_user_config()
    patterns_dir = get_patterns_dir(cfg)

    patterns = discover_patterns(patterns_dir)
    pattern = next((p for p in patterns if p.name == args.name), None)

    if not pattern:
        print(f"Pattern not found: {args.name}")
        return 1

    return run_pattern(pattern, args.args, interactive=not args.non_interactive)


def setup_pattern_parser(subparsers):
    """Add pattern subcommands to the CLI parser."""
    pattern_parser = subparsers.add_parser('pattern', help='Pattern script management')
    pattern_sub = pattern_parser.add_subparsers(dest='pattern_command')

    # wks0 pattern list
    list_parser = pattern_sub.add_parser('list', help='List available patterns')
    list_parser.add_argument('--format', choices=['table', 'json'], default='table')

    # wks0 pattern show <name>
    show_parser = pattern_sub.add_parser('show', help='Show pattern documentation')
    show_parser.add_argument('name', help='Pattern name')

    # wks0 pattern run <name> [args...]
    run_parser = pattern_sub.add_parser('run', help='Execute a pattern')
    run_parser.add_argument('name', help='Pattern name')
    run_parser.add_argument('args', nargs='*', help='Arguments to pass to pattern')
    run_parser.add_argument('--non-interactive', action='store_true',
                           help='Run without interactive prompts')
```

### 3. MCP Server: `wks/mcp/pattern_server.py`

```python
"""MCP server that exposes pattern scripts as tools."""

import asyncio
from pathlib import Path
from typing import Any, Dict, List

from mcp.server import Server
from mcp.types import Tool, TextContent

from ..config import load_user_config
from ..patterns import discover_patterns, get_patterns_dir, Pattern


class PatternMCPServer:
    """MCP server for WKS patterns."""

    def __init__(self):
        self.server = Server("wks-patterns")
        self.patterns: List[Pattern] = []
        self._setup_handlers()

    def _setup_handlers(self):
        @self.server.list_tools()
        async def list_tools() -> List[Tool]:
            """Dynamically discover and expose patterns as tools."""
            cfg = load_user_config()
            patterns_dir = get_patterns_dir(cfg)
            self.patterns = discover_patterns(patterns_dir)

            tools = []
            for pattern in self.patterns:
                if not pattern.executable:
                    continue

                tools.append(Tool(
                    name=f"pattern_{pattern.name}",
                    description=pattern.description,
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "file_path": {
                                "type": "string",
                                "description": "Path to file to process"
                            },
                            "additional_args": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Additional arguments for the pattern",
                                "default": []
                            }
                        },
                        "required": ["file_path"]
                    }
                ))

            return tools

        @self.server.call_tool()
        async def call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
            """Execute a pattern script."""
            if not name.startswith("pattern_"):
                return [TextContent(type="text", text=f"Unknown tool: {name}")]

            pattern_name = name[8:]  # Remove "pattern_" prefix
            pattern = next((p for p in self.patterns if p.name == pattern_name), None)

            if not pattern:
                return [TextContent(
                    type="text",
                    text=f"Pattern not found: {pattern_name}"
                )]

            file_path = arguments.get("file_path", "")
            additional_args = arguments.get("additional_args", [])

            if not file_path:
                return [TextContent(
                    type="text",
                    text="Error: file_path is required"
                )]

            # Build command
            cmd_args = [file_path] + additional_args

            # Execute pattern
            import subprocess
            try:
                result = subprocess.run(
                    [str(pattern.path)] + cmd_args,
                    capture_output=True,
                    text=True,
                    timeout=60,
                    check=False
                )

                output = []
                if result.stdout:
                    output.append(f"Output:\n{result.stdout}")
                if result.stderr:
                    output.append(f"Errors:\n{result.stderr}")

                if result.returncode == 0:
                    output.insert(0, f"‚úì Pattern '{pattern_name}' completed successfully\n")
                else:
                    output.insert(0, f"‚úó Pattern '{pattern_name}' failed (exit code {result.returncode})\n")

                return [TextContent(type="text", text="\n".join(output))]

            except subprocess.TimeoutExpired:
                return [TextContent(
                    type="text",
                    text=f"Pattern '{pattern_name}' timed out after 60 seconds"
                )]
            except Exception as e:
                return [TextContent(
                    type="text",
                    text=f"Error executing pattern: {e}"
                )]

    async def run(self):
        """Run the MCP server."""
        from mcp.server.stdio import stdio_server

        async with stdio_server() as (read_stream, write_stream):
            await self.server.run(
                read_stream,
                write_stream,
                self.server.create_initialization_options()
            )


def main():
    """Entry point for pattern MCP server."""
    server = PatternMCPServer()
    asyncio.run(server.run())
```

### 4. CLI Integration

Add to `wks/cli.py`:

```python
from .cli_pattern import setup_pattern_parser

# In main():
def main():
    parser = argparse.ArgumentParser(...)
    subparsers = parser.add_subparsers(dest='command')

    # ... existing commands ...

    # Pattern commands
    setup_pattern_parser(subparsers)

    # ... rest of argument parsing ...

    # Pattern command dispatch
    if args.command == 'pattern':
        from .cli_pattern import cmd_pattern_list, cmd_pattern_show, cmd_pattern_run

        if args.pattern_command == 'list':
            return cmd_pattern_list(args)
        elif args.pattern_command == 'show':
            return cmd_pattern_show(args)
        elif args.pattern_command == 'run':
            return cmd_pattern_run(args)
        else:
            print("Usage: wks0 pattern {list|show|run}")
            return 1

    # MCP serve dispatch
    if args.command == 'mcp':
        if args.mcp_command == 'serve':
            if args.server == 'patterns':
                from .mcp.pattern_server import main as pattern_server_main
                return pattern_server_main()
```

### 5. Dependencies

Add to `setup.py`:
```python
install_requires=[
    # ... existing ...
    "mcp>=0.9.0",  # MCP SDK for AI integration
],
```

## Example Pattern Script

`~/.wks/patterns/picofday`:
```bash
#!/usr/bin/env bash
# Picture of the Day organizer
#
# Organizes a single significant scientific/technical image per day.
#
# Usage: picofday <image-file>
#
# What it does:
#   1. Calculates day of year (1-366)
#   2. Creates ~/Pictures/YYYY-Daily_Image/DDD/ directory
#   3. Moves image to DDD-OriginalName.ext
#   4. Creates hard link in Obsidian at Pictures/OfTheDay/_links/YYYY/DDD.ext
#   5. Prompts for caption creation

set -e

if [ $# -lt 1 ]; then
    echo "Usage: picofday <image-file>"
    exit 1
fi

IMAGE_PATH="$1"

if [ ! -f "$IMAGE_PATH" ]; then
    echo "Error: File not found: $IMAGE_PATH"
    exit 1
fi

# Get day of year
DAY_NUM=$(date +%j)
YEAR=$(date +%Y)

# Create directory
DEST_DIR="$HOME/Pictures/${YEAR}-Daily_Image/${DAY_NUM}"
mkdir -p "$DEST_DIR"

# Get basename without extension
BASENAME=$(basename "$IMAGE_PATH")
EXT="${BASENAME##*.}"
NAME="${BASENAME%.*}"

# Move file
DEST_FILE="${DEST_DIR}/${DAY_NUM}-${NAME}.${EXT}"
mv "$IMAGE_PATH" "$DEST_FILE"

echo "‚úì Moved to: $DEST_FILE"

# Create hard link for Obsidian
OBSIDIAN_LINK_DIR="$HOME/obsidian/Pictures/OfTheDay/_links/${YEAR}"
mkdir -p "$OBSIDIAN_LINK_DIR"

ln "$DEST_FILE" "${OBSIDIAN_LINK_DIR}/${DAY_NUM}.${EXT}"

echo "‚úì Linked in Obsidian: Pictures/OfTheDay/_links/${YEAR}/${DAY_NUM}.${EXT}"

# Prompt for caption
echo ""
echo "Create caption? [Y/n]"
read -r response
if [[ "$response" =~ ^[Yy]$ ]] || [ -z "$response" ]; then
    CAPTION_FILE="${DEST_DIR}/${DAY_NUM}-${NAME}.md"

    # Open editor
    ${EDITOR:-nano} "$CAPTION_FILE"

    # Link caption too
    if [ -f "$CAPTION_FILE" ]; then
        ln "$CAPTION_FILE" "${OBSIDIAN_LINK_DIR}/${DAY_NUM}.md"
        echo "‚úì Caption linked"
    fi
fi

echo "‚úì Complete"
```

## Usage Examples

### Human CLI
```bash
# List patterns
wks0 pattern list
# Output:
# Patterns directory: ~/.wks/patterns
#
# Name                 Description                         Path
# ‚úì picofday           Picture of the Day organizer       ~/.wks/patterns/picofday

# Show documentation
wks0 pattern show picofday

# Run pattern
wks0 pattern run picofday ~/Downloads/nuclear_data_plot.png

# JSON output
wks0 pattern list --format json
```

### AI MCP
Add to `~/Library/Application Support/Claude/claude_desktop_config.json`:
```json
{
  "mcpServers": {
    "wks-patterns": {
      "command": "wks0",
      "args": ["mcp", "serve", "patterns"]
    }
  }
}
```

AI can then use:
```
Tool: pattern_picofday
Arguments: {"file_path": "/Users/ww5/Downloads/figure.png"}
```

## Testing Strategy

### Unit Tests
- Pattern discovery from directory
- Documentation extraction from scripts
- Script execution (mocked)

### Integration Tests
- End-to-end CLI commands
- MCP server tool listing
- MCP server tool execution

### Manual Smoke Tests
1. Create test pattern script
2. `wks0 pattern list` shows it
3. `wks0 pattern show <name>` extracts docs
4. `wks0 pattern run <name>` executes
5. MCP server exposes as tool

## Design Principles

1. **Scripts are truth** - No separate documentation files
2. **Zero duplication** - Single execution path for CLI and MCP
3. **Minimal magic** - Simple discovery, simple execution
4. **User extensible** - Drop script in directory, it works
5. **Convention over config** - Comment format is the API
</file>

<file path="guides/related-design.md">
# Related Documents Design

**Status:** Design phase (Phase 5)
**Prerequisites:** Phase 4 (Pattern System) completion

## Overview

Extend `SimilarityDB.find_similar()` to provide a user-facing command for discovering semantically related documents.

## Command Design

```bash
# Basic usage
wks0 related <path>

# With filters
wks0 related paper.pdf --limit 10 --min-similarity 0.7 --type pdf

# JSON output
wks0 related paper.pdf --format json

# Explain why related
wks0 related paper.pdf --explain
```

## Implementation Approach

### Extend Existing Code
Current: `wks/similarity.py:549` has `SimilarityDB.find_similar()`
- Already supports `query_path`, `query_text`, `limit`, `min_similarity`, `mode`
- Returns `List[Tuple[str, float]]` (URI, similarity score)

### New CLI Module: `wks/cli_related.py`

```python
def cmd_related(args):
    """Find semantically similar documents."""
    # 1. Load config and build SimilarityDB
    # 2. Call find_similar() with args
    # 3. Format output (table/json)
    # 4. Optionally explain similarity
```

### Explanation Generation

Add method to `SimilarityDB`:
```python
def explain_similarity(self, src: Path, dst: Path, sim: float) -> str:
    """Generate human-readable explanation of why files are similar."""
    # - Check chunk-level overlaps
    # - Identify common topics (TODO: needs topic extraction)
    # - Simple heuristics based on similarity score
```

## Output Formats

### Table (default)
```
Similar to: ~/Documents/paper.pdf

0.892 - ~/Documents/2025-Research/related_paper.pdf
0.784 - ~/2025-NRC/technical_report.pdf
0.723 - ~/Documents/2024-Conference/presentation.pptx
```

### Explained
```
Similar to: ~/Documents/paper.pdf

0.892 - ~/Documents/2025-Research/related_paper.pdf
       Strong semantic overlap - likely same research area

0.784 - ~/2025-NRC/technical_report.pdf
       Moderate topical similarity - shared technical domain
```

### JSON
```json
[
  {
    "path": "~/Documents/2025-Research/related_paper.pdf",
    "similarity": 0.892
  },
  {
    "path": "~/2025-NRC/technical_report.pdf",
    "similarity": 0.784
  }
]
```

## Future Enhancements

- Chunk-level similarity (show which sections match)
- Topic extraction and labeling
- Temporal filtering (only recent documents)
- Cross-reference with Obsidian vault links
- Suggest creating vault connections

## Reference Implementation

See: `wks/cli.py` around line 2000+ for `db info --reference` pattern to follow.
</file>

<file path="scripts/deploy_wks0.sh">
# make sure version has been bumped from current installed wks0
# stop service on current ~/bin/wks0
# pipx install new
# make sure ~/bin has symlink to new pipx install (verify version)
# restart service from ~/bin/wks0
# verify no data loss in monitor db
</file>

<file path="tests/test_cli_related.py">
"""Tests for wks0 related command."""

import json
import pytest
from pathlib import Path
from unittest.mock import patch, MagicMock


def test_related_basic_table_output(tmp_path, monkeypatch):
    """Test wks0 related with table output format."""
    # Create test file
    test_file = tmp_path / "test.txt"
    test_file.write_text("This is a test document for similarity search.")

    # Mock the similarity database
    mock_db = MagicMock()
    mock_db.find_similar.return_value = [
        ("file:///path/to/similar1.txt", 0.92),
        ("file:///path/to/similar2.pdf", 0.78),
        ("/path/to/similar3.docx", 0.65),
    ]
    mock_db.client.close = MagicMock()

    def mock_load_similarity():
        return mock_db, {}

    monkeypatch.setattr("wks.cli._load_similarity_required", mock_load_similarity)

    # Run command
    from wks.cli import main
    result = main(["related", str(test_file)])

    assert result == 0
    mock_db.find_similar.assert_called_once()
    call_kwargs = mock_db.find_similar.call_args[1]
    assert call_kwargs['limit'] == 10
    assert call_kwargs['min_similarity'] == 0.0
    assert call_kwargs['mode'] == "file"


def test_related_json_output(tmp_path, monkeypatch, capsys):
    """Test wks0 related with JSON output format."""
    # Create test file
    test_file = tmp_path / "test.txt"
    test_file.write_text("This is a test document.")

    # Mock the similarity database
    mock_db = MagicMock()
    mock_db.find_similar.return_value = [
        ("file:///path/to/doc1.txt", 0.95),
        ("/path/to/doc2.pdf", 0.82),
    ]
    mock_db.client.close = MagicMock()

    def mock_load_similarity():
        return mock_db, {}

    monkeypatch.setattr("wks.cli._load_similarity_required", mock_load_similarity)

    # Run command
    from wks.cli import main
    result = main(["related", str(test_file), "--format", "json"])

    assert result == 0

    # Check JSON output
    captured = capsys.readouterr()
    output = json.loads(captured.out)
    assert len(output) == 2
    assert output[0]["path"] == "/path/to/doc1.txt"
    assert output[0]["similarity"] == 0.95
    assert output[1]["path"] == "/path/to/doc2.pdf"
    assert output[1]["similarity"] == 0.82


def test_related_with_limit(tmp_path, monkeypatch):
    """Test wks0 related with custom limit."""
    test_file = tmp_path / "test.txt"
    test_file.write_text("Test content")

    mock_db = MagicMock()
    mock_db.find_similar.return_value = []
    mock_db.client.close = MagicMock()

    def mock_load_similarity():
        return mock_db, {}

    monkeypatch.setattr("wks.cli._load_similarity_required", mock_load_similarity)

    from wks.cli import main
    result = main(["related", str(test_file), "--limit", "5"])

    assert result == 0
    call_kwargs = mock_db.find_similar.call_args[1]
    assert call_kwargs['limit'] == 5


def test_related_with_min_similarity(tmp_path, monkeypatch):
    """Test wks0 related with minimum similarity threshold."""
    test_file = tmp_path / "test.txt"
    test_file.write_text("Test content")

    mock_db = MagicMock()
    mock_db.find_similar.return_value = []
    mock_db.client.close = MagicMock()

    def mock_load_similarity():
        return mock_db, {}

    monkeypatch.setattr("wks.cli._load_similarity_required", mock_load_similarity)

    from wks.cli import main
    result = main(["related", str(test_file), "--min-similarity", "0.7"])

    assert result == 0
    call_kwargs = mock_db.find_similar.call_args[1]
    assert call_kwargs['min_similarity'] == 0.7


def test_related_file_not_found(tmp_path):
    """Test wks0 related with non-existent file."""
    from wks.cli import main
    nonexistent = tmp_path / "nonexistent.txt"

    result = main(["related", str(nonexistent)])
    assert result == 2


def test_related_no_results(tmp_path, monkeypatch, capsys):
    """Test wks0 related when no similar documents are found."""
    test_file = tmp_path / "test.txt"
    test_file.write_text("Test content")

    mock_db = MagicMock()
    mock_db.find_similar.return_value = []
    mock_db.client.close = MagicMock()

    def mock_load_similarity():
        return mock_db, {}

    monkeypatch.setattr("wks.cli._load_similarity_required", mock_load_similarity)

    from wks.cli import main
    result = main(["related", str(test_file)])

    assert result == 0
    captured = capsys.readouterr()
    assert "No similar documents found" in captured.out


def test_related_database_error(tmp_path, monkeypatch):
    """Test wks0 related handles database errors gracefully."""
    test_file = tmp_path / "test.txt"
    test_file.write_text("Test content")

    def mock_load_similarity():
        raise Exception("Database connection failed")

    monkeypatch.setattr("wks.cli._load_similarity_required", mock_load_similarity)

    from wks.cli import main
    result = main(["related", str(test_file)])

    assert result == 2


def test_related_find_similar_error(tmp_path, monkeypatch):
    """Test wks0 related handles find_similar errors gracefully."""
    test_file = tmp_path / "test.txt"
    test_file.write_text("Test content")

    mock_db = MagicMock()
    mock_db.find_similar.side_effect = Exception("Query failed")
    mock_db.client.close = MagicMock()

    def mock_load_similarity():
        return mock_db, {}

    monkeypatch.setattr("wks.cli._load_similarity_required", mock_load_similarity)

    from wks.cli import main
    result = main(["related", str(test_file)])

    assert result == 2
</file>

<file path="tests/test_dbmeta.py">
import mongomock
import pytest

from wks.dbmeta import (
    ensure_db_compat,
    resolve_db_compatibility,
    IncompatibleDatabase,
    SPACE_COMPAT_DEFAULT,
    TIME_COMPAT_DEFAULT,
)


def test_ensure_db_compat_inserts_metadata():
    client = mongomock.MongoClient()
    tag = ensure_db_compat(client, "wks_similarity", "space", SPACE_COMPAT_DEFAULT, product_version="0.0.0")
    assert tag == SPACE_COMPAT_DEFAULT
    doc = client["wks_similarity"]["_wks_meta"].find_one({"_id": "space"})
    assert doc["compat_tag"] == SPACE_COMPAT_DEFAULT
    assert doc["product_version"] == "0.0.0"


def test_ensure_db_compat_detects_mismatch():
    client = mongomock.MongoClient()
    ensure_db_compat(client, "wks_similarity", "space", "space-v1")
    with pytest.raises(IncompatibleDatabase):
        ensure_db_compat(client, "wks_similarity", "space", "space-v2")


def test_resolve_db_compat_defaults():
    cfg = {}
    space_tag, time_tag = resolve_db_compatibility(cfg)
    assert space_tag == SPACE_COMPAT_DEFAULT
    assert time_tag == TIME_COMPAT_DEFAULT


def test_resolve_db_compat_overrides():
    cfg = {"mongo": {"compatibility": {"space": "legacy-space", "time": "legacy-time"}}}
    space_tag, time_tag = resolve_db_compatibility(cfg)
    assert space_tag == "legacy-space"
    assert time_tag == "legacy-time"
</file>

<file path="tests/test_error_messages.py">
"""Tests for error message formatting."""

import pytest
from unittest.mock import patch
import pymongo.errors


def test_mongodb_connection_error(capsys):
    """Test MongoDB connection error message formatting."""
    from wks.error_messages import mongodb_connection_error

    with pytest.raises(SystemExit) as exc_info:
        mongodb_connection_error(
            "mongodb://localhost:27017/",
            pymongo.errors.ServerSelectionTimeoutError("Connection timeout")
        )

    assert exc_info.value.code == 1

    captured = capsys.readouterr()
    assert "MONGODB CONNECTION FAILED" in captured.out
    assert "mongodb://localhost:27017/" in captured.out
    assert "Connection timeout" in captured.out
    assert "brew services start mongodb-community" in captured.out


def test_missing_dependency_error(capsys):
    """Test missing dependency error message formatting."""
    from wks.error_messages import missing_dependency_error

    import_error = ImportError("No module named 'sentence_transformers'")

    with pytest.raises(SystemExit) as exc_info:
        missing_dependency_error("sentence-transformers", import_error)

    assert exc_info.value.code == 1

    captured = capsys.readouterr()
    assert "MISSING DEPENDENCY: sentence-transformers" in captured.out
    assert "pip install -e '.[all]'" in captured.out


def test_file_permission_error(capsys):
    """Test file permission error message formatting."""
    from wks.error_messages import file_permission_error

    perm_error = PermissionError("[Errno 13] Permission denied")

    with pytest.raises(SystemExit) as exc_info:
        file_permission_error("/path/to/file.txt", "read", perm_error)

    assert exc_info.value.code == 1

    captured = capsys.readouterr()
    assert "FILE PERMISSION ERROR" in captured.out
    assert "/path/to/file.txt" in captured.out
    assert "read" in captured.out
    assert "chmod u+rw" in captured.out


def test_vault_path_error(capsys):
    """Test vault path error message formatting."""
    from wks.error_messages import vault_path_error

    with pytest.raises(SystemExit) as exc_info:
        vault_path_error("/nonexistent/vault")

    assert exc_info.value.code == 1

    captured = capsys.readouterr()
    assert "INVALID OBSIDIAN VAULT PATH" in captured.out
    assert "/nonexistent/vault" in captured.out
    assert "vault_path" in captured.out


def test_model_download_error(capsys):
    """Test model download error message formatting."""
    from wks.error_messages import model_download_error

    http_error = Exception("Connection refused")

    with pytest.raises(SystemExit) as exc_info:
        model_download_error("all-MiniLM-L6-v2", http_error)

    assert exc_info.value.code == 1

    captured = capsys.readouterr()
    assert "MODEL DOWNLOAD FAILED" in captured.out
    assert "all-MiniLM-L6-v2" in captured.out
    assert "Connection refused" in captured.out
    assert "huggingface.co" in captured.out
</file>

<file path="tests/test_mongo_retry.py">
"""Tests for MongoDB retry logic."""

import pytest
import pymongo.errors

from wks.mongo_retry import mongo_retry, MongoRetryWrapper


def test_mongo_retry_success_first_attempt():
    """Test that successful operation doesn't retry."""
    call_count = 0

    @mongo_retry(max_attempts=3, delay_secs=0.01)
    def operation():
        nonlocal call_count
        call_count += 1
        return "success"

    result = operation()
    assert result == "success"
    assert call_count == 1


def test_mongo_retry_success_after_failures():
    """Test that operation succeeds after transient failures."""
    call_count = 0

    @mongo_retry(max_attempts=3, delay_secs=0.01)
    def operation():
        nonlocal call_count
        call_count += 1
        if call_count < 3:
            raise pymongo.errors.ConnectionFailure("Transient failure")
        return "success"

    result = operation()
    assert result == "success"
    assert call_count == 3


def test_mongo_retry_max_attempts_exceeded():
    """Test that operation raises after max attempts."""
    call_count = 0

    @mongo_retry(max_attempts=3, delay_secs=0.01)
    def operation():
        nonlocal call_count
        call_count += 1
        raise pymongo.errors.ConnectionFailure("Persistent failure")

    with pytest.raises(pymongo.errors.ConnectionFailure):
        operation()

    assert call_count == 3


def test_mongo_retry_non_retryable_exception():
    """Test that non-retryable exceptions are raised immediately."""
    call_count = 0

    @mongo_retry(max_attempts=3, delay_secs=0.01)
    def operation():
        nonlocal call_count
        call_count += 1
        raise ValueError("Not a MongoDB error")

    with pytest.raises(ValueError):
        operation()

    assert call_count == 1


def test_mongo_retry_wrapper_read_operation():
    """Test that MongoRetryWrapper retries read operations."""

    class MockCollection:
        def __init__(self):
            self.find_one_calls = 0
            self.name = "test_collection"

        def find_one(self, query):
            self.find_one_calls += 1
            if self.find_one_calls < 2:
                raise pymongo.errors.ConnectionFailure("Transient")
            return {"_id": "123", "data": "value"}

    mock_coll = MockCollection()
    wrapped = MongoRetryWrapper(mock_coll, max_attempts=3, delay_secs=0.01)

    result = wrapped.find_one({"_id": "123"})
    assert result == {"_id": "123", "data": "value"}
    assert mock_coll.find_one_calls == 2


def test_mongo_retry_wrapper_write_operation():
    """Test that MongoRetryWrapper does NOT retry write operations."""

    class MockCollection:
        def __init__(self):
            self.insert_one_calls = 0
            self.name = "test_collection"

        def insert_one(self, document):
            self.insert_one_calls += 1
            raise pymongo.errors.ConnectionFailure("Failure")

    mock_coll = MockCollection()
    wrapped = MongoRetryWrapper(mock_coll, max_attempts=3, delay_secs=0.01)

    # Write operations should NOT be retried
    with pytest.raises(pymongo.errors.ConnectionFailure):
        wrapped.insert_one({"data": "value"})

    # Should only be called once (no retry)
    assert mock_coll.insert_one_calls == 1


def test_mongo_retry_backoff():
    """Test that retry delay increases with backoff multiplier."""
    import time

    call_times = []

    @mongo_retry(max_attempts=3, delay_secs=0.1, backoff_multiplier=2.0)
    def operation():
        call_times.append(time.time())
        raise pymongo.errors.ConnectionFailure("Failure")

    with pytest.raises(pymongo.errors.ConnectionFailure):
        operation()

    # Verify exponential backoff
    assert len(call_times) == 3

    # First retry after ~0.1s
    delay1 = call_times[1] - call_times[0]
    assert 0.08 < delay1 < 0.15

    # Second retry after ~0.2s (2.0 * 0.1)
    delay2 = call_times[2] - call_times[1]
    assert 0.18 < delay2 < 0.25
</file>

<file path="tests/test_mongoctl.py">
"""Tests for MongoDB controller helpers."""

import pytest

from wks import mongoctl


def test_is_local_uri_accepts_trailing_slash():
    """Default localhost URIs with trailing slash should count as local."""
    assert mongoctl._is_local_uri("mongodb://localhost:27017/")


def test_is_local_uri_requires_single_loopback_host():
    """Multiple hosts disable local auto-management even if all are loopback."""
    uri = "mongodb://user:pw@127.0.0.1:27017,[::1]:27018/test?replicaSet=rs0"
    assert not mongoctl._is_local_uri(uri)


def test_is_local_uri_rejects_remote_hosts():
    """Remote URIs must not be considered local."""
    assert not mongoctl._is_local_uri("mongodb+srv://cluster0.example.mongodb.net")


def test_local_node_returns_host_and_port():
    """Local node helper should expose host and port for auto-start logic."""
    assert mongoctl._local_node("mongodb://localhost:27017/") == ("localhost", 27017)
    assert mongoctl._local_node("mongodb://127.0.0.1:27018/") == ("127.0.0.1", 27018)
    assert mongoctl._local_node("mongodb://localhost:27017,127.0.0.1:27018/") is None


def test_ensure_mongo_running_does_not_autostart_non_managed(monkeypatch):
    """Ports other than the managed one should not be auto-started."""
    import wks.mongoctl as mongoctl

    monkeypatch.setattr(mongoctl, "mongo_ping", lambda uri, timeout_ms=500: False)
    with pytest.raises(SystemExit):
        mongoctl.ensure_mongo_running("mongodb://localhost:27017/")
</file>

<file path="tests/test_monitor.py">
import unittest
from unittest.mock import MagicMock, patch, mock_open
from pathlib import Path
import os
import time
import json
from datetime import datetime, timedelta

from wks.monitor import WKSFileMonitor, start_monitoring
import wks.monitor
from watchdog.events import FileSystemEvent, DirCreatedEvent, FileCreatedEvent, FileModifiedEvent, FileMovedEvent, FileDeletedEvent, DirDeletedEvent, DirModifiedEvent, DirMovedEvent

class TestWKSFileMonitor(unittest.TestCase):
    def setUp(self):
        self.temp_dir = Path('/tmp/wks_test_monitor')
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        self.state_file = self.temp_dir / 'state.json'
        self.on_change_mock = MagicMock()

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir)

    def test_initialization(self):
        monitor = WKSFileMonitor(self.state_file)
        self.assertEqual(monitor.state_file, self.state_file)
        self.assertIsNotNone(monitor.ignore_patterns)
        self.assertIsNotNone(monitor.ignore_dirs)

    def test_load_state_no_file(self):
        monitor = WKSFileMonitor(self.state_file)
        self.assertEqual(monitor.state, {"files": {}, "last_update": None})

    def test_load_state_with_file(self):
        state = {"files": {"/tmp/wks_test_monitor/a.txt": {}}, "last_update": "2025-01-01T00:00:00"}
        with open(self.state_file, 'w') as f:
            json.dump(state, f)
        monitor = WKSFileMonitor(self.state_file)
        self.assertEqual(monitor.state, state)

    def test_load_state_corrupted_file(self):
        with open(self.state_file, 'w') as f:
            f.write("corrupted")
        monitor = WKSFileMonitor(self.state_file)
        self.assertEqual(monitor.state, {"files": {}, "last_update": None})
        self.assertTrue((self.state_file.with_suffix('.json.backup')).exists())

    def test_save_state(self):
        monitor = WKSFileMonitor(self.state_file)
        monitor.state = {"files": {"/tmp/wks_test_monitor/a.txt": {}}, "last_update": None}
        monitor._save_state()
        with open(self.state_file, 'r') as f:
            state = json.load(f)
        self.assertIn("last_update", state)
        self.assertIsNotNone(state["last_update"])

    def test_should_ignore(self):
        monitor = WKSFileMonitor(self.state_file, ignore_globs=['*.log'])
        self.assertTrue(monitor._should_ignore('/tmp/wks_test_monitor/.git/hooks'))
        self.assertTrue(monitor._should_ignore('/tmp/wks_test_monitor/node_modules/lib'))
        self.assertTrue(monitor._should_ignore('/tmp/wks_test_monitor/test.log'))
        self.assertFalse(monitor._should_ignore('/tmp/wks_test_monitor/a.txt'))

    def test_track_change(self):
        monitor = WKSFileMonitor(self.state_file, on_change=self.on_change_mock)
        file_path = self.temp_dir / 'a.txt'
        file_path.touch()
        monitor._track_change('created', str(file_path))
        self.assertIn(str(file_path.resolve()), monitor.state['files'])
        self.on_change_mock.assert_called_with('created', str(file_path.resolve()))

    def test_on_created(self):
        monitor = WKSFileMonitor(self.state_file, on_change=self.on_change_mock)
        file_path = self.temp_dir / 'a.txt'
        file_path.touch()
        event = FileCreatedEvent(str(file_path))
        monitor.on_created(event)
        self.assertIn(str(file_path.resolve()), monitor.state['files'])
        self.on_change_mock.assert_called_with('created', str(file_path.resolve()))

    def test_on_modified(self):
        monitor = WKSFileMonitor(self.state_file, on_change=self.on_change_mock)
        file_path = self.temp_dir / 'a.txt'
        file_path.touch()
        monitor._track_change('created', str(file_path))
        event = FileModifiedEvent(str(file_path))
        monitor.on_modified(event)
        self.assertEqual(len(monitor.state['files'][str(file_path.resolve())]['modifications']), 2)
        self.on_change_mock.assert_called_with('modified', str(file_path.resolve()))

    def test_on_moved(self):
        monitor = WKSFileMonitor(self.state_file, on_change=self.on_change_mock)
        src_path = self.temp_dir / 'a.txt'
        src_path.touch()
        monitor._track_change('created', str(src_path))
        dest_path = self.temp_dir / 'b.txt'
        event = FileMovedEvent(str(src_path), str(dest_path))
        monitor.on_moved(event)
        self.assertIn(str(dest_path.resolve()), monitor.state['files'])
        self.on_change_mock.assert_called_with('moved', (str(src_path.resolve()), str(dest_path.resolve())))

    def test_on_deleted(self):
        monitor = WKSFileMonitor(self.state_file, on_change=self.on_change_mock)
        file_path = self.temp_dir / 'a.txt'
        file_path.touch()
        monitor._track_change('created', str(file_path))
        event = FileDeletedEvent(str(file_path))
        monitor.on_deleted(event)
        self.assertEqual(len(monitor.state['files'][str(file_path.resolve())]['modifications']), 2)
        self.on_change_mock.assert_called_with('deleted', str(file_path.resolve()))

    def test_get_recent_changes(self):
        monitor = WKSFileMonitor(self.state_file)
        file_path = self.temp_dir / 'a.txt'
        file_path.touch()
        monitor._track_change('created', str(file_path))
        time.sleep(0.1)
        recent_changes = monitor.get_recent_changes(hours=1)
        self.assertIn(str(file_path.resolve()), recent_changes)

class TestStartMonitoring(unittest.TestCase):
    def setUp(self):
        self.temp_dir = Path('/tmp/wks_test_monitor_start')
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        self.state_file = self.temp_dir / 'state.json'

    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir)

    @patch('wks.monitor.PollingObserver')
    @patch('wks.monitor.KqueueObserver')
    @patch('wks.monitor.FSEventsObserver')
    @patch('wks.monitor.Observer')
    def test_start_monitoring(self, mock_observer, mock_fsevents, mock_kqueue, mock_polling):
        # Make all observers available
        wks.monitor.FSEventsObserver = mock_fsevents
        wks.monitor.KqueueObserver = mock_kqueue
        wks.monitor.PollingObserver = mock_polling
        wks.monitor.Observer = mock_observer
        
        mock_observer_instance = mock_observer.return_value
        mock_fsevents_instance = mock_fsevents.return_value
        mock_kqueue_instance = mock_kqueue.return_value
        mock_polling_instance = mock_polling.return_value

        observer = start_monitoring([self.temp_dir], self.state_file)
        self.assertIsNotNone(observer)

        # Check that one of the observers was used
        scheduled = False
        for instance in [mock_observer_instance, mock_fsevents_instance, mock_kqueue_instance, mock_polling_instance]:
            if instance.schedule.called:
                scheduled = True
                break
        self.assertTrue(scheduled)

        started = False
        for instance in [mock_observer_instance, mock_fsevents_instance, mock_kqueue_instance, mock_polling_instance]:
            if instance.start.called:
                started = True
                break
        self.assertTrue(started)


if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_phase1.py">
"""Test Phase 1: Display infrastructure, priority calculation, config migration."""

import pytest
from pathlib import Path

from wks.display.base import Display
from wks.display.cli import CLIDisplay
from wks.display.mcp import MCPDisplay
from wks.display.context import get_display, is_mcp_context
from wks.priority import calculate_priority, find_managed_directory, priority_examples
from wks.config_schema import is_old_config, migrate_config, validate_config


class TestDisplayInfrastructure:
    """Test display system."""

    def test_get_display_cli(self):
        """Test CLI display instantiation."""
        display = get_display("cli")
        assert isinstance(display, CLIDisplay)

    def test_get_display_mcp(self):
        """Test MCP display instantiation."""
        display = get_display("mcp")
        assert isinstance(display, MCPDisplay)

    def test_display_base_interface(self):
        """Test that both displays implement Display interface."""
        cli = get_display("cli")
        mcp = get_display("mcp")

        # Check key methods exist
        assert hasattr(cli, "status")
        assert hasattr(cli, "success")
        assert hasattr(cli, "error")
        assert hasattr(cli, "table")
        assert hasattr(cli, "progress_start")

        assert hasattr(mcp, "status")
        assert hasattr(mcp, "success")
        assert hasattr(mcp, "error")
        assert hasattr(mcp, "table")
        assert hasattr(mcp, "progress_start")


class TestPriorityCalculation:
    """Test priority calculation algorithm."""

    def test_find_managed_directory(self):
        """Test finding managed directory for a path."""
        managed_dirs = {
            "~/Desktop": 150,
            "~/Documents": 100,
            "~": 100,
        }

        home = Path.home()

        # Test exact match
        matched, priority = find_managed_directory(
            home / "Desktop",
            managed_dirs
        )
        assert priority == 150

        # Test nested path
        matched, priority = find_managed_directory(
            home / "Documents/2025-Project",
            managed_dirs
        )
        assert priority == 100

        # Test deepest match wins
        matched, priority = find_managed_directory(
            home / "other/path",
            managed_dirs
        )
        assert priority == 100  # Matches ~

    def test_priority_examples_from_spec(self):
        """Test that all SPEC.md examples pass."""
        results = priority_examples()

        for result in results:
            assert result["match"], (
                f"Priority mismatch for {result['path']}: "
                f"expected {result['expected']}, got {result['calculated']}"
            )

    def test_calculate_priority_basic(self):
        """Test basic priority calculation."""
        managed_dirs = {"~": 100}
        priority_config = {
            "depth_multiplier": 0.9,
            "extension_weights": {"default": 1.0}
        }

        home = Path.home()

        # File at home level
        priority = calculate_priority(
            home / "file.txt",
            managed_dirs,
            priority_config
        )
        assert priority == 100

        # File one level deep
        priority = calculate_priority(
            home / "subdir/file.txt",
            managed_dirs,
            priority_config
        )
        assert priority == 90  # 100 * 0.9


class TestConfigMigration:
    """Test config schema and migration."""

    def test_is_old_config_detection(self):
        """Test detecting old config format."""
        old_config = {
            "vault_path": "~/obsidian",
            "obsidian": {"base_dir": "WKS"},
            "similarity": {"enabled": True},
        }
        assert is_old_config(old_config) is True

        new_config = {
            "monitor": {"managed_directories": {"~": 100}},
            "vault": {"base_dir": "~/obsidian"},
            "related": {"engines": {}},
        }
        assert is_old_config(new_config) is False

    def test_migrate_config_structure(self):
        """Test config migration produces valid structure."""
        old_config = {
            "vault_path": "~/obsidian",
            "obsidian": {"base_dir": "WKS"},
            "similarity": {
                "enabled": True,
                "model": "all-MiniLM-L6-v2",
            },
            "monitor": {
                "include_paths": ["~"],
            },
            "mongo": {
                "uri": "mongodb://localhost:27017/",
            }
        }

        new_config = migrate_config(old_config)

        # Check new sections exist
        assert "monitor" in new_config
        assert "vault" in new_config
        assert "db" in new_config
        assert "related" in new_config
        assert "extract" in new_config
        assert "diff" in new_config
        assert "index" in new_config
        assert "search" in new_config

        # Check monitor has new fields
        assert "managed_directories" in new_config["monitor"]
        assert "priority" in new_config["monitor"]

        # Check vault converted from obsidian
        assert new_config["vault"]["base_dir"] == "~/obsidian"
        assert new_config["vault"]["wks_dir"] == "WKS"

        # Check related converted from similarity
        assert "engines" in new_config["related"]
        assert "embedding" in new_config["related"]["engines"]
        assert new_config["related"]["engines"]["embedding"]["model"] == "all-MiniLM-L6-v2"

    def test_validate_config(self):
        """Test config validation."""
        # Valid config
        valid_config = {
            "monitor": {
                "managed_directories": {"~": 100},
                "priority": {"depth_multiplier": 0.9},
                "database": "wks_monitor",
            },
            "vault": {
                "base_dir": "~/obsidian",
                "database": "wks_vault",
            },
            "db": {
                "uri": "mongodb://localhost:27017/",
            },
        }
        is_valid, errors = validate_config(valid_config)
        assert is_valid is True
        assert len(errors) == 0

        # Invalid config (missing sections)
        invalid_config = {
            "monitor": {},
        }
        is_valid, errors = validate_config(invalid_config)
        assert is_valid is False
        assert len(errors) > 0


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
</file>

<file path="tests/test_smoke.py">
"""
Smoke tests matching AGENTS.md requirements.

These tests verify core Space DB operations as specified in the
"Required Smoke Tests (Space DB)" section of AGENTS.md.
"""

import os
from pathlib import Path

import mongomock
import pytest


@pytest.fixture(autouse=True)
def patch_mongo(monkeypatch):
    """Patch MongoClient to use in-memory mongomock."""
    monkeypatch.setattr('wks.similarity.MongoClient', mongomock.MongoClient)


@pytest.fixture
def patch_model(monkeypatch):
    """Patch SentenceTransformer with dummy model."""
    class DummyModel:
        def __init__(self, *args, **kwargs):
            pass

        def encode(self, text, **kwargs):
            import hashlib
            h = hashlib.sha256(text.encode('utf-8')).digest()
            return [b / 255.0 for b in h[:8]]

    monkeypatch.setattr('wks.similarity.SentenceTransformer', DummyModel)


def test_smoke_index_new_file(tmp_path, patch_model):
    """
    Index new file ‚Üí wks0 db info -n 5 shows it.

    AGENTS.md requirement:
    "Index new file: `wks0 --display rich index ~/test/file.txt` ‚Üí
    `wks0 --display rich db info -n 5` shows it."
    """
    from wks.similarity import SimilarityDB

    # Create test file
    test_file = tmp_path / "test.txt"
    test_file.write_text("This is a test document for indexing." * 10)

    # Build DB and index file
    db = SimilarityDB(
        database_name='smoke_test_db',
        collection_name='smoke_test_coll',
        mongo_uri='mongodb://localhost:27017/',
        model_name='dummy',
        extract_engine='builtin',
    )

    # Index the file
    success = db.add_file(test_file)
    assert success is True

    # Verify it appears in stats
    stats = db.get_stats()
    assert stats['total_files'] == 1

    # Verify it's in the collection
    doc = db.collection.find_one({'path': test_file.resolve().as_uri()})
    assert doc is not None
    assert doc['filename'] == 'test.txt'


def test_smoke_reindex_unchanged_skipped(tmp_path, patch_model):
    """
    Re-index unchanged file: reports skipped; totals stable.

    AGENTS.md requirement:
    "Re‚Äëindex unchanged file: reports skipped; totals stable."
    """
    from wks.similarity import SimilarityDB

    # Create test file
    test_file = tmp_path / "test.txt"
    test_file.write_text("Unchanged content for reindex test." * 10)

    # Build DB
    db = SimilarityDB(
        database_name='smoke_reindex_db',
        collection_name='smoke_reindex_coll',
        mongo_uri='mongodb://localhost:27017/',
        model_name='dummy',
        extract_engine='builtin',
    )

    # First index
    success = db.add_file(test_file)
    assert success is True

    stats_before = db.get_stats()
    assert stats_before['total_files'] == 1

    # Re-index without changes
    success_reindex = db.add_file(test_file)
    assert success_reindex is False  # Should report unchanged

    # Verify totals remain stable
    stats_after = db.get_stats()
    assert stats_after['total_files'] == 1
    assert stats_after['total_files'] == stats_before['total_files']


def test_smoke_file_move_daemon_running(tmp_path, patch_model):
    """
    File move (daemon running): move file ‚Üí totals unchanged;
    single logical entry remains (path updated in place).

    AGENTS.md requirement:
    "File move (daemon running): move file ‚Üí totals unchanged;
    single logical entry remains (path updated in place)."
    """
    from wks.similarity import SimilarityDB

    # Create test file
    src = tmp_path / 'src' / 'file.txt'
    src.parent.mkdir(parents=True, exist_ok=True)
    src.write_text("Content for move test." * 10)

    # Build DB
    db = SimilarityDB(
        database_name='smoke_move_db',
        collection_name='smoke_move_coll',
        mongo_uri='mongodb://localhost:27017/',
        model_name='dummy',
        extract_engine='builtin',
    )

    # Index original file
    db.add_file(src)
    stats_before = db.get_stats()
    assert stats_before['total_files'] == 1

    # Move file physically
    dst = tmp_path / 'dst' / 'file.txt'
    dst.parent.mkdir(parents=True, exist_ok=True)
    os.rename(src, dst)

    # Simulate daemon move handling
    db.rename_file(src, dst)

    # Verify totals unchanged
    stats_after = db.get_stats()
    assert stats_after['total_files'] == 1

    # Verify path updated in place (no duplicate)
    old_doc = db.collection.find_one({'path': src.resolve().as_uri()})
    assert old_doc is None

    new_doc = db.collection.find_one({'path': dst.resolve().as_uri()})
    assert new_doc is not None
    assert new_doc['filename'] == 'file.txt'


def test_smoke_directory_move_daemon_running(tmp_path, patch_model):
    """
    Directory move (daemon running): move folder with files ‚Üí
    totals unchanged; descendants updated in place.

    AGENTS.md requirement:
    "Directory move (daemon running): move folder with files ‚Üí
    totals unchanged; descendants updated in place."
    """
    from wks.similarity import SimilarityDB

    # Create test directory with files
    base = tmp_path / 'dir'
    base.mkdir(parents=True, exist_ok=True)

    file1 = base / 'a.txt'
    file2 = base / 'sub' / 'b.txt'

    file1.write_text("File one content." * 10)
    file2.parent.mkdir(parents=True, exist_ok=True)
    file2.write_text("File two content." * 10)

    # Build DB
    db = SimilarityDB(
        database_name='smoke_dirmove_db',
        collection_name='smoke_dirmove_coll',
        mongo_uri='mongodb://localhost:27017/',
        model_name='dummy',
        extract_engine='builtin',
    )

    # Index files
    db.add_file(file1)
    db.add_file(file2)

    stats_before = db.get_stats()
    assert stats_before['total_files'] == 2

    # Move directory physically
    new_base = tmp_path / 'dir-moved'
    os.rename(base, new_base)

    # Simulate daemon directory move handling
    updated = db.rename_folder(base, new_base)
    assert updated == 2

    # Verify totals unchanged
    stats_after = db.get_stats()
    assert stats_after['total_files'] == 2

    # Verify descendants updated in place
    new_file1 = new_base / 'a.txt'
    new_file2 = new_base / 'sub' / 'b.txt'

    doc1 = db.collection.find_one({'path': new_file1.resolve().as_uri()})
    doc2 = db.collection.find_one({'path': new_file2.resolve().as_uri()})

    assert doc1 is not None
    assert doc2 is not None

    # Verify old paths gone
    old_doc1 = db.collection.find_one({'path': file1.resolve().as_uri()})
    old_doc2 = db.collection.find_one({'path': file2.resolve().as_uri()})

    assert old_doc1 is None
    assert old_doc2 is None
</file>

<file path="wks/cli/commands/__init__.py">
"""CLI command modules."""
</file>

<file path="wks/cli/commands/related.py">
"""Related command - find semantically similar documents (Semantic Engines layer)."""

import argparse
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import unquote, urlparse

from .index import load_similarity_required


def parse_related_query_path(path_str: str, display: Any) -> Optional[Path]:
    """Parse and validate query path for related command."""
    query_path = Path(path_str).expanduser().resolve()
    if not query_path.exists():
        display.error(f"File not found: {query_path}")
        return None
    return query_path


def load_similarity_for_related(display: Any) -> Tuple[Any, int]:
    """Load similarity database for related command.

    Returns:
        Tuple of (db, exit_code) where exit_code is 0 on success
    """
    display.status("Loading similarity database...")
    try:
        db, _ = load_similarity_required()
        display.success("Connected to database")
        return db, 0
    except SystemExit as e:
        return None, (e.code if isinstance(e.code, int) else 1)
    except Exception as e:
        display.error(f"Error loading similarity database: {e}")
        return None, 2


def find_similar_documents(db: Any, query_path: Path, limit: int, min_similarity: float, display: Any) -> Tuple[List[Tuple[str, float]], int]:
    """Find similar documents.

    Returns:
        Tuple of (results, exit_code)
    """
    display.status(f"Finding similar documents to: {query_path.name}")
    try:
        results = db.find_similar(
            query_path=query_path,
            limit=limit,
            min_similarity=min_similarity,
            mode="file"
        )
        return results, 0
    except Exception as e:
        display.error(f"Error finding similar documents: {e}")
        return [], 2
    finally:
        try:
            db.client.close()
        except Exception:
            pass


def uri_to_path(path_uri: str) -> Path:
    """Convert file:// URI to Path."""
    if path_uri.startswith("file://"):
        parsed = urlparse(path_uri)
        return Path(unquote(parsed.path or ""))
    return Path(path_uri)


def format_related_results_json(results: List[Tuple[str, float]]) -> List[Dict[str, Any]]:
    """Format related results as JSON."""
    output = []
    for path_uri, similarity in results:
        display_path = uri_to_path(path_uri)
        output.append({
            "path": str(display_path),
            "similarity": round(similarity, 3)
        })
    return output


def format_related_results_table(results: List[Tuple[str, float]], query_path: Path) -> List[Dict[str, str]]:
    """Format related results as table data."""
    table_data = []
    for path_uri, similarity in results:
        display_path = uri_to_path(path_uri)
        sim_pct = similarity * 100
        table_data.append({
            "Similarity": f"{sim_pct:5.1f}%",
            "Path": str(display_path)
        })
    return table_data


def related_cmd(args: argparse.Namespace) -> int:
    """Find semantically similar documents."""
    display = args.display_obj

    # Parse input path
    query_path = parse_related_query_path(args.path, display)
    if query_path is None:
        return 2

    # Load similarity DB
    db, exit_code = load_similarity_for_related(display)
    if db is None:
        return exit_code

    # Find similar documents
    results, exit_code = find_similar_documents(db, query_path, args.limit, args.min_similarity, display)
    if exit_code != 0:
        return exit_code

    # Format output
    if args.format == "json" or args.display == "mcp":
        output = format_related_results_json(results)
        display.json_output(output)
    else:
        if not results:
            display.info(f"No similar documents found for: {query_path}")
            return 0
        table_data = format_related_results_table(results, query_path)
        display.table(table_data, title=f"Similar to: {query_path}")

    return 0


def setup_related_parser(subparsers) -> None:
    """Setup related command parser."""
    rel = subparsers.add_parser("related", help="Find semantically similar documents")
    rel.add_argument("path", help="Reference file to find similar documents for")
    rel.add_argument("--limit", type=int, default=10, help="Maximum number of results (default: 10)")
    rel.add_argument("--min-similarity", type=float, default=0.0, help="Minimum similarity threshold 0.0-1.0 (default: 0.0)")
    rel.add_argument("--format", choices=["table", "json"], default="table", help="Output format (default: table)")
    rel.set_defaults(func=related_cmd)
</file>

<file path="wks/cli/commands/service.py">
"""Service management commands (daemon start/stop/status/install/uninstall)."""

import argparse
import os
import platform
import signal
import subprocess
import sys
import time
from pathlib import Path

from ...constants import WKS_HOME_EXT
from ...display.context import get_display
from ...service_controller import (
    LOCK_FILE,
    ServiceController,
    ServiceStatusData,
    agent_installed,
    daemon_start_launchd,
    daemon_status_launchd,
    daemon_stop_launchd,
    default_mongo_uri,
    is_macos,
    stop_managed_mongo,
)
from ..display_strategies import get_display_strategy
from ..helpers import maybe_write_json
from ... import mongoctl


# Launchd helpers
def _agent_label() -> str:
    """Unique launchd label bound to the new CLI name."""
    return "com.wieselquist.wks0"


def _agent_plist_path() -> Path:
    """Path to launchd plist file."""
    return Path.home() / "Library" / "LaunchAgents" / f"{_agent_label()}.plist"


def _launchctl_quiet(*args: str) -> int:
    """Run launchctl command quietly."""
    try:
        return subprocess.call(["launchctl", *args], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except FileNotFoundError:
        print("launchctl not found; macOS only")
        return 2


def _plist_path() -> Path:
    """Path to plist file (alias for _agent_plist_path for compatibility)."""
    return _agent_plist_path()


# Service command implementations
def daemon_status(args: argparse.Namespace) -> int:
    """Show daemon status using appropriate display strategy."""
    try:
        strategy = get_display_strategy(args)
    except ValueError as e:
        return 2

    # Live mode requires CLI display
    if getattr(args, "live", False):
        args.display = "cli"
        args.display_obj = get_display("cli")

    status = None
    try:
        status = ServiceController.get_status()
    except Exception as exc:
        display = getattr(args, "display_obj", None)
        if display:
            display.error("Failed to gather service status", details=str(exc))
        else:
            sys.stderr.write(f"Failed to gather service status: {exc}\n")
        return 2

    if status is None:
        return 2

    payload = status.to_dict()
    maybe_write_json(args, payload)

    return strategy.render(status, args)


def daemon_start(_: argparse.Namespace):
    """Start daemon in background or via launchd if installed."""
    mongoctl.ensure_mongo_running(default_mongo_uri(), record_start=True)
    if is_macos() and agent_installed():
        daemon_start_launchd()
        return
    # Start as background process: python -m wks.daemon
    env = os.environ.copy()
    python = sys.executable
    log_dir = Path.home() / WKS_HOME_EXT
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / "daemon.log"
    # Prefer running against the local source tree when available
    try:
        proj_root = Path(__file__).resolve().parents[2]
        env["PYTHONPATH"] = f"{proj_root}:{env.get('PYTHONPATH', '')}"
        workdir = str(proj_root)
    except Exception:
        workdir = None
    with open(log_file, "ab", buffering=0) as lf:
        # Detach: create a new session
        kwargs = {
            "stdout": lf,
            "stderr": lf,
            "stdin": subprocess.DEVNULL,
            "start_new_session": True,
            "env": env,
            **({"cwd": workdir} if workdir else {}),
        }
        try:
            p = subprocess.Popen([python, "-m", "wks.daemon"], **kwargs)
        except Exception as e:
            print(f"Failed to start daemon: {e}")
            sys.exit(1)
    print(f"WKS daemon started (PID {p.pid}). Log: {log_file}")


def _daemon_stop_core(stop_mongo: bool = True) -> None:
    """Core daemon stop logic."""
    if is_macos() and agent_installed():
        daemon_stop_launchd()
        if stop_mongo:
            stop_managed_mongo()
        return
    if not LOCK_FILE.exists():
        print("WKS daemon is not running")
        if stop_mongo:
            stop_managed_mongo()
        return
    try:
        pid_line = LOCK_FILE.read_text().strip().splitlines()[0]
        pid = int(pid_line)
    except Exception:
        print("Could not read PID from lock; try killing manually")
        return
    try:
        os.kill(pid, signal.SIGTERM)
        print(f"Sent SIGTERM to PID {pid}")
    except Exception as e:
        print(f"Failed to send SIGTERM to PID {pid}: {e}")
    finally:
        if stop_mongo:
            stop_managed_mongo()


def daemon_stop(_: argparse.Namespace):
    """Stop daemon."""
    _daemon_stop_core(stop_mongo=True)


def daemon_restart(args: argparse.Namespace):
    """Restart daemon."""
    # macOS launchd-managed restart
    if is_macos() and agent_installed():
        try:
            mongoctl.ensure_mongo_running(default_mongo_uri(), record_start=True)
        except Exception:
            pass
        try:
            daemon_stop_launchd()
        except Exception:
            pass
        time.sleep(0.5)
        daemon_start_launchd()
        return

    # Fallback: stop/start without touching databases
    try:
        _daemon_stop_core(stop_mongo=False)
    except Exception:
        pass
    time.sleep(0.5)
    try:
        mongoctl.ensure_mongo_running(default_mongo_uri(), record_start=True)
    except Exception:
        pass
    daemon_start(args)


def daemon_install(args: argparse.Namespace):
    """Install launchd agent (macOS)."""
    if platform.system() != "Darwin":
        print("install is macOS-only (launchd)")
        return
    pl = _plist_path()
    pl.parent.mkdir(parents=True, exist_ok=True)
    log_dir = Path.home() / WKS_HOME_EXT
    log_dir.mkdir(exist_ok=True)
    # Use the current interpreter (works for system Python, venv, and pipx)
    python = sys.executable
    proj_root = Path(__file__).resolve().parents[2]
    xml = f"""
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
  <key>Label</key>
  <string>com.wieselquist.wks0</string>
  <key>LimitLoadToSessionType</key>
  <string>Aqua</string>
  <key>ProgramArguments</key>
  <array>
    <string>{python}</string>
    <string>-m</string>
    <string>wks.daemon</string>
  </array>
  <key>WorkingDirectory</key>
  <string>{proj_root}</string>
  <key>EnvironmentVariables</key>
  <dict>
    <key>PYTHONPATH</key>
    <string>{proj_root}</string>
    <key>TOKENIZERS_PARALLELISM</key>
    <string>false</string>
  </dict>
  <key>RunAtLoad</key>
  <true/>
  <key>KeepAlive</key>
  <true/>
  <key>StandardOutPath</key>
  <string>{log_dir}/daemon.log</string>
  <key>StandardErrorPath</key>
  <string>{log_dir}/daemon.error.log</string>
 </dict>
</plist>
""".strip()
    pl.write_text(xml)
    uid = os.getuid()
    from ..helpers import make_progress
    with make_progress(total=6, display=args.display) as prog:
        prog.update("ensure mongo")
        mongoctl.ensure_mongo_running(default_mongo_uri(), record_start=True)
        prog.update("bootout legacy")
        # Only bootout immediate predecessor if it exists
        old_plist = Path.home() / "Library" / "LaunchAgents" / "com.wieselquist.wks0.plist"
        if old_plist.exists():
            _launchctl_quiet("bootout", f"gui/{uid}", str(old_plist))
        prog.update("bootstrap")
        _launchctl_quiet("bootstrap", f"gui/{uid}", str(pl))
        prog.update("enable")
        _launchctl_quiet("enable", f"gui/{uid}/com.wieselquist.wks0")
        prog.update("kickstart")
        _launchctl_quiet("kickstart", "-k", f"gui/{uid}/com.wieselquist.wks0")
        prog.update("done")
    print(f"Installed and started: {pl}")


def daemon_uninstall(args: argparse.Namespace):
    """Uninstall launchd agent (macOS)."""
    if platform.system() != "Darwin":
        print("uninstall is macOS-only (launchd)")
        return
    pl = _plist_path()
    uid = os.getuid()
    from ..helpers import make_progress
    # Only clean up current label and immediate predecessor
    labels_to_remove = [
        "com.wieselquist.wks0",  # Current
        "com.wieselquist.wks0",  # Immediate predecessor
    ]
    with make_progress(total=len(labels_to_remove) * 2, display=args.display) as prog:
        for label in labels_to_remove:
            plist_path = Path.home() / "Library" / "LaunchAgents" / (label + ".plist")
            prog.update(f"bootout {label}")
            _launchctl_quiet("bootout", f"gui/{uid}", str(plist_path))
            prog.update(f"remove {label}.plist")
            if plist_path.exists():
                plist_path.unlink()
    stop_managed_mongo()
    print("Uninstalled.")


def _stop_service_for_reset(args: argparse.Namespace) -> None:
    """Stop current service agent."""
    try:
        if platform.system() == "Darwin":
            _launchctl_quiet("bootout", f"gui/{os.getuid()}", str(_plist_path()))
        else:
            daemon_stop(args)
    except Exception:
        pass


def _clear_local_agent_state() -> None:
    """Clear local agent state files."""
    try:
        home = Path.home()
        for name in [
            'file_ops.jsonl', 'monitor_state.json', 'activity_state.json', 'health.json',
            'daemon.lock', 'daemon.log', 'daemon.error.log'
        ]:
            p = home / WKS_HOME_EXT / name
            try:
                if p.exists():
                    p.unlink()
            except Exception:
                pass
    except Exception:
        pass


def _start_service_for_reset(args: argparse.Namespace) -> None:
    """Start service again after reset."""
    try:
        if platform.system() == "Darwin":
            pl = _plist_path()
            _launchctl_quiet("bootstrap", f"gui/{os.getuid()}", str(pl))
            _launchctl_quiet("enable", f"gui/{os.getuid()}/com.wieselquist.wks0")
            _launchctl_quiet("kickstart", "-k", f"gui/{os.getuid()}/com.wieselquist.wks0")
        else:
            daemon_start(args)
    except Exception:
        pass


def service_reset(args: argparse.Namespace) -> int:
    """Reset service: stop, clear state, restart."""
    _stop_service_for_reset(args)
    stop_managed_mongo()

    try:
        mongoctl.ensure_mongo_running(default_mongo_uri(), record_start=True)
    except SystemExit:
        return 2

    _clear_local_agent_state()
    _start_service_for_reset(args)

    try:
        daemon_status(args)
    except Exception:
        pass

    return 0


def setup_service_parser(subparsers) -> None:
    """Setup service command parser."""
    svc = subparsers.add_parser("service", help="Install/start/stop the WKS daemon (macOS)")
    svcsub = svc.add_subparsers(dest="svc_cmd")

    svcinst = svcsub.add_parser("install", help="Install launchd agent (macOS)")
    svcinst.set_defaults(func=daemon_install)

    svcrem = svcsub.add_parser("uninstall", help="Uninstall launchd agent (macOS)")
    svcrem.set_defaults(func=daemon_uninstall)

    svcstart = svcsub.add_parser("start", help="Start daemon in background or via launchd if installed")
    svcstart.set_defaults(func=daemon_start)

    svcstop = svcsub.add_parser("stop", help="Stop daemon")
    svcstop.set_defaults(func=daemon_stop)

    svcstatus = svcsub.add_parser("status", help="Daemon status")
    svcstatus.add_argument(
        "--live",
        action="store_true",
        help="Keep display updated automatically (refreshes every 2 seconds)"
    )
    svcstatus.set_defaults(func=daemon_status)

    svcrestart = svcsub.add_parser("restart", help="Restart daemon")
    svcrestart.set_defaults(func=daemon_restart)

    svcreset = svcsub.add_parser("reset", help="Stop service, reset databases/state, and start service cleanly")
    svcreset.set_defaults(func=service_reset)
</file>

<file path="wks/cli/__init__.py">
"""WKS CLI package - organized command structure."""

from .main import main
from ..config import load_config

__all__ = ["main", "load_config"]
</file>

<file path="wks/cli/constants.py">
"""Constants for CLI commands."""

from ..constants import WKS_HOME_EXT, WKS_EXTRACT_EXT, WKS_HOME_DISPLAY

DEFAULT_MONITOR_INCLUDE_PATHS = ["~"]
DEFAULT_MONITOR_EXCLUDE_PATHS = ["~/Library", "~/obsidian", f"{WKS_HOME_DISPLAY}"]
DEFAULT_MONITOR_IGNORE_DIRS = [".git", "_build", WKS_HOME_EXT, WKS_EXTRACT_EXT]
DEFAULT_MONITOR_IGNORE_GLOBS = ["*.tmp", "*~", "._*"]

DEFAULT_OBSIDIAN_CONFIG = {
    "base_dir": "WKS",
    "log_max_entries": 500,
    "active_files_max_rows": 50,
    "source_max_chars": 40,
    "destination_max_chars": 40,
    "docs_keep": 99,
}

DEFAULT_SIMILARITY_EXTS = [
    ".md",
    ".txt",
    ".py",
    ".ipynb",
    ".tex",
    ".docx",
    ".pptx",
    ".pdf",
    ".html",
    ".csv",
    ".xlsx",
]

# Supported display modes
DISPLAY_CHOICES = ["cli", "mcp"]

DB_QUERY_MARKDOWN_TEMPLATE = """### {{ scope|capitalize }} query ‚Äî {{ collection }}
{% if rows %}
| # | Document |
| --- | --- |
{% for row in rows %}
| {{ loop.index }} | {{ row | replace('|', '\\|') }} |
{% endfor %}
{% else %}
_No documents found._
{% endif %}
""".strip()
</file>

<file path="wks/cli/dataclasses.py">
"""Data classes for CLI command results."""

from __future__ import annotations

from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional


@dataclass
class FileTimings:
    """Timing information for file processing stages."""
    hash: Optional[float] = None
    extract: Optional[float] = None
    embed: Optional[float] = None
    db: Optional[float] = None
    chunks: Optional[float] = None
    obsidian: Optional[float] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dict for JSON serialization."""
        return asdict(self)


@dataclass
class FileSummary:
    """Summary of file processing result."""
    path: Path
    status: str
    timings: FileTimings = field(default_factory=FileTimings)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dict for JSON serialization."""
        return {
            "path": str(self.path),
            "status": self.status,
            "timings": self.timings.to_dict(),
        }


@dataclass
class DatabaseSummary:
    """Database summary information."""
    database: str
    collection: str
    total_files: int
    total_bytes: Optional[int] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dict for JSON serialization."""
        result = {
            "database": self.database,
            "collection": self.collection,
            "total_files": self.total_files,
        }
        if self.total_bytes is not None:
            result["total_bytes"] = self.total_bytes
        return result


@dataclass
class IndexResult:
    """Result of an index operation."""
    mode: str
    requested: List[str]
    added: int
    skipped: int
    errors: int
    files: List[FileSummary]
    database: Optional[DatabaseSummary] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dict for JSON serialization."""
        result = {
            "mode": self.mode,
            "requested": self.requested,
            "added": self.added,
            "skipped": self.skipped,
            "errors": self.errors,
            "files": [f.to_dict() for f in self.files],
        }
        if self.database:
            result["database"] = self.database.to_dict()
        return result
</file>

<file path="wks/cli/main.py">
"""Main CLI entry point - sets up argument parser and routes commands."""

import argparse
import subprocess
from pathlib import Path
from typing import List, Optional

from ..display.context import add_display_argument, get_display
from ..utils import get_package_version
from .commands.config import show_config
from .commands.index import setup_index_parser
from .commands.monitor import setup_monitor_parser
from .commands.related import setup_related_parser
from .commands.service import setup_service_parser


def main(argv: Optional[List[str]] = None) -> int:
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(prog="wks0", description="WKS management CLI")
    sub = parser.add_subparsers(dest="cmd")
    
    # Global display mode
    pkg_version = get_package_version()
    git_sha = ""
    try:
        repo_root = Path(__file__).resolve().parents[2]
        out = subprocess.check_output(
            ["git", "rev-parse", "--short", "HEAD"],
            stderr=subprocess.DEVNULL,
            cwd=str(repo_root),
        )
        git_sha = out.decode("utf-8", errors="ignore").strip()
    except Exception:
        git_sha = ""
    version_str = f"wks0 {pkg_version}"
    if git_sha:
        version_str = f"{version_str} ({git_sha})"
    parser.add_argument(
        "--version",
        action="version",
        version=version_str,
        help="Show CLI version and exit",
    )
    
    # Add --display argument (cli or mcp, auto-detected)
    add_display_argument(parser)

    # Config command - show config file
    cfg = sub.add_parser("config", help="Show configuration file")
    cfg.set_defaults(func=show_config)

    # Setup command parsers
    setup_service_parser(sub)
    setup_monitor_parser(sub)
    setup_index_parser(sub)
    setup_related_parser(sub)
    
    # DB command (from cli_db module)
    from .. import cli_db
    db_parser = cli_db.setup_db_parser(sub)
    
    # MCP server command
    mcp = sub.add_parser("mcp", help="MCP (Model Context Protocol) server for AI integration")
    mcpsub = mcp.add_subparsers(dest="mcp_cmd")
    mcprun = mcpsub.add_parser("run", help="Start MCP server (stdio transport)")
    
    def mcp_run(args: argparse.Namespace) -> int:
        """Start MCP server for AI integration."""
        from ..mcp_server import main as mcp_main
        mcp_main()
        return 0
    mcprun.set_defaults(func=mcp_run)

    # Parse arguments
    args = parser.parse_args(argv)
    
    # Auto-detect display mode if not specified
    if not hasattr(args, "display") or args.display is None:
        args.display = "cli"  # Default to CLI
    
    # Get display object
    args.display_obj = get_display(args.display)

    # Handle no command - show help for command groups
    if not hasattr(args, "func"):
        help_registry = {
            'config': cfg,
            'service': None,  # Will be set by setup_service_parser
            'monitor': None,  # Will be set by setup_monitor_parser
            'db': db_parser,
            'mcp': mcp,
        }
        cmd = getattr(args, 'cmd', None)
        if cmd == 'config':
            cfg.print_help()
            return 2
        elif cmd == 'db':
            db_parser.print_help()
            return 2
        elif cmd == 'mcp':
            mcp.print_help()
            return 2
        parser.print_help()
        return 2

    # Execute command
    try:
        return args.func(args)
    except KeyboardInterrupt:
        return 130
    except Exception as e:
        if args.display_obj:
            args.display_obj.error(f"Command failed: {e}")
        else:
            print(f"Error: {e}")
        return 1
</file>

<file path="wks/display/__init__.py">
"""Display layer for WKS - CLI and MCP output formatting."""

from .base import Display
from .cli import CLIDisplay
from .mcp import MCPDisplay
from .context import get_display, is_mcp_context

__all__ = ["Display", "CLIDisplay", "MCPDisplay", "get_display", "is_mcp_context"]
</file>

<file path="wks/display/base.py">
"""Abstract base class for display implementations."""

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from pathlib import Path


class Display(ABC):
    """Abstract base for CLI and MCP display implementations."""

    @abstractmethod
    def status(self, message: str, **kwargs) -> None:
        """Display a status message.

        Args:
            message: Status text to display
            kwargs: Implementation-specific options
        """
        pass

    @abstractmethod
    def success(self, message: str, **kwargs) -> None:
        """Display a success message.

        Args:
            message: Success text
            kwargs: Implementation-specific options (e.g., data for MCP)
        """
        pass

    @abstractmethod
    def error(self, message: str, **kwargs) -> None:
        """Display an error message.

        Args:
            message: Error text
            kwargs: Implementation-specific options (e.g., details for MCP)
        """
        pass

    @abstractmethod
    def warning(self, message: str, **kwargs) -> None:
        """Display a warning message.

        Args:
            message: Warning text
            kwargs: Implementation-specific options
        """
        pass

    @abstractmethod
    def info(self, message: str, **kwargs) -> None:
        """Display an informational message.

        Args:
            message: Info text
            kwargs: Implementation-specific options
        """
        pass

    @abstractmethod
    def table(self, data: List[Dict[str, Any]], headers: Optional[List[str]] = None, **kwargs) -> None:
        """Display data in table format.

        Args:
            data: List of rows (dicts mapping column name to value)
            headers: Optional explicit column headers (if None, infer from data)
            kwargs: Implementation-specific options (title, etc.)
        """
        pass

    @abstractmethod
    def progress_start(self, total: int, description: str = "", **kwargs) -> Any:
        """Start a progress bar/indicator.

        Args:
            total: Total number of items
            description: Description of operation
            kwargs: Implementation-specific options

        Returns:
            Progress handle/context for updates
        """
        pass

    @abstractmethod
    def progress_update(self, handle: Any, advance: int = 1, **kwargs) -> None:
        """Update progress.

        Args:
            handle: Progress handle from progress_start
            advance: Number of items to advance
            kwargs: Implementation-specific options (description update, etc.)
        """
        pass

    @abstractmethod
    def progress_finish(self, handle: Any, **kwargs) -> None:
        """Finish/close progress indicator.

        Args:
            handle: Progress handle from progress_start
            kwargs: Implementation-specific options
        """
        pass

    @abstractmethod
    def spinner_start(self, description: str = "", **kwargs) -> Any:
        """Start a spinner for indeterminate operations.

        Args:
            description: Description of operation
            kwargs: Implementation-specific options

        Returns:
            Spinner handle/context for updates
        """
        pass

    @abstractmethod
    def spinner_update(self, handle: Any, description: str, **kwargs) -> None:
        """Update spinner description.

        Args:
            handle: Spinner handle from spinner_start
            description: New description
            kwargs: Implementation-specific options
        """
        pass

    @abstractmethod
    def spinner_finish(self, handle: Any, message: str = "", **kwargs) -> None:
        """Finish/stop spinner.

        Args:
            handle: Spinner handle from spinner_start
            message: Final message to display
            kwargs: Implementation-specific options
        """
        pass

    @abstractmethod
    def tree(self, data: Dict[str, Any], title: str = "", **kwargs) -> None:
        """Display hierarchical data as a tree.

        Args:
            data: Nested dict/list structure
            title: Optional title for tree
            kwargs: Implementation-specific options
        """
        pass

    @abstractmethod
    def json_output(self, data: Any, **kwargs) -> None:
        """Output structured data (for MCP, this is the main output method).

        Args:
            data: Data to output
            kwargs: Implementation-specific options (indent, etc.)
        """
        pass

    @abstractmethod
    def panel(self, content: str, title: str = "", **kwargs) -> None:
        """Display content in a panel/box.

        Args:
            content: Content to display
            title: Panel title
            kwargs: Implementation-specific options (border style, etc.)
        """
        pass
</file>

<file path="wks/display/context.py">
"""Context detection and display factory for CLI vs MCP environments."""

import os
import sys
from typing import Literal

from .base import Display
from .cli import CLIDisplay
from .mcp import MCPDisplay


DisplayMode = Literal["cli", "mcp"]


def is_mcp_context() -> bool:
    """Detect if we're running in an MCP context.

    Returns:
        True if MCP environment detected, False otherwise
    """
    # Check environment variables that MCP might set
    if os.getenv("MCP_MODE") == "1":
        return True

    if os.getenv("MCP_SERVER") is not None:
        return True

    # Check if stdin/stdout are pipes (common in MCP)
    # But also check we're not in a regular terminal with pipes
    if not sys.stdout.isatty():
        # If TERM is not set and we're piped, likely MCP
        if os.getenv("TERM") is None:
            return True

    return False


def get_display(mode: DisplayMode = None) -> Display:
    """Get appropriate display implementation.

    Args:
        mode: Explicit display mode ("cli" or "mcp")
              If None, auto-detect based on context

    Returns:
        Display implementation (CLIDisplay or MCPDisplay)
    """
    if mode is None:
        # Auto-detect
        mode = "mcp" if is_mcp_context() else "cli"

    if mode == "mcp":
        return MCPDisplay()
    elif mode == "cli":
        return CLIDisplay()
    else:
        raise ValueError(f"Invalid display mode: {mode}. Must be 'cli' or 'mcp'")


def add_display_argument(parser) -> None:
    """Add --display argument to an argparse parser.

    Args:
        parser: argparse.ArgumentParser instance
    """
    default_mode = "mcp" if is_mcp_context() else "cli"

    parser.add_argument(
        "--display",
        choices=["cli", "mcp"],
        default=default_mode,
        help=f"Output display format (default: {default_mode}, auto-detected)"
    )
</file>

<file path="wks/display/mcp.py">
"""MCP display implementation - JSON output only."""

import json
import sys
from datetime import datetime
from typing import Any, Dict, List, Optional

from .base import Display


class MCPDisplay(Display):
    """JSON-based display for MCP integration."""

    def __init__(self):
        self._output_buffer = []  # Collect all outputs
        self._progress_states = {}  # Track progress state by handle

    def _output(self, data: Dict[str, Any]) -> None:
        """Output JSON to stdout."""
        print(json.dumps(data), flush=True)

    def status(self, message: str, **kwargs) -> None:
        """Output status as JSON."""
        self._output({
            "type": "status",
            "message": message,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        })

    def success(self, message: str, **kwargs) -> None:
        """Output success as JSON."""
        output = {
            "type": "success",
            "message": message,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        if "data" in kwargs:
            output["data"] = kwargs["data"]
        self._output(output)

    def error(self, message: str, **kwargs) -> None:
        """Output error as JSON."""
        output = {
            "type": "error",
            "message": message,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        if "details" in kwargs:
            output["details"] = kwargs["details"]
        self._output(output)

    def warning(self, message: str, **kwargs) -> None:
        """Output warning as JSON."""
        self._output({
            "type": "warning",
            "message": message,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        })

    def info(self, message: str, **kwargs) -> None:
        """Output info as JSON."""
        self._output({
            "type": "info",
            "message": message,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        })

    def table(self, data: List[Dict[str, Any]], headers: Optional[List[str]] = None, **kwargs) -> None:
        """Output table data as JSON array."""
        output = {
            "type": "table",
            "data": data,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        if headers:
            output["headers"] = headers
        if "title" in kwargs:
            output["title"] = kwargs["title"]
        self._output(output)

    def progress_start(self, total: int, description: str = "", **kwargs) -> Any:
        """Record progress start (MCP doesn't show live progress)."""
        handle = id(self)  # Simple handle
        self._progress_states[handle] = {
            "total": total,
            "current": 0,
            "description": description
        }
        return handle

    def progress_update(self, handle: Any, advance: int = 1, **kwargs) -> None:
        """Update progress state (not output, just tracking)."""
        if handle in self._progress_states:
            self._progress_states[handle]["current"] += advance
            if "description" in kwargs:
                self._progress_states[handle]["description"] = kwargs["description"]

    def progress_finish(self, handle: Any, **kwargs) -> None:
        """Output final progress state."""
        if handle in self._progress_states:
            state = self._progress_states[handle]
            self._output({
                "type": "progress_complete",
                "total": state["total"],
                "completed": state["current"],
                "description": state["description"],
                "timestamp": datetime.utcnow().isoformat() + "Z"
            })
            del self._progress_states[handle]

    def spinner_start(self, description: str = "", **kwargs) -> Any:
        """Record spinner start (MCP doesn't show spinners)."""
        handle = id(self) + 1000  # Different from progress handles
        return handle

    def spinner_update(self, handle: Any, description: str, **kwargs) -> None:
        """No-op for MCP (spinners not shown)."""
        pass

    def spinner_finish(self, handle: Any, message: str = "", **kwargs) -> None:
        """Output completion message if provided."""
        if message:
            self.status(message)

    def tree(self, data: Dict[str, Any], title: str = "", **kwargs) -> None:
        """Output tree data as nested JSON."""
        output = {
            "type": "tree",
            "data": data,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        if title:
            output["title"] = title
        self._output(output)

    def json_output(self, data: Any, **kwargs) -> None:
        """Output structured data (primary method for MCP)."""
        self._output({
            "type": "data",
            "data": data,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        })

    def panel(self, content: str, title: str = "", **kwargs) -> None:
        """Output panel content as simple JSON."""
        output = {
            "type": "panel",
            "content": content,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        if title:
            output["title"] = title
        self._output(output)
</file>

<file path="wks/cli.py.backup">
"""
WKS command-line interface.

Provides simple commands for managing the daemon, config, and local MongoDB.
"""

from __future__ import annotations

import argparse
import contextlib
import hashlib
import math
import os
import platform
import re
import signal
import subprocess
import sys
import time
from dataclasses import dataclass, field, asdict
import json
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import unquote, urlparse

import fnmatch
import pymongo
import shutil

from .monitor_controller import MonitorController, MonitorValidator
from .config import (
    apply_similarity_mongo_defaults,
    mongo_settings,
    DEFAULT_TIMESTAMP_FORMAT,
    timestamp_format,
    load_config as config_load_config,
    get_config_path,
)
from .constants import WKS_HOME_EXT, WKS_EXTRACT_EXT, WKS_DOT_DIRS, WKS_HOME_DISPLAY
from .display.context import get_display, add_display_argument
from .extractor import Extractor
from .status import record_db_activity, load_db_activity_summary, load_db_activity_history
from .dbmeta import (
    IncompatibleDatabase,
    ensure_db_compat,
    resolve_db_compatibility,
)
from .utils import get_package_version
from . import mongoctl
from .templating import render_template


LOCK_FILE = Path.home() / WKS_HOME_EXT / "daemon.lock"

DEFAULT_MONITOR_INCLUDE_PATHS = ["~"]
DEFAULT_MONITOR_EXCLUDE_PATHS = ["~/Library", "~/obsidian", f"{WKS_HOME_DISPLAY}"]
DEFAULT_MONITOR_IGNORE_DIRS = [".git", "_build", WKS_HOME_EXT, WKS_EXTRACT_EXT]
DEFAULT_MONITOR_IGNORE_GLOBS = ["*.tmp", "*~", "._*"]

DEFAULT_OBSIDIAN_CONFIG = {
    "base_dir": "WKS",
    "log_max_entries": 500,
    "active_files_max_rows": 50,
    "source_max_chars": 40,
    "destination_max_chars": 40,
    "docs_keep": 99,
}

DEFAULT_SIMILARITY_EXTS = [
    ".md",
    ".txt",
    ".py",
    ".ipynb",
    ".tex",
    ".docx",
    ".pptx",
    ".pdf",
    ".html",
    ".csv",
    ".xlsx",
]

# Legacy display modes (mapped to cli/mcp internally)
DISPLAY_CHOICES_LEGACY = ["auto", "rich", "plain", "markdown", "json", "none"]
# New display modes
DISPLAY_CHOICES = ["cli", "mcp"]

STATUS_MARKDOWN_TEMPLATE = """| Key | Value |
| --- | --- |
{% for key, value in rows %}
| {{ key }} | {{ value | replace('|', '\\|') | replace('\n', '<br>') }} |
{% endfor %}
""".strip()

DB_QUERY_MARKDOWN_TEMPLATE = """### {{ scope|capitalize }} query ‚Äî {{ collection }}
{% if rows %}
| # | Document |
| --- | --- |
{% for row in rows %}
| {{ loop.index }} | {{ row | replace('|', '\\|') }} |
{% endfor %}
{% else %}
_No documents found._
{% endif %}
""".strip()


# _package_version moved to utils.get_package_version


def _human_bytes(value: Optional[int]) -> str:
    if value is None:
        return "-"
    try:
        val = float(value)
    except Exception:
        return "-"
    units = ["B", "kB", "MB", "GB", "TB"]
    idx = 0
    while val >= 1024.0 and idx < len(units) - 1:
        val /= 1024.0
        idx += 1
    return f"{val:7.2f} {units[idx]:>2}"


def _fmt_bool(value: Optional[bool]) -> str:
    if value is None:
        return "-"
    return "true" if value else "false"


def _format_timestamp_value(value: Optional[Any], fmt: str) -> str:
    if value is None:
        return ""
    text = str(value).strip()
    if not text:
        return ""
    try:
        s = text
        if s.endswith("Z"):
            s = s[:-1] + "+00:00"
        s = s.replace("Z", "+00:00")
        dt = datetime.fromisoformat(s)
    except Exception:
        try:
            fallback = text.replace("T", " ").replace("Z", "")
            dt = datetime.fromisoformat(fallback)
        except Exception:
            return text
    try:
        return dt.strftime(fmt)
    except Exception:
        return text


def _doc_path_to_local(doc: Dict[str, Any]) -> Optional[Path]:
    local = doc.get("path_local")
    if isinstance(local, str) and local:
        try:
            return Path(local).expanduser()
        except Exception:
            pass
    uri = doc.get("path")
    if isinstance(uri, str) and uri:
        if uri.startswith("file://"):
            try:
                parsed = urlparse(uri)
                return Path(unquote(parsed.path or "")).expanduser()
            except Exception:
                return None
        try:
            return Path(uri).expanduser()
        except Exception:
            return None
    return None


@dataclass
class ServiceStatusLaunch:
    state: Optional[str] = None
    active_count: Optional[str] = None
    pid: Optional[str] = None
    program: Optional[str] = None
    arguments: Optional[str] = None
    working_dir: Optional[str] = None
    stdout: Optional[str] = None
    stderr: Optional[str] = None
    runs: Optional[str] = None
    last_exit: Optional[str] = None
    path: Optional[str] = None
    type: Optional[str] = None

    def present(self) -> bool:
        return any(
            [
                self.state,
                self.pid,
                self.program,
                self.arguments,
                self.working_dir,
                self.stdout,
                self.stderr,
                self.path,
            ]
        )

    def as_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class ServiceStatusDB:
    tracked_files: Optional[int] = None
    last_updated: Optional[str] = None
    total_size_bytes: Optional[int] = None
    latest_files: List[Dict[str, str]] = field(default_factory=list)

    def present(self) -> bool:
        return any(
            [
                self.tracked_files is not None,
                self.last_updated,
                self.total_size_bytes is not None,
                self.latest_files,
            ]
        )

    def as_dict(self) -> Dict[str, Any]:
        return {
            "tracked_files": self.tracked_files,
            "last_updated": self.last_updated,
            "total_size_bytes": self.total_size_bytes,
            "latest_files": list(self.latest_files),
        }


@dataclass
class ServiceStatusData:
    running: Optional[bool] = None
    heartbeat: Optional[str] = None
    uptime: Optional[str] = None
    pid: Optional[int] = None
    beats_per_min: Optional[float] = None
    pending_deletes: Optional[int] = None
    pending_mods: Optional[int] = None
    ok: Optional[bool] = None
    lock: Optional[bool] = None
    last_error: Optional[str] = None
    db_last_operation: Optional[str] = None
    db_last_operation_detail: Optional[str] = None
    db_last_operation_iso: Optional[str] = None
    db_ops_last_minute: Optional[int] = None
    fs_rate_short: Optional[float] = None
    fs_rate_long: Optional[float] = None
    fs_rate_weighted: Optional[float] = None
    launch: ServiceStatusLaunch = field(default_factory=ServiceStatusLaunch)
    db: ServiceStatusDB = field(default_factory=ServiceStatusDB)
    notes: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "service": {
                "running": self.running,
                "heartbeat": self.heartbeat,
                "uptime": self.uptime,
                "pid": self.pid,
                "beats_per_min": self.beats_per_min,
                "pending_deletes": self.pending_deletes,
                "pending_mods": self.pending_mods,
                "ok": self.ok,
                "lock": self.lock,
                "last_error": self.last_error,
                "db_last_operation": self.db_last_operation,
                "db_last_operation_detail": self.db_last_operation_detail,
                "db_last_operation_iso": self.db_last_operation_iso,
                "db_ops_last_minute": self.db_ops_last_minute,
                "fs_rate_short": self.fs_rate_short,
                "fs_rate_long": self.fs_rate_long,
                "fs_rate_weighted": self.fs_rate_weighted,
            },
            "launch_agent": self.launch.as_dict(),
            "space_db": self.db.as_dict(),
            "notes": list(self.notes),
        }

    def to_json(self) -> str:
        return json.dumps(self.to_dict(), indent=2, sort_keys=True)

    def to_rows(self) -> List[Tuple[str, str]]:
        rows: List[Tuple[str, str]] = []
        rows.append(("Running", _fmt_bool(self.running)))
        rows.append(("Heartbeat", self.heartbeat or "-"))
        rows.append(("Uptime", self.uptime or "-"))
        rows.append(("PID", str(self.pid) if self.pid is not None else "-"))
        rows.append(
            ("DB ops/min", f"{self.beats_per_min:.2f}" if isinstance(self.beats_per_min, (int, float)) else "-")
        )
        rows.append(
            ("Pending deletes", str(self.pending_deletes) if self.pending_deletes is not None else "-")
        )
        rows.append(("Pending mods", str(self.pending_mods) if self.pending_mods is not None else "-"))
        rows.append(("OK", _fmt_bool(self.ok)))
        rows.append(("Lock", _fmt_bool(self.lock)))
        if self.last_error:
            rows.append(("Last error", self.last_error))
        if self.db_last_operation or self.db_last_operation_iso:
            desc = self.db_last_operation or "-"
            if self.db_last_operation_detail:
                desc = f"{desc} ({self.db_last_operation_detail})"
            if self.db_last_operation_iso:
                desc = f"{desc} @ {self.db_last_operation_iso}"
            rows.append(("Last operation", desc))
        if self.db_ops_last_minute is not None:
            rows.append(("DB ops (last min)", str(self.db_ops_last_minute)))
        if self.fs_rate_short is not None:
            rows.append(("FS ops/sec (10s)", f"{self.fs_rate_short:.2f}"))
        if self.fs_rate_long is not None:
            rows.append(("FS ops/sec (10m)", f"{self.fs_rate_long:.2f}"))
        if self.fs_rate_weighted is not None:
            rows.append(("FS ops/sec (weighted)", f"{self.fs_rate_weighted:.2f}"))
        if self.launch.present():
            rows.append(("Launch state", self.launch.state or "-"))
            rows.append(("Launch PID", self.launch.pid or "-"))
            program_desc = self.launch.arguments or self.launch.program or "-"
            rows.append(("Launch program", program_desc))
            rows.append(("Launch stdout", self.launch.stdout or "-"))
            rows.append(("Launch stderr", self.launch.stderr or "-"))
            rows.append(("Launch runs", self.launch.runs or "-"))
            rows.append(("Launch last exit", self.launch.last_exit or "-"))
            rows.append(("Launch path", self.launch.path or "-"))
            rows.append(("Launch type", self.launch.type or "-"))
        if self.db.present():
            rows.append(
                (
                    "DB tracked files",
                    str(self.db.tracked_files) if self.db.tracked_files is not None else "-",
                )
            )
            rows.append(("DB last updated", self.db.last_updated or "-"))
            rows.append(
                (
                    "DB total size",
                    _human_bytes(self.db.total_size_bytes),
                )
            )
        if self.notes:
            rows.append(("Notes", "; ".join(self.notes)))
        return rows

    def to_markdown(self) -> str:
        rows = self.to_rows()
        lines = ["| Key | Value |", "| --- | --- |"]
        for key, value in rows:
            val = value if value else "-"
            val = val.replace("|", "\\|").replace("\n", "<br>")
            lines.append(f"| {key} | {val} |")
        return "\n".join(lines)

def load_config() -> Dict[str, Any]:
    """Load WKS configuration from default location."""
    return config_load_config()


def _merge_defaults(defaults: List[str], user: Optional[List[str]]) -> List[str]:
    merged: List[str] = []
    for item in defaults:
        if item not in merged:
            merged.append(item)
    for item in user or []:
        if item not in merged:
            merged.append(item)
    return merged


def print_config(args: argparse.Namespace) -> None:
    cfg = load_config()
    mongo_cfg = mongo_settings(cfg)
    space_tag, time_tag = resolve_db_compatibility(cfg)
    mongo_out = dict(mongo_cfg)
    mongo_out["compatibility"] = {
        "space": space_tag,
        "time": time_tag,
    }

    obs_raw = cfg.get("obsidian") or {}
    obsidian = {
        "base_dir": obs_raw.get("base_dir", DEFAULT_OBSIDIAN_CONFIG["base_dir"]),
        "log_max_entries": int(obs_raw.get("log_max_entries", DEFAULT_OBSIDIAN_CONFIG["log_max_entries"])),
        "active_files_max_rows": int(obs_raw.get("active_files_max_rows", DEFAULT_OBSIDIAN_CONFIG["active_files_max_rows"])),
        "source_max_chars": int(obs_raw.get("source_max_chars", DEFAULT_OBSIDIAN_CONFIG["source_max_chars"])),
        "destination_max_chars": int(obs_raw.get("destination_max_chars", DEFAULT_OBSIDIAN_CONFIG["destination_max_chars"])),
        "docs_keep": int(obs_raw.get("docs_keep", DEFAULT_OBSIDIAN_CONFIG["docs_keep"])),
    }

    mon_raw = cfg.get("monitor") or {}
    monitor = {
        "include_paths": _merge_defaults(DEFAULT_MONITOR_INCLUDE_PATHS, mon_raw.get("include_paths")),
        "exclude_paths": _merge_defaults(DEFAULT_MONITOR_EXCLUDE_PATHS, mon_raw.get("exclude_paths")),
        "ignore_dirnames": _merge_defaults(DEFAULT_MONITOR_IGNORE_DIRS, mon_raw.get("ignore_dirnames")),
        "ignore_globs": _merge_defaults(DEFAULT_MONITOR_IGNORE_GLOBS, mon_raw.get("ignore_globs")),
        "state_file": mon_raw.get("state_file", f"{WKS_HOME_DISPLAY}/monitor_state.json"),
    }

    activity_raw = cfg.get("activity") or {}
    activity = {
        "state_file": activity_raw.get("state_file", f"{WKS_HOME_DISPLAY}/activity_state.json"),
    }

    display_cfg = cfg.get("display") or {}
    display = {
        "timestamp_format": display_cfg.get("timestamp_format", timestamp_format(cfg) or DEFAULT_TIMESTAMP_FORMAT),
    }

    ext_raw = cfg.get("extract") or {}
    extract = {
        "engine": ext_raw.get("engine", "docling"),
        "ocr": bool(ext_raw.get("ocr", False)),
        "timeout_secs": int(ext_raw.get("timeout_secs", 30)),
        "options": dict(ext_raw.get("options") or {}),
    }

    sim_raw = cfg.get("similarity") or {}
    include_exts = sim_raw.get("include_extensions")
    if not include_exts:
        include_exts = DEFAULT_SIMILARITY_EXTS
    similarity = {
        "enabled": bool(sim_raw.get("enabled", True)),
        "model": sim_raw.get("model", "all-MiniLM-L6-v2"),
        "include_extensions": [ext.lower() for ext in include_exts],
        "min_chars": int(sim_raw.get("min_chars", 10)),
        "max_chars": int(sim_raw.get("max_chars", 200000)),
        "chunk_chars": int(sim_raw.get("chunk_chars", 1500)),
        "chunk_overlap": int(sim_raw.get("chunk_overlap", 200)),
        "offline": bool(sim_raw.get("offline", True)),
        "respect_monitor_ignores": bool(sim_raw.get("respect_monitor_ignores", False)),
    }

    metrics_raw = cfg.get("metrics") or {}
    metrics = {
        "fs_rate_short_window_secs": int(metrics_raw.get("fs_rate_short_window_secs", 10)),
        "fs_rate_long_window_secs": int(metrics_raw.get("fs_rate_long_window_secs", 600)),
        "fs_rate_short_weight": float(metrics_raw.get("fs_rate_short_weight", 0.8)),
        "fs_rate_long_weight": float(metrics_raw.get("fs_rate_long_weight", 0.2)),
    }

    payload = {
        "vault_path": cfg.get("vault_path", "~/obsidian"),
        "obsidian": obsidian,
        "monitor": monitor,
        "activity": activity,
        "display": display,
        "mongo": mongo_out,
        "extract": extract,
        "similarity": similarity,
        "metrics": metrics,
    }

    _maybe_write_json(args, payload)
    if not _display_enabled(args.display):
        return
    if args.display == "rich":
        try:
            from rich.console import Console
            from rich.syntax import Syntax

            Console().print(Syntax(_json_dumps(payload), "json", word_wrap=False, indent_guides=True))
            return
        except Exception:
            pass
    print(_json_dumps(payload))


def _pid_running(pid: int) -> bool:
    try:
        os.kill(pid, 0)
        return True
    except Exception:
        return False


def _stop_managed_mongo() -> None:
    mongoctl.stop_managed_mongo()


def _agent_label() -> str:
    # Unique launchd label bound to the new CLI name
    return "com.wieselquist.wks0"


def _agent_plist_path() -> Path:
    return Path.home() / "Library" / "LaunchAgents" / f"{_agent_label()}.plist"


def _is_macos() -> bool:
    return platform.system() == "Darwin"


def _launchctl(*args: str) -> int:
    try:
        # Suppress noisy stderr/stdout from launchctl; we use return codes
        return subprocess.call(["launchctl", *args], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except FileNotFoundError:
        return 2


def _agent_installed() -> bool:
    return _agent_plist_path().exists()


def _daemon_start_launchd():
    uid = os.getuid()
    pl = str(_agent_plist_path())
    # Prefer kickstart for already-bootstrapped agents
    rc = _launchctl("kickstart", "-k", f"gui/{uid}/{_agent_label()}")
    if rc == 0:
        return
    # If kickstart failed, try bootstrapping fresh
    _launchctl("bootout", f"gui/{uid}", pl)
    _launchctl("bootstrap", f"gui/{uid}", pl)
    _launchctl("enable", f"gui/{uid}/{_agent_label()}")
    _launchctl("kickstart", "-k", f"gui/{uid}/{_agent_label()}")


def _daemon_stop_launchd():
    uid = os.getuid()
    _launchctl("bootout", f"gui/{uid}", str(_agent_plist_path()))


def _daemon_status_launchd() -> int:
    uid = os.getuid()
    try:
        return subprocess.call(["launchctl", "print", f"gui/{uid}/{_agent_label()}"])
    except Exception:
        return 3


def daemon_status(args: argparse.Namespace) -> int:
    status = ServiceStatusData()

    try:
        mongoctl.ensure_mongo_running(_default_mongo_uri(), record_start=False)
    except Exception:
        pass

    @dataclass
    class LaunchAgentStatusInternal:
        state: str = ""
        active_count: str = ""
        path: str = ""
        type: str = ""
        program: str = ""
        arguments: str = ""
        working_dir: str = ""
        stdout: str = ""
        stderr: str = ""
        runs: str = ""
        pid: str = ""
        last_exit: str = ""

    launch_info: Optional[LaunchAgentStatusInternal] = None
    if _is_macos() and _agent_installed():
        try:
            uid = os.getuid()
            out = subprocess.check_output(
                ["launchctl", "print", f"gui/{uid}/{_agent_label()}"],
                stderr=subprocess.STDOUT,
            )
            launch_text = out.decode("utf-8", errors="ignore")
            import re as _re

            def _find(pattern: str, default: str = "") -> str:
                match = _re.search(pattern, launch_text)
                return match.group(1).strip() if match else default

            launch_info = LaunchAgentStatusInternal(
                active_count=_find(r"active count =\s*(\d+)"),
                path=_find(r"\n\s*path =\s*(.*)"),
                type=_find(r"\n\s*type =\s*(.*)"),
                state=_find(r"\n\s*state =\s*(.*)"),
                program=_find(r"\n\s*program =\s*(.*)"),
                working_dir=_find(r"\n\s*working directory =\s*(.*)"),
                stdout=_find(r"\n\s*stdout path =\s*(.*)"),
                stderr=_find(r"\n\s*stderr path =\s*(.*)"),
                runs=_find(r"\n\s*runs =\s*(\d+)"),
                pid=_find(r"\n\s*pid =\s*(\d+)"),
                last_exit=_find(r"\n\s*last exit code =\s*(\d+)"),
            )
            try:
                args_block = _re.search(r"arguments = \{([^}]*)\}", launch_text, _re.DOTALL)
                if args_block:
                    lines = [ln.strip() for ln in args_block.group(1).splitlines() if ln.strip()]
                    if launch_info:
                        launch_info.arguments = " ".join(lines)
            except Exception:
                pass
        except Exception:
            status.notes.append("Launch agent status unavailable")

    if launch_info:
        status.launch = ServiceStatusLaunch(
            state=launch_info.state or None,
            active_count=launch_info.active_count or None,
            pid=launch_info.pid or None,
            program=launch_info.program or None,
            arguments=launch_info.arguments or None,
            working_dir=launch_info.working_dir or None,
            stdout=launch_info.stdout or None,
            stderr=launch_info.stderr or None,
            runs=launch_info.runs or None,
            last_exit=launch_info.last_exit or None,
            path=launch_info.path or None,
            type=launch_info.type or None,
        )

    health_path = Path.home() / WKS_HOME_EXT / "health.json"
    health: Dict[str, Any] = {}
    try:
        if health_path.exists():
            health = json.load(open(health_path, "r"))
    except Exception:
        status.notes.append("Failed to read health metrics")
        health = {}

    if health:
        status.running = bool(health.get("lock_present"))
        status.heartbeat = str(health.get("heartbeat_iso") or "")
        status.uptime = str(health.get("uptime_hms") or "")
        try:
            status.pid = int(health.get("pid"))
        except Exception:
            status.pid = None
        try:
            bpm = health.get("avg_beats_per_min")
            status.beats_per_min = float(bpm) if bpm is not None else None
        except Exception:
            status.beats_per_min = None
        status.pending_deletes = health.get("pending_deletes")
        status.pending_mods = health.get("pending_mods")
        status.ok = False if health.get("last_error") else True
        status.lock = bool(health.get("lock_present"))
        if health.get("last_error"):
            status.last_error = str(health.get("last_error"))
        if health.get("db_last_operation"):
            status.db_last_operation = health.get("db_last_operation")
        if health.get("db_last_operation_detail"):
            status.db_last_operation_detail = health.get("db_last_operation_detail")
        if health.get("db_last_operation_iso"):
            status.db_last_operation_iso = health.get("db_last_operation_iso")
        if health.get("db_ops_last_minute") is not None:
            try:
                status.db_ops_last_minute = int(health.get("db_ops_last_minute"))
            except Exception:
                status.db_ops_last_minute = None
        for attr, key in [
            ("fs_rate_short", "fs_rate_short"),
            ("fs_rate_long", "fs_rate_long"),
            ("fs_rate_weighted", "fs_rate_weighted"),
        ]:
            try:
                val = health.get(key)
                setattr(status, attr, float(val) if val is not None else None)
            except Exception:
                setattr(status, attr, None)
    else:
        lock_exists = LOCK_FILE.exists()
        status.lock = lock_exists
        if lock_exists:
            try:
                pid = int(LOCK_FILE.read_text().strip().splitlines()[0])
                status.pid = pid
                status.running = _pid_running(pid)
            except Exception:
                status.notes.append("Lock present but PID unavailable")
                status.running = None
        else:
            status.running = False
        if not status.notes and not lock_exists:
            status.notes.append("WKS daemon: not running")

    summary = load_db_activity_summary()
    if summary:
        if not status.db_last_operation:
            status.db_last_operation = summary.get("operation") or None
        if not status.db_last_operation_detail:
            status.db_last_operation_detail = summary.get("detail") or None
        if not status.db_last_operation_iso:
            status.db_last_operation_iso = summary.get("timestamp_iso") or None
        if not status.heartbeat:
            status.heartbeat = summary.get("timestamp_iso") or status.heartbeat

    recent = load_db_activity_history(60)
    if status.db_ops_last_minute is None:
        status.db_ops_last_minute = len(recent)
    if status.beats_per_min is None and recent:
        status.beats_per_min = float(len(recent))
    if status.beats_per_min is None:
        status.beats_per_min = float(status.db_ops_last_minute or 0)

    try:
        cfg = load_config()
        ts_format = timestamp_format(cfg)
        mongo_cfg = mongo_settings(cfg)
        space_tag, _ = resolve_db_compatibility(cfg)
        client = pymongo.MongoClient(
            mongo_cfg["uri"],
            serverSelectionTimeoutMS=300,
            connectTimeoutMS=300,
        )
        client.admin.command("ping")
        try:
            ensure_db_compat(
                client,
                mongo_cfg["space_database"],
                "space",
                space_tag,
                product_version=get_package_version(),
            )
        except IncompatibleDatabase as exc:
            status.notes.append(str(exc))
            try:
                client.close()
            except Exception:
                pass
            coll = None
        else:
            coll = client[mongo_cfg["space_database"]][mongo_cfg["space_collection"]]
        if coll is not None:
            status.db.tracked_files = coll.count_documents({})
            last_doc = coll.find({}, {"timestamp": 1}).sort("timestamp", -1).limit(1)
            for doc in last_doc:
                formatted = _format_timestamp_value(doc.get("timestamp"), ts_format)
                status.db.last_updated = formatted or str(doc.get("timestamp", ""))

            total_size: Optional[int] = None
            try:
                agg = coll.aggregate(
                    [{"$group": {"_id": None, "total": {"$sum": {"$cond": [{"$gt": ["$bytes", 0]}, "$bytes", 0]}}}}]
                )
                agg_doc = next(agg, None)
                if agg_doc and agg_doc.get("total") is not None:
                    total_candidate = agg_doc.get("total")
                    if isinstance(total_candidate, (int, float)):
                        total_size = int(total_candidate)
            except Exception:
                total_size = None

            if total_size in (None, 0):
                try:
                    approx_total = 0
                    found_any = False
                    missing_metadata = False
                    for doc in coll.find({}, {"path": 1, "path_local": 1, "bytes": 1}).limit(1000):
                        found_any = True
                        bval = doc.get("bytes")
                        if isinstance(bval, (int, float)) and bval > 0:
                            approx_total += int(bval)
                            continue
                        missing_metadata = True
                        local_path = _doc_path_to_local(doc)
                        if local_path and local_path.exists():
                            try:
                                approx_total += local_path.stat().st_size
                            except Exception:
                                pass
                    if found_any and approx_total > 0:
                        total_size = approx_total
                    elif not missing_metadata and total_size is None:
                        total_size = approx_total
                except Exception:
                    pass

            if isinstance(total_size, (int, float)) and total_size >= 0:
                status.db.total_size_bytes = int(total_size)
            else:
                status.db.total_size_bytes = None
            latest = coll.find({}, {"path": 1, "timestamp": 1}).sort("timestamp", -1).limit(5)
            records = []
            for doc in latest:
                ts_value = _format_timestamp_value(doc.get("timestamp"), ts_format)
                if not ts_value:
                    ts_value = str(doc.get("timestamp", ""))
                records.append({"timestamp": ts_value, "path": str(doc.get("path", ""))})
            status.db.latest_files = records
            try:
                client.close()
            except Exception:
                pass
    except SystemExit:
        raise
    except Exception as exc:
        status.notes.append(f"Space DB stats unavailable: {exc}")

    payload = status.to_dict()
    _maybe_write_json(args, payload)

    display_mode = args.display
    if display_mode == "none":
        return 0
    if display_mode == "json":
        print(_json_dumps(payload))
        return 0
    if display_mode == "markdown":
        print(render_template(STATUS_MARKDOWN_TEMPLATE, {"rows": status.to_rows()}))
        return 0

    use_rich = False
    plain_mode = False
    if display_mode == "rich":
        use_rich = True
    elif display_mode == "plain":
        use_rich = True
        plain_mode = True
    elif display_mode == "auto":
        try:
            use_rich = sys.stdout.isatty()
        except Exception:
            use_rich = False

    if use_rich:
        try:
            from rich import box
            from rich.console import Console
            from rich.table import Table
        except Exception:
            use_rich = False

    if use_rich:
        colorful = (display_mode in {"rich", "auto"}) and not plain_mode
        console = Console(
            force_terminal=True,
            color_system=None if colorful else "standard",
            markup=colorful,
            highlight=False,
            soft_wrap=False,
        )
        table = Table(
            title="WKS Service Status",
            header_style="bold" if colorful else "",
            box=box.SQUARE if colorful else box.SIMPLE,
            expand=False,
            pad_edge=False,
        )
        table.add_column("Key", style="cyan" if colorful else "", overflow="fold")
        table.add_column("Value", style="white" if colorful else "", overflow="fold")
        for key, value in status.to_rows():
            table.add_row(key, value)
        console.print(table)
        return 0

    print(_json_dumps(payload))
    return 0


def _read_health_snapshot() -> Dict[str, Any]:
    health_path = Path.home() / WKS_HOME_EXT / "health.json"
    if not health_path.exists():
        return {}
    try:
        with open(health_path, "r") as fh:
            return json.load(fh)
    except Exception:
        return {}


def _wait_for_health_update(previous_heartbeat: Optional[str], timeout: float = 5.0) -> None:
    health_path = Path.home() / WKS_HOME_EXT / "health.json"
    if timeout <= 0:
        return
    deadline = time.time() + timeout
    while time.time() < deadline:
        if health_path.exists():
            try:
                with open(health_path, "r") as fh:
                    data = json.load(fh)
                hb = data.get("heartbeat_iso") or str(data.get("heartbeat"))
                if hb and hb != previous_heartbeat:
                    return
            except Exception:
                pass
        time.sleep(0.25)


def daemon_start(_: argparse.Namespace):
    mongoctl.ensure_mongo_running(_default_mongo_uri(), record_start=True)
    if _is_macos() and _agent_installed():
        _daemon_start_launchd()
        return
    # Start as background process: python -m wks.daemon
    env = os.environ.copy()
    python = sys.executable
    log_dir = Path.home() / WKS_HOME_EXT
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / "daemon.log"
    # Prefer running against the local source tree when available
    try:
        proj_root = Path(__file__).resolve().parents[1]
        env["PYTHONPATH"] = f"{proj_root}:{env.get('PYTHONPATH','')}"
        workdir = str(proj_root)
    except Exception:
        workdir = None
    with open(log_file, "ab", buffering=0) as lf:
        # Detach: create a new session
        kwargs = {
            "stdout": lf,
            "stderr": lf,
            "stdin": subprocess.DEVNULL,
            "start_new_session": True,
            "env": env,
            **({"cwd": workdir} if workdir else {}),
        }
        try:
            p = subprocess.Popen([python, "-m", "wks.daemon"], **kwargs)
        except Exception as e:
            print(f"Failed to start daemon: {e}")
            sys.exit(1)
    print(f"WKS daemon started (PID {p.pid}). Log: {log_file}")


def _daemon_stop_core(stop_mongo: bool = True) -> None:
    if _is_macos() and _agent_installed():
        _daemon_stop_launchd()
        if stop_mongo:
            _stop_managed_mongo()
        return
    if not LOCK_FILE.exists():
        print("WKS daemon is not running")
        if stop_mongo:
            _stop_managed_mongo()
        return
    try:
        pid_line = LOCK_FILE.read_text().strip().splitlines()[0]
        pid = int(pid_line)
    except Exception:
        print("Could not read PID from lock; try killing manually")
        return
    try:
        os.kill(pid, signal.SIGTERM)
        print(f"Sent SIGTERM to PID {pid}")
    except Exception as e:
        print(f"Failed to send SIGTERM to PID {pid}: {e}")
    finally:
        if stop_mongo:
            _stop_managed_mongo()


def daemon_stop(_: argparse.Namespace):
    _daemon_stop_core(stop_mongo=True)


def daemon_restart(args: argparse.Namespace):
    previous_health = _read_health_snapshot()
    previous_heartbeat = previous_health.get("heartbeat_iso") or previous_health.get("heartbeat")

    # macOS launchd-managed restart
    if _is_macos() and _agent_installed():
        try:
            mongoctl.ensure_mongo_running(_default_mongo_uri(), record_start=True)
        except Exception:
            pass
        try:
            _daemon_stop_launchd()
        except Exception:
            pass
        time.sleep(0.5)
        _daemon_start_launchd()
        _wait_for_health_update(previous_heartbeat, timeout=5.0)
        return

    # Fallback: stop/start without touching databases
    try:
        _daemon_stop_core(stop_mongo=False)
    except Exception:
        pass
    time.sleep(0.5)
    try:
        mongoctl.ensure_mongo_running(_default_mongo_uri(), record_start=True)
    except Exception:
        pass
    daemon_start(args)
    _wait_for_health_update(previous_heartbeat, timeout=5.0)




# ----------------------------- Similarity CLI ------------------------------ #
def _build_similarity_from_config(require_enabled: bool = True):
    try:
        from .similarity import build_similarity_from_config  # type: ignore
    except ImportError as e:
        error_msg = str(e).lower()
        if "sentence" in error_msg or "transformers" in error_msg:
            from .error_messages import missing_dependency_error
            missing_dependency_error("sentence-transformers", e)
        elif "docling" in error_msg:
            from .error_messages import missing_dependency_error
            missing_dependency_error("docling", e)
        else:
            print(f"\nSimilarity features not available: {e}")
            print("Install with: pip install -e '.[all]'\n")
            if require_enabled:
                raise SystemExit(2)
        return None, None
    except Exception as e:
        print(f"Similarity not available: {e}")
        if require_enabled:
            raise SystemExit(2)
        return None, None
    cfg = load_config()
    space_tag, _ = resolve_db_compatibility(cfg)
    pkg_version = get_package_version()
    try:
        db, sim_cfg = build_similarity_from_config(
            cfg,
            require_enabled=require_enabled,
            compatibility_tag=space_tag,
            product_version=pkg_version,
        )
        return db, sim_cfg
    except IncompatibleDatabase as exc:
        print(exc)
        if require_enabled:
            raise SystemExit(2)
        return None, None
    except Exception as e:
        mongo_uri = mongo_settings(cfg)["uri"]
        if mongo_uri.startswith("mongodb://localhost:27027") and shutil.which("mongod"):
            dbroot = Path.home() / WKS_HOME_EXT / "mongodb"
            dbpath = dbroot / "db"
            logfile = dbroot / "mongod.log"
            dbpath.mkdir(parents=True, exist_ok=True)
            try:
                subprocess.check_call([
                    "mongod", "--dbpath", str(dbpath), "--logpath", str(logfile),
                    "--fork", "--bind_ip", "127.0.0.1", "--port", "27027"
                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                db, sim_cfg = build_similarity_from_config(
                    cfg,
                    require_enabled=require_enabled,
                    compatibility_tag=space_tag,
                    product_version=pkg_version,
                )
                return db, sim_cfg
            except IncompatibleDatabase as exc2:
                print(exc2)
                if require_enabled:
                    raise SystemExit(2)
                return None, None
            except Exception as e2:
                print(f"Failed to auto-start local mongod: {e2}")
                if require_enabled:
                    raise SystemExit(2)
                return None, None
        print(f"Failed to initialize similarity DB: {e}")
        if require_enabled:
            raise SystemExit(2)
        return None, None


def _load_similarity_db() -> Any:
    db, _ = _build_similarity_from_config(require_enabled=False)
    return db


def _iter_files(paths: List[str], include_exts: List[str], cfg: Dict[str, Any]) -> List[Path]:
    """Yield files under paths; optionally respect monitor ignores.

    By default, only extension filtering is applied (no implicit directory skips).
    If similarity.respect_monitor_ignores is true, uses monitor.exclude_paths,
    monitor.ignore_dirnames, and monitor.ignore_globs from config.
    """
    sim = cfg.get('similarity', {})
    respect = bool(sim.get('respect_monitor_ignores', False))
    mon = cfg.get('monitor', {}) if respect else {}
    exclude_paths = [Path(p).expanduser().resolve() for p in (mon.get('exclude_paths') or [])]
    ignore_dirnames = set(mon.get('ignore_dirnames') or [])
    ignore_globs = list(mon.get('ignore_globs') or [])

    def _is_within(child: Path, base: Path) -> bool:
        try:
            child.resolve().relative_to(base.resolve())
            return True
        except Exception:
            return False

    def _auto_skip(p: Path) -> bool:
        parts = p.parts
        return any(part in WKS_DOT_DIRS for part in parts)

    def _skip(p: Path) -> bool:
        if _auto_skip(p):
            return True
        if not respect:
            return False
        # Exclude explicit paths
        for ex in exclude_paths:
            if _is_within(p, ex):
                return True
        # Ignore if any directory segment is in ignore_dirnames
        for part in p.resolve().parts:
            if part in ignore_dirnames:
                return True
        # Glob-based ignores
        pstr = p.as_posix()
        base = p.name
        from fnmatch import fnmatchcase as _fn
        for g in ignore_globs:
            try:
                if _fn(pstr, g) or _fn(base, g):
                    return True
            except Exception:
                continue
        return False

    out: List[Path] = []
    for p in paths:
        pp = Path(p).expanduser()
        if not pp.exists():
            continue
        if _auto_skip(pp):
            continue
        if pp.is_file():
            if (not include_exts or pp.suffix.lower() in include_exts) and not _skip(pp):
                out.append(pp)
        else:
            for x in pp.rglob('*'):
                if not x.is_file():
                    continue
                if _auto_skip(x):
                    continue
                if include_exts and x.suffix.lower() not in include_exts:
                    continue
                if _skip(x):
                    continue
                out.append(x)
    return out


def _build_extractor(cfg: Dict[str, Any]) -> Extractor:
    ext = cfg.get("extract") or {}
    sim = cfg.get("similarity") or {}
    return Extractor(
        engine=ext.get("engine", "docling"),
        ocr=bool(ext.get("ocr", False)),
        timeout_secs=int(ext.get("timeout_secs", 30)),
        options=dict(ext.get("options") or {}),
        max_chars=int(sim.get("max_chars", 200000)),
        write_extension=ext.get("write_extension"),
    )


def _file_checksum(path: Path) -> str:
    hasher = hashlib.sha256()
    with open(path, "rb") as fh:
        for chunk in iter(lambda: fh.read(1024 * 1024), b""):
            if not chunk:
                break
            hasher.update(chunk)
    return hasher.hexdigest()


def _as_file_uri_local(path: Path) -> str:
    try:
        return path.expanduser().resolve().as_uri()
    except ValueError:
        return "file://" + path.expanduser().resolve().as_posix()


def _format_duration(seconds: float) -> str:
    if seconds >= 1:
        return f"{seconds:.2f}s"
    return f"{seconds * 1000:.1f}ms"


def _resolve_display_mode(args: argparse.Namespace, default: str = "rich") -> str:
    mode = getattr(args, "display", None) or default
    if mode not in DISPLAY_CHOICES:
        return default
    return mode


def _display_enabled(mode: str) -> bool:
    return mode != "none"


def _json_dumps(payload: Any) -> str:
    return json.dumps(payload, indent=2, ensure_ascii=False, default=str)


def _maybe_write_json(args: argparse.Namespace, payload: Any) -> None:
    path = getattr(args, "json_path", None)
    if not path:
        return
    text = _json_dumps(payload)
    if path == "-":
        sys.__stdout__.write(text + "\n")
        sys.__stdout__.flush()
        return
    dest = Path(path).expanduser()
    try:
        parent = dest.parent
        if not parent.exists():
            parent.mkdir(parents=True, exist_ok=True)
        dest.write_text(text + "\n", encoding="utf-8")
    except Exception as exc:
        sys.__stderr__.write(f"Failed to write JSON output to {dest}: {exc}\n")


def _make_progress(total: int, display: str):
    from contextlib import contextmanager
    import time

    def _clip(text: str, limit: int = 48) -> str:
        if len(text) <= limit:
            return text
        if limit <= 1:
            return text[:limit]
        return text[: limit - 1] + "‚Ä¶"

    def _hms(secs: float) -> str:
        secs = max(0, int(secs))
        h = secs // 3600
        m = (secs % 3600) // 60
        s = secs % 60
        return f"{h:02d}:{m:02d}:{s:02d}"

    if display == "none":
        @contextmanager
        def _noop():
            class _Progress:
                def update(self, label: str, advance: int = 1) -> None:
                    return None

                def close(self) -> None:
                    return None

            yield _Progress()

        return _noop()

    use_rich = display in {"rich", "plain"}
    if use_rich:
        try:
            from rich.console import Console
            from rich.progress import (
                Progress,
                SpinnerColumn,
                BarColumn,
                TextColumn,
                TimeRemainingColumn,
                TimeElapsedColumn,
            )
        except Exception:
            use_rich = False

    if use_rich:
        console = Console(
            force_terminal=True,
            color_system=None if display == "rich" else "standard",
            markup=(display == "rich"),
            highlight=False,
            soft_wrap=False,
        )
        spinner_style = "cyan" if display == "rich" else ""
        bar_complete = "green" if display == "rich" else "white"
        bar_finished = "green" if display == "rich" else "white"
        bar_pulse = "white"
        label_template = "{task.fields[current]:<36}"
        counter_template = "{task.completed}/{task.total}" if total else "{task.completed}"

        @contextmanager
        def _rp():
            start = time.perf_counter()
            last = {"label": "Starting"}
            with Progress(
                SpinnerColumn(style=spinner_style),
                TextColumn(label_template, justify="left"),
                BarColumn(
                    bar_width=32,
                    complete_style=bar_complete,
                    finished_style=bar_finished,
                    pulse_style=bar_pulse,
                ),
                TextColumn(counter_template, justify="right"),
                TimeRemainingColumn(),
                TimeElapsedColumn(),
                transient=False,
                console=console,
                refresh_per_second=12,
            ) as progress:
                task = progress.add_task(
                    "wks0",
                    total=total if total else None,
                    current="Starting",
                )

                class _RichProgress:
                    def update(self, label: str, advance: int = 1) -> None:
                        clipped = _clip(label)
                        last["label"] = clipped
                        progress.update(task, advance=advance, current=clipped)

                    def close(self) -> None:
                        return None

                yield _RichProgress()
            elapsed = time.perf_counter() - start
            label = last.get("label") or "Completed"
            console.print(
                f"{label} finished in {_format_duration(elapsed)}",
                style="dim" if display == "rich" else "",
            )

        return _rp()

    @contextmanager
    def _bp():
        start = time.time()
        done = {"n": 0, "label": "Starting"}

        class _BasicProgress:
            def update(self, label: str, advance: int = 1) -> None:
                done["label"] = label
                done["n"] += advance
                n = done["n"]
                pct = (n / total * 100.0) if total else 100.0
                elapsed = time.time() - start
                eta = _hms((elapsed / n) * (total - n)) if n > 0 and total > n else _hms(0)
                print(f"[{n}/{total}] {pct:5.1f}% ETA {eta}  {label}")

            def close(self) -> None:
                return None

        try:
            yield _BasicProgress()
        finally:
            elapsed = time.time() - start
            label = done.get("label") or "Completed"
            print(f"[done] {label} finished in {_format_duration(elapsed)}")

    return _bp()


# ----------------------------- LLM helpers --------------------------------- # (removed)

# ----------------------------- Naming utilities ---------------------------- #
_DATE_RE = re.compile(r"^\d{4}(?:_\d{2})?(?:_\d{2})?$")
_GOOD_NAME_RE = re.compile(r"^[A-Za-z0-9_]+$")
_FOLDER_RE = re.compile(r"^(\d{4}(?:_\d{2})?(?:_\d{2})?)-([A-Za-z0-9_]+)$")


def _sanitize_name(name: str) -> str:
    s = name.strip()
    s = s.replace('-', '_')
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^A-Za-z0-9_]", "_", s)
    s = re.sub(r"_+", "_", s)
    return s.strip('_') or "Untitled"


def _normalize_date(date_str: str) -> str:
    s = date_str.strip()
    s = s.replace('-', '_')
    if not _DATE_RE.match(s):
        raise ValueError(f"Invalid DATE format: {date_str}")
    # Validate components
    parts = s.split('_')
    y = int(parts[0])
    if y < 1900 or y > 3000:
        raise ValueError("YEAR out of range")
    if len(parts) > 1:
        m = int(parts[1])
        if m < 1 or m > 12:
            raise ValueError("MONTH out of range")
    if len(parts) > 2:
        d = int(parts[2])
        if d < 1 or d > 31:
            raise ValueError("DAY out of range")
    return s


def _date_for_scope(scope: str, path: Path) -> str:
    ts = int(path.stat().st_mtime) if path.exists() else int(time.time())
    lt = time.localtime(ts)
    if scope == 'project':
        return f"{lt.tm_year:04d}"
    if scope == 'document':
        return f"{lt.tm_year:04d}_{lt.tm_mon:02d}"
    if scope == 'deadline':
        return f"{lt.tm_year:04d}_{lt.tm_mon:02d}_{lt.tm_mday:02d}"
    raise ValueError("scope must be one of: project|document|deadline")


# names_route_cmd removed


def names_check_cmd(args: argparse.Namespace) -> int:
    # Removed: analyze helpers are not part of the minimal CLI
    print("names check is not available in this build")
    return 2


def _pascalize_token(tok: str) -> str:
    if not tok:
        return tok
    if tok.isupper() and tok.isalpha():
        return tok
    if tok.isalpha() and len(tok) <= 4:
        return tok.upper()
    return tok[:1].upper() + tok[1:].lower()


def _pascalize_name(raw: str) -> str:
    # Replace spaces and illegal chars with underscores, then collapse
    s = re.sub(r"[^A-Za-z0-9_\-]+", "_", raw.strip())
    s = re.sub(r"_+", "_", s).strip("_")
    # Split on hyphens to remove them from namestring
    parts = s.split('-')
    out_parts = []
    for part in parts:
        if '_' in part:
            subs = part.split('_')
            out_parts.append('_'.join(_pascalize_token(t) for t in subs if t))
        else:
            out_parts.append(_pascalize_token(part))
    return ''.join(out_parts)


# names fix features removed


def _load_similarity_required() -> Tuple[Any, Dict[str, Any]]:
    db, sim_cfg = _build_similarity_from_config(require_enabled=True)
    if db is None or sim_cfg is None:
        raise SystemExit(2)
    return db, sim_cfg


# migration removed


def _mongo_client_params(
    server_timeout: int = 500,
    connect_timeout: int = 500,
    cfg: Optional[Dict[str, Any]] = None,
    *,
    ensure_running: bool = True,
) -> Tuple[pymongo.MongoClient, Dict[str, str]]:
    """Return (client, normalized mongo settings)."""
    if cfg is None:
        cfg = load_config()
    mongo_cfg = mongo_settings(cfg)
    client = mongoctl.create_client(
        mongo_cfg['uri'],
        server_timeout=server_timeout,
        connect_timeout=connect_timeout,
        ensure_running=ensure_running,
    )
    return client, mongo_cfg


def _default_mongo_uri() -> str:
    return mongo_settings(load_config())['uri']


def _is_within(path: Path, base: Path) -> bool:
    try:
        path.resolve().relative_to(base.resolve())
        return True
    except Exception:
        return False


def _should_skip_dir(dirpath: Path, ignore_dirnames: List[str]) -> bool:
    parts = dirpath.parts
    for part in parts:
        if part in ignore_dirnames:
            return True
        if part in WKS_DOT_DIRS:
            return True
        if part.startswith('.'):
            return True
    return False


def _should_skip_file(path: Path, ignore_patterns: List[str], ignore_globs: List[str]) -> bool:
    # Dotfiles (including .wks/.wkso artefact directories)
    if path.name.startswith('.') or any(part in WKS_DOT_DIRS for part in path.parts):
        return True
    # Pattern tokens match any segment exactly
    for tok in ignore_patterns:
        if tok in path.parts:
            return True
    # Glob matches against full path and basename
    pstr = path.as_posix()
    for g in ignore_globs:
        if fnmatch.fnmatchcase(pstr, g) or fnmatch.fnmatchcase(path.name, g):
            return True
    return False


"""Backfill removed for simplicity."""


# ----------------------------- Obsidian helpers ---------------------------- #
def _load_vault() -> Any:
    from .obsidian import ObsidianVault  # lazy import
    cfg = load_config()
    vault_path = cfg.get('vault_path')
    if not vault_path:
        print(f"Fatal: 'vault_path' is required in {WKS_HOME_DISPLAY}/config.json")
        raise SystemExit(2)
    obs = cfg.get('obsidian', {})
    base_dir = obs.get('base_dir')
    if not base_dir:
        print(f"Fatal: 'obsidian.base_dir' is required in {WKS_HOME_DISPLAY}/config.json (e.g., 'WKS')")
        raise SystemExit(2)
    # Require explicit logging caps/widths
    for k in ["log_max_entries", "active_files_max_rows", "source_max_chars", "destination_max_chars"]:
        if k not in obs:
            print(f"Fatal: missing required config key: obsidian.{k}")
            raise SystemExit(2)
    vault = ObsidianVault(
        Path(vault_path).expanduser(),
        base_dir=base_dir,
        log_max_entries=int(obs["log_max_entries"]),
        active_files_max_rows=int(obs["active_files_max_rows"]),
        source_max_chars=int(obs["source_max_chars"]),
        destination_max_chars=int(obs["destination_max_chars"]),
    )
    return vault


"""Unnecessary helpers removed for simplicity (obs connect, debug match, init logs, etc.)."""


def main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(prog="wks0", description="WKS management CLI")
    sub = parser.add_subparsers(dest="cmd")
    # Global display mode
    pkg_version = get_package_version()
    git_sha = ""
    try:
        repo_root = Path(__file__).resolve().parents[1]
        out = subprocess.check_output(
            ["git", "rev-parse", "--short", "HEAD"],
            stderr=subprocess.DEVNULL,
            cwd=str(repo_root),
        )
        git_sha = out.decode("utf-8", errors="ignore").strip()
    except Exception:
        git_sha = ""
    version_str = f"wks0 {pkg_version}"
    if git_sha:
        version_str = f"{version_str} ({git_sha})"
    parser.add_argument(
        "--version",
        action="version",
        version=version_str,
        help="Show CLI version and exit",
    )
    # Add --display argument (cli or mcp, auto-detected)
    add_display_argument(parser)
    parser.add_argument(
        "--json",
        dest="json_path",
        help="Optional path to write structured JSON output; use '-' for stdout.",
    )

    # Lightweight progress helpers
    cfg = sub.add_parser("config", help="Config commands")
    cfg_sub = cfg.add_subparsers(dest="cfg_cmd")
    cfg_print = cfg_sub.add_parser("print", help="Print effective config")
    cfg_print.set_defaults(func=print_config)

    # Service management (macOS launchd)

    

    # Optional install/uninstall on macOS (hide behind help)
    def _launchctl_quiet(*args: str) -> int:
        try:
            return subprocess.call(["launchctl", *args], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        except FileNotFoundError:
            print("launchctl not found; macOS only")
            return 2

    def _plist_path() -> Path:
        return Path.home()/"Library"/"LaunchAgents"/"com.wieselquist.wks0.plist"

    def daemon_install(args: argparse.Namespace):
        if platform.system() != "Darwin":
            print("install is macOS-only (launchd)")
            return
        pl = _plist_path()
        pl.parent.mkdir(parents=True, exist_ok=True)
        log_dir = Path.home()/WKS_HOME_EXT
        log_dir.mkdir(exist_ok=True)
        # Use the current interpreter (works for system Python, venv, and pipx)
        python = sys.executable
        proj_root = Path(__file__).resolve().parents[1]
        xml = f"""
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
  <key>Label</key>
  <string>com.wieselquist.wks0</string>
  <key>LimitLoadToSessionType</key>
  <string>Aqua</string>
  <key>ProgramArguments</key>
  <array>
    <string>{python}</string>
    <string>-m</string>
    <string>wks.daemon</string>
  </array>
  <key>WorkingDirectory</key>
  <string>{proj_root}</string>
  <key>EnvironmentVariables</key>
  <dict>
    <key>PYTHONPATH</key>
    <string>{proj_root}</string>
    <key>TOKENIZERS_PARALLELISM</key>
    <string>false</string>
  </dict>
  <key>RunAtLoad</key>
  <true/>
  <key>KeepAlive</key>
  <true/>
  <key>StandardOutPath</key>
  <string>{log_dir}/daemon.log</string>
  <key>StandardErrorPath</key>
  <string>{log_dir}/daemon.error.log</string>
 </dict>
</plist>
""".strip()
        pl.write_text(xml)
        uid = os.getuid()
        with _make_progress(total=6, display=args.display) as prog:
            prog.update("ensure mongo")
            mongoctl.ensure_mongo_running(_default_mongo_uri(), record_start=True)
            prog.update("bootout legacy")
            for old in [
                "com.wieselquist.wks0",
                "com.wieselquist.wksctl",
                "com.wieselquist.wks",
                "com.wieselquist.wks.db",
            ]:
                try:
                    _launchctl_quiet("bootout", f"gui/{uid}", str(Path.home()/"Library"/"LaunchAgents"/(old+".plist")))
                except Exception:
                    pass
            prog.update("bootstrap")
            _launchctl_quiet("bootstrap", f"gui/{uid}", str(pl))
            prog.update("enable")
            _launchctl_quiet("enable", f"gui/{uid}/com.wieselquist.wks0")
            prog.update("kickstart")
            _launchctl_quiet("kickstart", "-k", f"gui/{uid}/com.wieselquist.wks0")
            prog.update("done")
        print(f"Installed and started: {pl}")

    def daemon_uninstall(args: argparse.Namespace):
        if platform.system() != "Darwin":
            print("uninstall is macOS-only (launchd)")
            return
        pl = _plist_path()
        uid = os.getuid()
        with _make_progress(total=4, display=args.display) as prog:
            for label in ["com.wieselquist.wks0", "com.wieselquist.wksctl", "com.wieselquist.wks", "com.wieselquist.wks.db"]:
                prog.update(f"bootout {label}")
                try:
                    _launchctl_quiet("bootout", f"gui/{uid}", str(Path.home()/"Library"/"LaunchAgents"/(label+".plist")))
                except Exception:
                    pass
                prog.update(f"remove {label}.plist")
                try:
                    (Path.home()/"Library"/"LaunchAgents"/(label+".plist")).unlink()
                except Exception:
                    pass
        _stop_managed_mongo()
        print("Uninstalled.")

    # install/uninstall bound under service group below

    # Single entry for service management
    svc = sub.add_parser("service", help="Install/start/stop the WKS daemon (macOS)")
    svcsub = svc.add_subparsers(dest="svc_cmd")
    svcinst = svcsub.add_parser("install", help="Install launchd agent (macOS)")
    svcinst.set_defaults(func=daemon_install)
    svcrem = svcsub.add_parser("uninstall", help="Uninstall launchd agent (macOS)")
    svcrem.set_defaults(func=daemon_uninstall)
    svcstart2 = svcsub.add_parser("start", help="Start daemon in background or via launchd if installed")
    svcstart2.set_defaults(func=daemon_start)
    svcstop2 = svcsub.add_parser("stop", help="Stop daemon")
    svcstop2.set_defaults(func=daemon_stop)
    svcstatus2 = svcsub.add_parser("status", help="Daemon status")
    svcstatus2.set_defaults(func=daemon_status)
    svcrestart2 = svcsub.add_parser("restart", help="Restart daemon")
    svcrestart2.set_defaults(func=daemon_restart)

    # Service reset: stop agent, reset DB and local state, restart agent
    svcreset = svcsub.add_parser("reset", help="Stop service, reset databases/state, and start service cleanly")
    def _service_reset(args: argparse.Namespace) -> int:
        # Stop current agent (quiet if not running)
        try:
            if platform.system() == "Darwin":
                _launchctl_quiet("bootout", f"gui/{os.getuid()}", str(_plist_path()))
            else:
                daemon_stop(args)
        except Exception:
            pass
        # Reuse DB reset steps
        _db_reset(args)
        _stop_managed_mongo()
        # Ensure Mongo is running again before restart
        try:
            mongoctl.ensure_mongo_running(_default_mongo_uri(), record_start=True)
        except SystemExit:
            return 2
        # Clear local agent state (keep config.json)
        try:
            home = Path.home()
            for name in [
                'file_ops.jsonl','monitor_state.json','activity_state.json','health.json',
                'daemon.lock','daemon.log','daemon.error.log'
            ]:
                p = home/WKS_HOME_EXT/name
                try:
                    if p.exists():
                        p.unlink()
                except Exception:
                    pass
        except Exception:
            pass
        # Start service again
        try:
            if platform.system() == "Darwin":
                pl = _plist_path()
                _launchctl_quiet("bootstrap", f"gui/{os.getuid()}", str(pl))
                _launchctl_quiet("enable", f"gui/{os.getuid()}/com.wieselquist.wks0")
                _launchctl_quiet("kickstart", "-k", f"gui/{os.getuid()}/com.wieselquist.wks0")
            else:
                daemon_start(args)
        except Exception:
            pass
        # Show status and space stats
        try:
            daemon_status(args)
        except Exception:
            pass
        try:
            _db_info(argparse.Namespace(space=True, time=False, latest=5, display=getattr(args,'display','rich')))
        except Exception:
            pass
        return 0
    svcreset.set_defaults(func=_service_reset)

    # Monitor command: filesystem monitoring status and configuration
    mon = sub.add_parser("monitor", help="Filesystem monitoring status and configuration")
    monsub = mon.add_subparsers(dest="monitor_cmd", required=False)

    # monitor status
    monstatus = monsub.add_parser("status", help="Show monitoring statistics")
    def _monitor_status_cmd(args: argparse.Namespace) -> int:
        """Show monitoring statistics."""
        display = args.display_obj

        # Show config location and WKS_HOME
        from .utils import wks_home_path
        config_file = get_config_path()
        display.info(f"Reading config from: {config_file}")

        wks_home_display = os.environ.get("WKS_HOME", str(wks_home_path()))
        display.info(f"WKS_HOME: {wks_home_display}")

        cfg = load_config()

        # Get monitor config
        monitor_config = cfg.get("monitor", {})

        # Get database connection
        mongo_config = cfg.get("mongo", {})
        if not mongo_config:
            mongo_config = cfg.get("db", {})

        mongo_uri = mongo_config.get("uri", "mongodb://localhost:27017/")
        db_name = monitor_config.get("database", "wks")
        coll_name = monitor_config.get("collection", "monitor")

        # Run validation first
        include_paths = set(monitor_config.get("include_paths", []))
        exclude_paths = set(monitor_config.get("exclude_paths", []))
        managed_dirs = set(monitor_config.get("managed_directories", {}).keys())
        ignore_dirnames = monitor_config.get("ignore_dirnames", [])
        ignore_globs = monitor_config.get("ignore_globs", [])

        issues = []  # Inconsistencies (red)
        redundancies = []  # Redundant items (yellow)

        # Check for conflicts
        conflicts = include_paths & exclude_paths
        for path in conflicts:
            issues.append(f"Path in both include_paths and exclude_paths: {path}")

        # Check for duplicate managed directories (nested)
        from pathlib import Path as P
        managed_list = list(managed_dirs)
        for i, dir1 in enumerate(managed_list):
            p1 = P(dir1).expanduser().resolve()
            for dir2 in managed_list[i+1:]:
                p2 = P(dir2).expanduser().resolve()
                try:
                    if p1 == p2:
                        redundancies.append(f"Duplicate managed_directories: {dir1} and {dir2} resolve to same path")
                except:
                    pass

        # Check for redundant ignore_dirnames (matched by ignore_globs)
        for dirname in ignore_dirnames:
            is_valid, error_msg = MonitorValidator.validate_ignore_dirname(dirname, ignore_globs)
            if not is_valid:
                redundancies.append(f"ignore_dirnames entry '{dirname}': {error_msg}")

        # Check for invalid ignore_globs (syntax errors)
        for glob_pattern in ignore_globs:
            is_valid, error_msg = MonitorValidator.validate_ignore_glob(glob_pattern)
            if not is_valid:
                issues.append(f"ignore_globs entry '{glob_pattern}': {error_msg}")

        # Check for managed_directories that would not be monitored
        managed_dirs_dict = monitor_config.get("managed_directories", {})
        for managed_path in managed_dirs_dict.keys():
            is_valid, error_msg = MonitorValidator.validate_managed_directory(
                managed_path,
                list(include_paths),
                list(exclude_paths),
                ignore_dirnames,
                ignore_globs
            )
            if not is_valid:
                issues.append(f"managed_directories entry '{managed_path}' would NOT be monitored: {error_msg}")

        # Check for vault_path in include/exclude paths (vault is automatically ignored)
        vault_path = cfg.get("vault_path")
        if vault_path:
            vault_resolved = str(P(vault_path).expanduser().resolve())

            # Check if vault_path is EXPLICITLY in include_paths (ERROR - exact match only)
            for include_path in include_paths:
                include_resolved = str(P(include_path).expanduser().resolve())
                if vault_resolved == include_resolved:
                    issues.append(f"vault_path '{vault_path}' explicitly in include_paths - vault is automatically ignored")

            # Check if vault_path is in exclude_paths (WARNING - redundant)
            for exclude_path in exclude_paths:
                exclude_resolved = str(P(exclude_path).expanduser().resolve())
                if vault_resolved == exclude_resolved:
                    redundancies.append(f"exclude_paths entry '{exclude_path}' is redundant - vault_path is automatically ignored")

        # Check for ~/.wks and .wkso (automatically ignored system paths)
        wks_home = str(P("~/.wks").expanduser().resolve())

        # Check if ~/.wks is EXPLICITLY in include_paths (ERROR - exact match only)
        for include_path in include_paths:
            include_resolved = str(P(include_path).expanduser().resolve())
            if wks_home == include_resolved:
                issues.append(f"WKS home '~/.wks' explicitly in include_paths - WKS home is automatically ignored")

        # Check if ~/.wks is in exclude_paths (WARNING - redundant)
        for exclude_path in exclude_paths:
            exclude_resolved = str(P(exclude_path).expanduser().resolve())
            if wks_home == exclude_resolved:
                redundancies.append(f"exclude_paths entry '{exclude_path}' is redundant - WKS home is automatically ignored")

        # Check if .wkso is in ignore_dirnames (WARNING - redundant)
        if ".wkso" in ignore_dirnames:
            redundancies.append(f"ignore_dirnames entry '.wkso' is redundant - .wkso directories are automatically ignored")

        # Check if ~/.wks is in managed_directories (ERROR)
        for managed_path in managed_dirs_dict.keys():
            managed_resolved = str(P(managed_path).expanduser().resolve())
            if managed_resolved == wks_home or managed_resolved.startswith(wks_home + "/"):
                issues.append(f"managed_directories entry '{managed_path}' is in WKS home - cannot manage WKS home directory")

        # Connect to MongoDB
        display.status(f"Connecting to {db_name}.{coll_name}...")
        try:
            from pymongo import MongoClient
            client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
            client.server_info()
            db = client[db_name]
            coll = db[coll_name]

            # Get statistics
            total_files = coll.count_documents({})

            managed_dirs_dict = monitor_config.get("managed_directories", {})

            # Build table data
            status_data = [
                {"Setting": "Tracked Files", "Value": str(total_files)},
                {"Setting": "", "Value": ""},
                {"Setting": "managed_directories", "Value": str(len(managed_dirs_dict))},
            ]

            # Build sets of problematic paths for coloring
            red_paths = set()
            yellow_paths = set()
            for issue in issues:
                for path in managed_dirs_dict.keys() | include_paths | exclude_paths:
                    if f"'{path}'" in issue or f" {path}" in issue or issue.endswith(path):
                        red_paths.add(path)

            for redund in redundancies:
                for path in managed_dirs_dict.keys() | include_paths | exclude_paths:
                    if f"'{path}'" in redund or f" {path}" in redund or redund.endswith(path):
                        yellow_paths.add(path)

            # Calculate max pip count and max number width for alignment
            import math
            max_pip_count = 0
            max_num_width = 0
            for priority in managed_dirs_dict.values():
                if priority <= 1:
                    pip_count = 1
                else:
                    pip_count = int(math.log10(priority)) + 1
                max_pip_count = max(max_pip_count, pip_count)
                max_num_width = max(max_num_width, len(str(priority)))

            # Add managed directories with logarithmic pip visualization and validation
            for path, priority in sorted(managed_dirs_dict.items(), key=lambda x: -x[1]):
                # Create logarithmic pip visualization
                # 1 pip for <=1, 2 pips for <10, 3 pips for <100, 4 pips for <1000, etc.
                if priority <= 1:
                    pip_count = 1
                else:
                    pip_count = int(math.log10(priority)) + 1
                pips = "‚ñ™" * pip_count

                # Validate that this managed directory would be monitored
                is_valid, error_msg = MonitorValidator.validate_managed_directory(
                    path,
                    list(include_paths),
                    list(exclude_paths),
                    ignore_dirnames,
                    ignore_globs
                )

                # Collect validation messages and get status symbol
                if error_msg:
                    issues.append(f"managed_directories entry '{path}': {error_msg}")
                status_symbol = MonitorValidator.status_symbol(error_msg, is_valid)

                # Left-align pips, right-align numbers, add status symbol
                pips_padded = pips.ljust(max_pip_count)
                num_padded = str(priority).rjust(max_num_width)
                priority_display = f"{pips_padded} {num_padded} {status_symbol}"

                status_data.append({
                    "Setting": f"  {path}",
                    "Value": priority_display
                })

            status_data.append({"Setting": "", "Value": ""})
            status_data.append({"Setting": "include_paths", "Value": str(len(include_paths))})
            for path in sorted(include_paths):
                error_msg = None if path not in (red_paths | yellow_paths) else "issue"
                is_valid = path not in red_paths
                status_data.append({"Setting": f"  {path}", "Value": MonitorValidator.status_symbol(error_msg, is_valid)})

            status_data.append({"Setting": "", "Value": ""})
            status_data.append({"Setting": "exclude_paths", "Value": str(len(exclude_paths))})
            for path in sorted(exclude_paths):
                error_msg = None if path not in (red_paths | yellow_paths) else "issue"
                is_valid = path not in red_paths
                status_data.append({"Setting": f"  {path}", "Value": MonitorValidator.status_symbol(error_msg, is_valid)})

            # Build ignore rules list with validation
            ignore_list = []
            ignore_list.append(("ignore_dirnames", str(len(ignore_dirnames))))
            ignore_list.append(("", ""))

            # Validate each ignore_dirname
            for dirname in ignore_dirnames:
                is_valid, error_msg = MonitorValidator.validate_ignore_dirname(dirname, ignore_globs)
                ignore_list.append((f"  {dirname}", MonitorValidator.status_symbol(error_msg, is_valid)))
                if error_msg:
                    (redundancies if is_valid else issues).append(f"ignore_dirnames entry '{dirname}': {error_msg}")

            ignore_list.append(("", ""))
            ignore_list.append(("ignore_globs", str(len(ignore_globs))))

            # Validate each ignore_glob for syntax errors
            for glob_pattern in ignore_globs:
                is_valid, error_msg = MonitorValidator.validate_ignore_glob(glob_pattern)
                ignore_list.append((f"  {glob_pattern}", MonitorValidator.status_symbol(error_msg, is_valid)))
                if error_msg:
                    (redundancies if is_valid else issues).append(f"ignore_globs pattern '{glob_pattern}': {error_msg}")

            # Combine into single table with 4 columns
            max_rows = max(len(status_data), len(ignore_list))
            combined_data = []

            for i in range(max_rows):
                row = {}
                if i < len(status_data):
                    row["Setting"] = status_data[i]["Setting"]
                    row["Value"] = status_data[i]["Value"]
                else:
                    row["Setting"] = ""
                    row["Value"] = ""

                if i < len(ignore_list):
                    row["Ignore Rule"] = ignore_list[i][0]
                    row["Count"] = ignore_list[i][1]
                else:
                    row["Ignore Rule"] = ""
                    row["Count"] = ""

                combined_data.append(row)

            display.table(
                combined_data,
                headers=["Setting", "Value", "Ignore Rule", "Count"],
                title="Monitor Status",
                column_justify={"Value": "right", "Count": "right"}
            )

            # Print issues and redundancies
            if issues:
                display.error(f"\nInconsistencies found ({len(issues)}):")
                for issue in issues:
                    display.error(f"  ‚Ä¢ {issue}")

            if redundancies:
                display.warning(f"\nRedundancies found ({len(redundancies)}):")
                for redund in redundancies:
                    display.warning(f"  ‚Ä¢ {redund}")

            if not issues and not redundancies:
                display.success("\n‚úì No configuration issues found")

            client.close()
            return 0

        except Exception as e:
            display.error(f"Failed to get monitor status: {e}")
            return 2

    monstatus.set_defaults(func=_monitor_status_cmd)

    # monitor validate - check for inconsistencies
    monvalidate = monsub.add_parser("validate", help="Check for configuration inconsistencies")
    def _monitor_validate_cmd(args: argparse.Namespace) -> int:
        """Check for configuration inconsistencies."""
        display = args.display_obj
        cfg = load_config()
        monitor_config = cfg.get("monitor", {})

        include_paths = set(monitor_config.get("include_paths", []))
        exclude_paths = set(monitor_config.get("exclude_paths", []))
        managed_dirs = set(monitor_config.get("managed_directories", {}).keys())

        issues = []
        warnings = []

        # Check 1: Paths in both include and exclude
        conflicts = include_paths & exclude_paths
        if conflicts:
            for path in conflicts:
                issues.append(f"Path in both include and exclude: {path}")

        # Check 2: Duplicate managed directories (same resolved path)
        from pathlib import Path as P
        managed_list = list(managed_dirs)
        for i, dir1 in enumerate(managed_list):
            p1 = P(dir1).expanduser().resolve()
            for dir2 in managed_list[i+1:]:
                p2 = P(dir2).expanduser().resolve()
                try:
                    if p1 == p2:
                        warnings.append(f"Duplicate managed directories: {dir1} and {dir2} resolve to same path")
                except:
                    pass

        # Display results
        if not issues and not warnings:
            display.success("No configuration issues found")
            return 0

        if issues:
            display.error(f"Found {len(issues)} error(s):")
            for issue in issues:
                display.error(f"  ‚Ä¢ {issue}")

        if warnings:
            display.warning(f"Found {len(warnings)} warning(s):")
            for warning in warnings:
                display.warning(f"  ‚Ä¢ {warning}")

        return 1 if issues else 0

    monvalidate.set_defaults(func=_monitor_validate_cmd)

    # monitor check - test if a path would be monitored
    moncheck = monsub.add_parser("check", help="Check if a path would be monitored")
    moncheck.add_argument("path", help="Path to check")
    def _monitor_check_cmd(args: argparse.Namespace) -> int:
        """Check if a path would be monitored."""
        display = args.display_obj
        cfg = load_config()
        monitor_config = cfg.get("monitor", {})

        # Get configuration
        include_paths = monitor_config.get("include_paths", [])
        exclude_paths = monitor_config.get("exclude_paths", [])
        ignore_dirnames = monitor_config.get("ignore_dirnames", [])
        ignore_globs = monitor_config.get("ignore_globs", [])
        managed_dirs = monitor_config.get("managed_directories", {})
        priority_config = monitor_config.get("priority", {})

        # Resolve path
        from pathlib import Path as P
        test_path = P(args.path).expanduser().resolve()

        # Build decision chain
        decisions = []
        is_monitored = True
        reason = None
        priority = None

        # Step 1: Check if path exists (informational)
        if test_path.exists():
            decisions.append(("‚úì", f"Path exists: {test_path}"))
        else:
            decisions.append(("‚ö†", f"Path does not exist (checking as if it did): {test_path}"))

        # Step 2: Check include_paths
        included = False
        for include_path in include_paths:
            include_resolved = P(include_path).expanduser().resolve()
            try:
                # Check if test_path is under include_path
                test_path.relative_to(include_resolved)
                included = True
                decisions.append(("‚úì", f"Matches include_paths: {include_path}"))
                break
            except ValueError:
                continue

        if not included:
            is_monitored = False
            reason = "Not under any include_paths"
            decisions.append(("‚úó", reason))

        # Step 3: Check exclude_paths
        if is_monitored:
            for exclude_path in exclude_paths:
                exclude_resolved = P(exclude_path).expanduser().resolve()
                try:
                    test_path.relative_to(exclude_resolved)
                    is_monitored = False
                    reason = f"Matches exclude_paths: {exclude_path}"
                    decisions.append(("‚úó", reason))
                    break
                except ValueError:
                    continue

            if is_monitored:
                decisions.append(("‚úì", "Not in exclude_paths"))

        # Step 4: Check ignore_dirnames
        if is_monitored:
            path_parts = test_path.parts
            for part in path_parts:
                if part in ignore_dirnames:
                    is_monitored = False
                    reason = f"Directory name '{part}' in ignore_dirnames"
                    decisions.append(("‚úó", reason))
                    break

            if is_monitored:
                decisions.append(("‚úì", "No directory names match ignore_dirnames"))

        # Step 5: Check ignore_globs
        if is_monitored:
            import fnmatch
            matched_glob = None
            for glob_pattern in ignore_globs:
                # Check against full path
                if fnmatch.fnmatch(str(test_path), glob_pattern):
                    matched_glob = glob_pattern
                    break
                # Check against filename only
                if fnmatch.fnmatch(test_path.name, glob_pattern):
                    matched_glob = glob_pattern
                    break

            if matched_glob:
                is_monitored = False
                reason = f"Matches ignore_globs: {matched_glob}"
                decisions.append(("‚úó", reason))
            else:
                decisions.append(("‚úì", "Does not match ignore_globs"))

        # Step 6: Calculate priority if monitored
        if is_monitored:
            from wks.priority import calculate_priority
            priority = calculate_priority(test_path, managed_dirs, priority_config)
            decisions.append(("‚úì", f"Priority score: {priority}"))

            # Show which managed directory matched
            from wks.priority import find_managed_directory
            matched_dir, base_priority = find_managed_directory(test_path, managed_dirs)
            if matched_dir:
                decisions.append(("‚Ñπ", f"Managed directory: {matched_dir} (base priority {base_priority})"))

        # Display results
        if is_monitored:
            display.success(f"Path WOULD be monitored: {test_path}")
            if priority:
                display.info(f"Priority: {priority}")
        else:
            display.error(f"Path would NOT be monitored: {test_path}")
            if reason:
                display.error(f"Reason: {reason}")

        # Show decision chain
        display.info("\nDecision chain:")
        for symbol, message in decisions:
            if symbol == "‚úì":
                display.success(f"  {message}")
            elif symbol == "‚úó":
                display.error(f"  {message}")
            elif symbol == "‚ö†":
                display.warning(f"  {message}")
            else:
                display.info(f"  {message}")

        return 0 if is_monitored else 1

    moncheck.set_defaults(func=_monitor_check_cmd)

    # Helper classes for monitor validation
    # Helper function for modifying config lists

    def _modify_monitor_list(display, list_name: str, value: str, operation: str, resolve_path: bool = True) -> int:
        """Modify a monitor config list (add/remove)."""
        config_path = get_config_path()

        if not config_path.exists():
            display.error(f"Config file not found: {config_path}")
            return 2

        # Read current config
        with open(config_path) as f:
            cfg = json.load(f)

        # Get monitor section
        if "monitor" not in cfg:
            cfg["monitor"] = {}

        if list_name not in cfg["monitor"]:
            cfg["monitor"][list_name] = []

        # Normalize path for comparison if needed
        if resolve_path:
            # Resolve the input path
            value_resolved = str(Path(value).expanduser().resolve())

            # Preserve tilde notation if the resolved path is in home directory
            home_dir = str(Path.home())
            if value_resolved.startswith(home_dir):
                value_to_store = "~" + value_resolved[len(home_dir):]
            else:
                value_to_store = value_resolved

            # Find if this path exists in the list (comparing resolved versions)
            existing_entry = None
            for entry in cfg["monitor"][list_name]:
                entry_resolved = str(Path(entry).expanduser().resolve())
                if entry_resolved == value_resolved:
                    existing_entry = entry
                    break
        else:
            value_resolved = value
            value_to_store = value
            existing_entry = value if value in cfg["monitor"][list_name] else None

        # Perform operation
        if operation == "add":
            # Validate ignore_dirnames before adding
            if list_name == "ignore_dirnames":
                ignore_globs = cfg["monitor"].get("ignore_globs", [])
                is_valid, error_msg = MonitorValidator.validate_ignore_dirname(value_resolved if not resolve_path else value, ignore_globs)
                if not is_valid:
                    display.error(error_msg)
                    return 1

            if existing_entry:
                display.warning(f"Already in {list_name}: {existing_entry}")
                return 0

            # Store using tilde notation when possible
            cfg["monitor"][list_name].append(value_to_store)
            display.success(f"Added to {list_name}: {value_to_store}")
        elif operation == "remove":
            if not existing_entry:
                display.warning(f"Not in {list_name}: {value}")
                return 0
            cfg["monitor"][list_name].remove(existing_entry)
            display.success(f"Removed from {list_name}: {existing_entry}")

        # Write back
        with open(config_path, "w") as f:
            json.dump(cfg, f, indent=4)

        display.info("Restart the monitor service for changes to take effect")
        return 0

    # Helper function to show a list
    def _show_monitor_list(display, list_name: str, title: str) -> int:
        """Show contents of a monitor config list with validation status."""
        cfg = load_config()
        monitor_config = cfg.get("monitor", {})
        items = monitor_config.get(list_name, [])

        if not items:
            display.info(f"No {list_name} configured")
            return 0

        # Get other config items for validation
        ignore_globs = monitor_config.get("ignore_globs", [])
        ignore_dirnames = monitor_config.get("ignore_dirnames", [])
        include_paths = monitor_config.get("include_paths", [])
        exclude_paths = monitor_config.get("exclude_paths", [])
        managed_dirs = monitor_config.get("managed_directories", {})

        from pathlib import Path as P

        table_data = []
        for i, item in enumerate(items, 1):
            is_valid, error_msg = True, None

            if list_name == "ignore_dirnames":
                is_valid, error_msg = MonitorValidator.validate_ignore_dirname(item, ignore_globs)
            elif list_name == "ignore_globs":
                is_valid, error_msg = MonitorValidator.validate_ignore_glob(item)
            elif list_name in ("include_paths", "exclude_paths"):
                try:
                    path_obj = P(item).expanduser().resolve()
                    if not path_obj.exists():
                        is_valid = list_name == "exclude_paths"  # Warning for exclude, error for include
                        error_msg = "Path does not exist" + (" (will be ignored if created)" if is_valid else "")
                    elif not path_obj.is_dir():
                        is_valid, error_msg = False, "Not a directory"
                except Exception as e:
                    is_valid, error_msg = False, f"Invalid path: {e}"

            table_data.append({"#": str(i), "Value": item, "Status": MonitorValidator.status_symbol(error_msg, is_valid)})

        display.table(table_data, title=title)
        return 0

    # monitor include_paths add/remove/list
    mon_include = monsub.add_parser("include_paths", help="Manage include_paths")
    mon_include_sub = mon_include.add_subparsers(dest="include_paths_op", required=False)

    # Default action: show list
    def _monitor_include_default(args: argparse.Namespace) -> int:
        if not args.include_paths_op:
            return _show_monitor_list(args.display_obj, "include_paths", "Include Paths")
        return 0
    mon_include.set_defaults(func=_monitor_include_default)

    mon_include_add = mon_include_sub.add_parser("add", help="Add path(s) to include_paths")
    mon_include_add.add_argument("paths", nargs='+', help="Path(s) to monitor")
    def _monitor_include_add(args: argparse.Namespace) -> int:
        for path in args.paths:
            result = _modify_monitor_list(args.display_obj, "include_paths", path, "add", resolve_path=True)
            if result != 0:
                return result
        return 0
    mon_include_add.set_defaults(func=_monitor_include_add)

    mon_include_remove = mon_include_sub.add_parser("remove", help="Remove path(s) from include_paths")
    mon_include_remove.add_argument("paths", nargs='+', help="Path(s) to remove")
    def _monitor_include_remove(args: argparse.Namespace) -> int:
        for path in args.paths:
            result = _modify_monitor_list(args.display_obj, "include_paths", path, "remove", resolve_path=True)
            if result != 0:
                return result
        return 0
    mon_include_remove.set_defaults(func=_monitor_include_remove)

    # monitor exclude_paths add/remove
    mon_exclude = monsub.add_parser("exclude_paths", help="Manage exclude_paths")
    mon_exclude_sub = mon_exclude.add_subparsers(dest="exclude_paths_op", required=False)

    def _monitor_exclude_default(args: argparse.Namespace) -> int:
        if not args.exclude_paths_op:
            return _show_monitor_list(args.display_obj, "exclude_paths", "Exclude Paths")
        return 0
    mon_exclude.set_defaults(func=_monitor_exclude_default)

    mon_exclude_add = mon_exclude_sub.add_parser("add", help="Add path(s) to exclude_paths")
    mon_exclude_add.add_argument("paths", nargs='+', help="Path(s) to exclude")
    def _monitor_exclude_add(args: argparse.Namespace) -> int:
        for path in args.paths:
            result = _modify_monitor_list(args.display_obj, "exclude_paths", path, "add", resolve_path=True)
            if result != 0:
                return result
        return 0
    mon_exclude_add.set_defaults(func=_monitor_exclude_add)

    mon_exclude_remove = mon_exclude_sub.add_parser("remove", help="Remove path(s) from exclude_paths")
    mon_exclude_remove.add_argument("paths", nargs='+', help="Path(s) to remove")
    def _monitor_exclude_remove(args: argparse.Namespace) -> int:
        for path in args.paths:
            result = _modify_monitor_list(args.display_obj, "exclude_paths", path, "remove", resolve_path=True)
            if result != 0:
                return result
        return 0
    mon_exclude_remove.set_defaults(func=_monitor_exclude_remove)

    # monitor ignore_dirnames add/remove
    mon_ignore_dir = monsub.add_parser("ignore_dirnames", help="Manage ignore_dirnames")
    mon_ignore_dir_sub = mon_ignore_dir.add_subparsers(dest="ignore_dirnames_op", required=False)

    def _monitor_ignore_dir_default(args: argparse.Namespace) -> int:
        if not args.ignore_dirnames_op:
            return _show_monitor_list(args.display_obj, "ignore_dirnames", "Ignore Directory Names")
        return 0
    mon_ignore_dir.set_defaults(func=_monitor_ignore_dir_default)

    mon_ignore_dir_add = mon_ignore_dir_sub.add_parser("add", help="Add directory name(s) to ignore_dirnames")
    mon_ignore_dir_add.add_argument("dirnames", nargs='+', help="Directory name(s) to ignore (e.g., node_modules)")
    def _monitor_ignore_dir_add(args: argparse.Namespace) -> int:
        for dirname in args.dirnames:
            result = _modify_monitor_list(args.display_obj, "ignore_dirnames", dirname, "add", resolve_path=False)
            if result != 0:
                return result
        return 0
    mon_ignore_dir_add.set_defaults(func=_monitor_ignore_dir_add)

    mon_ignore_dir_remove = mon_ignore_dir_sub.add_parser("remove", help="Remove directory name(s) from ignore_dirnames")
    mon_ignore_dir_remove.add_argument("dirnames", nargs='+', help="Directory name(s) to remove")
    def _monitor_ignore_dir_remove(args: argparse.Namespace) -> int:
        for dirname in args.dirnames:
            result = _modify_monitor_list(args.display_obj, "ignore_dirnames", dirname, "remove", resolve_path=False)
            if result != 0:
                return result
        return 0
    mon_ignore_dir_remove.set_defaults(func=_monitor_ignore_dir_remove)

    # monitor ignore_globs add/remove
    mon_ignore_glob = monsub.add_parser("ignore_globs", help="Manage ignore_globs")
    mon_ignore_glob_sub = mon_ignore_glob.add_subparsers(dest="ignore_globs_op", required=False)

    def _monitor_ignore_glob_default(args: argparse.Namespace) -> int:
        if not args.ignore_globs_op:
            return _show_monitor_list(args.display_obj, "ignore_globs", "Ignore Glob Patterns")
        return 0
    mon_ignore_glob.set_defaults(func=_monitor_ignore_glob_default)

    mon_ignore_glob_add = mon_ignore_glob_sub.add_parser("add", help="Add glob pattern(s) to ignore_globs")
    mon_ignore_glob_add.add_argument("patterns", nargs='+', help="Glob pattern(s) to ignore (e.g., *.tmp)")
    def _monitor_ignore_glob_add(args: argparse.Namespace) -> int:
        for pattern in args.patterns:
            result = _modify_monitor_list(args.display_obj, "ignore_globs", pattern, "add", resolve_path=False)
            if result != 0:
                return result
        return 0
    mon_ignore_glob_add.set_defaults(func=_monitor_ignore_glob_add)

    mon_ignore_glob_remove = mon_ignore_glob_sub.add_parser("remove", help="Remove glob pattern(s) from ignore_globs")
    mon_ignore_glob_remove.add_argument("patterns", nargs='+', help="Pattern(s) to remove")
    def _monitor_ignore_glob_remove(args: argparse.Namespace) -> int:
        for pattern in args.patterns:
            result = _modify_monitor_list(args.display_obj, "ignore_globs", pattern, "remove", resolve_path=False)
            if result != 0:
                return result
        return 0
    mon_ignore_glob_remove.set_defaults(func=_monitor_ignore_glob_remove)

    # monitor managed add/remove/set-priority
    mon_managed = monsub.add_parser("managed", help="Manage managed_directories with priorities")
    mon_managed_sub = mon_managed.add_subparsers(dest="managed_op", required=False)

    def _monitor_managed_default(args: argparse.Namespace) -> int:
        if not args.managed_op:
            cfg = load_config()
            monitor_config = cfg.get("monitor", {})
            managed_dirs = monitor_config.get("managed_directories", {})

            if not managed_dirs:
                args.display_obj.info("No managed_directories configured")
                return 0

            table_data = []
            for path, priority in sorted(managed_dirs.items(), key=lambda x: -x[1]):
                table_data.append({"Path": path, "Priority": str(priority)})

            args.display_obj.table(table_data, title="Managed Directories")
            return 0
        return 0
    mon_managed.set_defaults(func=_monitor_managed_default)

    mon_managed_add = mon_managed_sub.add_parser("add", help="Add managed directory with priority")
    mon_managed_add.add_argument("path", help="Directory path")
    mon_managed_add.add_argument("--priority", type=int, required=True, help="Priority score (e.g., 100)")
    def _monitor_managed_add(args: argparse.Namespace) -> int:
        config_path = get_config_path()

        if not config_path.exists():
            args.display_obj.error(f"Config file not found: {config_path}")
            return 2

        path = str(Path(args.path).expanduser().resolve())

        with open(config_path) as f:
            cfg = json.load(f)

        if "monitor" not in cfg:
            cfg["monitor"] = {}
        if "managed_directories" not in cfg["monitor"]:
            cfg["monitor"]["managed_directories"] = {}

        cfg["monitor"]["managed_directories"][path] = args.priority

        with open(config_path, "w") as f:
            json.dump(cfg, f, indent=4)

        args.display_obj.success(f"Added managed directory: {path} (priority {args.priority})")
        args.display_obj.info("Restart the monitor service for changes to take effect")
        return 0
    mon_managed_add.set_defaults(func=_monitor_managed_add)

    mon_managed_remove = mon_managed_sub.add_parser("remove", help="Remove managed directory")
    mon_managed_remove.add_argument("path", help="Directory path to remove")
    def _monitor_managed_remove(args: argparse.Namespace) -> int:
        config_path = get_config_path()

        if not config_path.exists():
            args.display_obj.error(f"Config file not found: {config_path}")
            return 2

        path = str(Path(args.path).expanduser().resolve())

        with open(config_path) as f:
            cfg = json.load(f)

        if "monitor" not in cfg or "managed_directories" not in cfg["monitor"]:
            args.display_obj.warning("No managed_directories configured")
            return 0

        if path not in cfg["monitor"]["managed_directories"]:
            args.display_obj.warning(f"Not a managed directory: {path}")
            return 0

        del cfg["monitor"]["managed_directories"][path]

        with open(config_path, "w") as f:
            json.dump(cfg, f, indent=4)

        args.display_obj.success(f"Removed managed directory: {path}")
        args.display_obj.info("Restart the monitor service for changes to take effect")
        return 0
    mon_managed_remove.set_defaults(func=_monitor_managed_remove)

    mon_managed_priority = mon_managed_sub.add_parser("set-priority", help="Set priority for managed directory")
    mon_managed_priority.add_argument("path", help="Directory path")
    mon_managed_priority.add_argument("priority", type=int, help="New priority score")
    def _monitor_managed_priority(args: argparse.Namespace) -> int:
        config_path = get_config_path()

        if not config_path.exists():
            args.display_obj.error(f"Config file not found: {config_path}")
            return 2

        path = str(Path(args.path).expanduser().resolve())

        with open(config_path) as f:
            cfg = json.load(f)

        if "monitor" not in cfg or "managed_directories" not in cfg["monitor"]:
            args.display_obj.error("No managed_directories configured")
            return 2

        if path not in cfg["monitor"]["managed_directories"]:
            args.display_obj.error(f"Not a managed directory: {path}")
            return 2

        old_priority = cfg["monitor"]["managed_directories"][path]
        cfg["monitor"]["managed_directories"][path] = args.priority

        with open(config_path, "w") as f:
            json.dump(cfg, f, indent=4)

        args.display_obj.success(f"Updated priority: {path} ({old_priority} ‚Üí {args.priority})")
        args.display_obj.info("Restart the monitor service for changes to take effect")
        return 0
    mon_managed_priority.set_defaults(func=_monitor_managed_priority)

    # Extract text without indexing
    extp = sub.add_parser("extract", help="Extract document text using the configured pipeline")
    extp.add_argument("paths", nargs="+", help="Files or directories to extract")
    def _extract_cmd(args: argparse.Namespace) -> int:
        cfg = load_config()
        include_exts = [e.lower() for e in (cfg.get('similarity', {}).get('include_extensions') or [])]
        files = _iter_files(args.paths, include_exts, cfg)
        if not files:
            print("No files to extract (check paths/extensions)")
            return 0
        extractor = _build_extractor(cfg)
        extracted = 0
        skipped = 0
        errors = 0
        outputs: List[Tuple[Path, Path]] = []
        with _make_progress(total=len(files), display=args.display) as prog:
            for f in files:
                prog.update(f.name, advance=0)
                try:
                    result = extractor.extract(f, persist=True)
                    if result.content_path:
                        extracted += 1
                        outputs.append((f, Path(result.content_path)))
                    else:
                        skipped += 1
                except Exception:
                    errors += 1
                finally:
                    prog.update(f.name, advance=1)
        for src, artefact in outputs:
            print(f"{src} -> {artefact}")
        print(f"Extracted {extracted} file(s), skipped {skipped}, errors {errors}")
        return 0
    extp.set_defaults(func=_extract_cmd)

    # Top-level index command (moved out of analyze)
    idx = sub.add_parser("index", help="Index files or directories (recursive) into similarity DB with progress")
    idx.add_argument("--untrack", action="store_true", help="Remove tracked entries (and artefacts) instead of indexing")
    idx.add_argument("paths", nargs="+", help="Files or directories to process")
    def _index_cmd(args: argparse.Namespace) -> int:
        cfg = load_config()
        include_exts = [e.lower() for e in (cfg.get('similarity', {}).get('include_extensions') or [])]
        files = _iter_files(args.paths, include_exts, cfg)
        if not files:
            print("No files to process (check paths/extensions)")
            return 0

        display_mode = args.display

        if args.untrack:
            removed = 0
            missing = 0
            errors = 0
            outcomes: List[Dict[str, Any]] = []
            with _make_progress(total=len(files), display=args.display) as prog:
                prog.update("Connecting to DB‚Ä¶", advance=0)
                db, _ = _load_similarity_required()
                for f in files:
                    prog.update(f"{f.name} ‚Ä¢ untrack", advance=0)
                    try:
                        if db.remove_file(f):
                            removed += 1
                            outcomes.append({"path": str(f), "status": "removed"})
                        else:
                            missing += 1
                            outcomes.append({"path": str(f), "status": "not_tracked"})
                    except Exception as exc:
                        errors += 1
                        outcomes.append({"path": str(f), "status": f"error: {exc}"})
                    finally:
                        prog.update(f.name, advance=1)
                prog.update("Untracking complete", advance=0)
            payload = {
                "mode": "untrack",
                "requested": [str(p) for p in files],
                "removed": removed,
                "missing": missing,
                "errors": errors,
                "files": outcomes,
            }
            _maybe_write_json(args, payload)
            print(f"Untracked {removed} file(s), missing {missing}, errors {errors}")
            return 0

        hash_times: Dict[Path, Optional[float]] = {}
        checksums: Dict[Path, Optional[str]] = {}
        file_sizes: Dict[Path, Optional[int]] = {}
        for f in files:
            try:
                h_start = time.perf_counter()
                checksum = _file_checksum(f)
                hash_times[f] = time.perf_counter() - h_start
                checksums[f] = checksum
            except Exception:
                hash_times[f] = None
                checksums[f] = None
            try:
                file_sizes[f] = f.stat().st_size
            except Exception:
                file_sizes[f] = None

        pre_skipped: List[Path] = []
        files_to_process = list(files)
        try:
            client, mongo_cfg = _mongo_client_params(
                server_timeout=300,
                connect_timeout=300,
                cfg=cfg,
                ensure_running=False,
            )
        except Exception:
            client = None
            mongo_cfg = None
        if client is not None and mongo_cfg is not None:
            try:
                coll = client[mongo_cfg['space_database']][mongo_cfg['space_collection']]
                to_process: List[Path] = []
                for f in files:
                    checksum = checksums.get(f)
                    if checksum is None:
                        to_process.append(f)
                        continue
                    doc = coll.find_one({"path": _as_file_uri_local(f)})
                    try:
                        record_db_activity("index.precheck", str(f))
                    except Exception:
                        pass
                    if not doc:
                        doc = coll.find_one({"path_local": str(f.resolve())})
                    if doc and doc.get("checksum") == checksum:
                        pre_skipped.append(f)
                    else:
                        to_process.append(f)
                files_to_process = to_process
            finally:
                try:
                    client.close()
                except Exception:
                    pass

        def _fmt_duration(seconds: Optional[float]) -> str:
            if seconds is None:
                return "‚Äî"
            if seconds >= 1:
                return f"{seconds:6.2f} {'s':>2}"
            return f"{seconds * 1000:6.1f} {'ms':>2}"

        stage_labels = [
            ("hash", "Hash"),
            ("extract", "Extract"),
            ("embed", "Embed"),
            ("db", "DB"),
            ("chunks", "Chunks"),
            ("obsidian", "Obsidian"),
        ]

        def _truncate_cell(text: str, limit: Optional[int]) -> str:
            if limit is None or limit <= 0 or len(text) <= limit:
                return text
            if limit == 1:
                return text[:1]
            return text[: limit - 1] + "‚Ä¶"

        def _format_cell(text: str, width: int, align: str) -> str:
            if align == "right":
                return text.rjust(width)
            if align == "center":
                return text.center(width)
            return text.ljust(width)

        def _render_plain_boxed_table(
            title: str,
            header: List[str],
            rows: List[List[str]],
            align: List[str],
            limits: Optional[Dict[int, int]] = None,
        ) -> None:
            if not rows:
                return
            limits = limits or {}
            processed: List[List[str]] = []
            for row in rows:
                processed_row: List[str] = []
                for idx, cell in enumerate(row):
                    text = str(cell)
                    text = _truncate_cell(text, limits.get(idx))
                    processed_row.append(text)
                processed.append(processed_row)
            widths: List[int] = []
            for idx, head in enumerate(header):
                col_width = len(str(head))
                for row in processed:
                    col_width = max(col_width, len(row[idx]))
                limit = limits.get(idx)
                if limit:
                    col_width = min(col_width, limit)
                widths.append(col_width)
            total_width = sum(widths) + 3 * (len(widths) - 1)
            border = "+" + "-" * (total_width + 2) + "+"
            print()
            print(border)
            print("| " + title.center(total_width) + " |")
            print(border)
            header_line = " | ".join(
                _format_cell(str(head), widths[idx], "center" if align[idx] == "right" else align[idx])
                for idx, head in enumerate(header)
            )
            print("| " + header_line + " |")
            separator = "-+-".join("-" * width for width in widths)
            print("| " + separator + " |")
            for row in processed:
                line = " | ".join(
                    _format_cell(cell, widths[idx], align[idx]) for idx, cell in enumerate(row)
                )
                print("| " + line + " |")
            print(border)

        def _render_timing_summary(entries: List[Dict[str, Any]], display_mode: str, fmt) -> None:
            if not entries:
                return
            if display_mode == "none":
                return
            totals: Dict[str, float] = {key: 0.0 for key, _ in stage_labels}
            counts: Dict[str, int] = {key: 0 for key, _ in stage_labels}
            for entry in entries:
                for key, _ in stage_labels:
                    val = entry.get(key)
                    if isinstance(val, (int, float)):
                        totals[key] += val
                        counts[key] += 1

            use_rich = display_mode in {"rich", "plain"}

            if use_rich:
                try:
                    from rich import box
                    from rich.console import Console
                    from rich.panel import Panel
                    from rich.table import Table
                except Exception:
                    use_rich = False
                if use_rich:
                    console = Console(
                        force_terminal=True,
                        color_system=None if display_mode == "rich" else "standard",
                        markup=(display_mode == "rich"),
                        highlight=False,
                        soft_wrap=False,
                    )
                    console.print()
                    detail = Table(
                        show_header=True,
                        header_style="bold" if display_mode == "rich" else "",
                        expand=False,
                        box=box.SQUARE if display_mode == "rich" else box.SIMPLE,
                        pad_edge=False,
                    )
                    detail.add_column("#", justify="right", no_wrap=True, overflow="ignore", min_width=2, max_width=3)
                    detail.add_column(
                        "File",
                        style="cyan" if display_mode == "rich" else "",
                        no_wrap=False,
                        overflow="fold",
                        min_width=12,
                        max_width=28,
                    )
                    detail.add_column(
                        "Status",
                        style="magenta" if display_mode == "rich" else "",
                        no_wrap=False,
                        overflow="fold",
                        min_width=8,
                        max_width=20,
                    )
                    for _, label in stage_labels:
                        detail.add_column(label, justify="right", no_wrap=True, overflow="ignore", min_width=9, max_width=11)
                    for idx, entry in enumerate(entries, 1):
                        row = [str(idx), entry['path'].name, entry['status']]
                        for key, _ in stage_labels:
                            row.append(fmt(entry.get(key)))
                        detail.add_row(*row)
                    total_files = 0
                    for value in counts.values():
                        if value > total_files:
                            total_files = value
                    if not total_files:
                        total_files = len(entries)
                    total_row = ["-", "Totals", f"{total_files} file(s)"]
                    for key, _ in stage_labels:
                        total_row.append(fmt(totals[key] if counts.get(key) else None))
                    detail.add_row(*total_row, style="bold")
                    width = console.width or 80
                    console.print(Panel.fit(detail, title="Timing Details", border_style="dim"), width=min(max(width, 72), 110))
                    return

            header = ["#", "File", "Status"] + [label for _, label in stage_labels]
            align = ["right", "left", "left"] + ["right"] * len(stage_labels)
            limits = {1: 32, 2: 20}
            details_rows: List[List[str]] = []
            for idx, entry in enumerate(entries, 1):
                row = [str(idx), entry['path'].name, entry['status']]
                for key, _ in stage_labels:
                    row.append(fmt(entry.get(key)))
                details_rows.append(row)
            total_files_plain = 0
            for value in counts.values():
                if value > total_files_plain:
                    total_files_plain = value
            if not total_files_plain:
                total_files_plain = len(entries)
            totals_row = ["-", "Totals", f"{total_files_plain} file(s)"]
            for key, _ in stage_labels:
                totals_row.append(fmt(totals[key] if counts.get(key) else None))
            details_rows.append(totals_row)
            _render_plain_boxed_table("Timing Details", header, details_rows, align, limits)
            return

        if not files_to_process:
            skipped = len(pre_skipped)
            total_files = None
            db_summary: Optional[Dict[str, Any]] = None
            try:
                client, mongo_cfg = _mongo_client_params(
                    server_timeout=300,
                    connect_timeout=300,
                    cfg=cfg,
                    ensure_running=False,
                )
                coll = client[mongo_cfg['space_database']][mongo_cfg['space_collection']]
                total_files = coll.count_documents({})
                db_summary = {
                    "database": mongo_cfg['space_database'],
                    "collection": mongo_cfg['space_collection'],
                    "total_files": total_files,
                }
            except Exception:
                db_summary = None
            finally:
                if client is not None:
                    try:
                        client.close()
                    except Exception:
                        pass

            cached_summaries = [
                {
                    "path": f,
                    "status": "cached",
                    "hash": hash_times.get(f),
                    "extract": None,
                    "embed": None,
                    "db": None,
                    "chunks": None,
                    "obsidian": None,
                }
                for f in pre_skipped
            ]

            payload = {
                "mode": "index",
                "requested": [str(p) for p in files],
                "added": 0,
                "skipped": skipped,
                "errors": 0,
                "files": [
                    {
                        "path": str(entry["path"]),
                        "status": entry["status"],
                        "timings": {
                            "hash": entry.get("hash"),
                            "extract": entry.get("extract"),
                            "embed": entry.get("embed"),
                            "db": entry.get("db"),
                            "chunks": entry.get("chunks"),
                            "obsidian": entry.get("obsidian"),
                        },
                    }
                    for entry in cached_summaries
                ],
            }
            if db_summary:
                payload["database"] = db_summary
            _maybe_write_json(args, payload)

            print("Nothing to index; all files already current.")
            print(f"Indexed 0 file(s), skipped {skipped}, errors 0")
            if db_summary:
                print(
                    f"DB: {db_summary['database']}.{db_summary['collection']} total_files={db_summary['total_files']}"
                )
            if cached_summaries:
                _render_timing_summary(cached_summaries, display_mode, _fmt_duration)
            return 0

        db = None
        extractor = None
        vault = None
        docs_keep = int((cfg.get('obsidian') or {}).get('docs_keep', 99))
        added = 0
        skipped = len(pre_skipped)
        errors = 0
        summaries: List[Dict[str, Any]] = []
        with _make_progress(total=len(files_to_process), display=args.display) as prog:
            prog.update(f"Pre-checking {len(files)} file(s)‚Ä¶", advance=0)
            prog.update("Connecting to DB‚Ä¶", advance=0)
            db, _ = _load_similarity_required()
            extractor = _build_extractor(cfg)
            vault = _load_vault()
            for f in files_to_process:
                prog.update(f"{f.name} ‚Ä¢ extract", advance=0)
                try:
                    extract_start = time.perf_counter()
                    extraction = extractor.extract(f, persist=True)
                    extract_time = time.perf_counter() - extract_start
                except Exception as exc:
                    errors += 1
                    prog.update(f.name, advance=1)
                    summaries.append({
                        "path": f,
                        "status": f"error: {exc}",
                        "hash": hash_times.get(f),
                        "extract": None,
                        "embed": None,
                        "db": None,
                        "chunks": None,
                        "obsidian": None,
                    })
                    continue

                prog.update(f"{f.name} ‚Ä¢ embed", advance=0)
                updated = False
                rec_timings: Dict[str, float] = {}
                kwargs: Dict[str, Any] = {}
                checksum_value = checksums.get(f)
                if checksum_value is not None:
                    kwargs['file_checksum'] = checksum_value
                size_value = file_sizes.get(f)
                if size_value is not None:
                    kwargs['file_bytes'] = size_value
                try:
                    updated = db.add_file(
                        f,
                        extraction=extraction,
                        **kwargs,
                    )
                    rec = db.get_last_add_result() or {}
                    rec_timings = rec.get('timings') or {}
                except Exception:
                    errors += 1
                    prog.update(f.name, advance=1)
                    summaries.append({
                        "path": f,
                        "status": "error",
                        "hash": hash_times.get(f),
                        "extract": extract_time,
                        "embed": None,
                        "db": None,
                        "chunks": None,
                        "obsidian": None,
                    })
                    continue

                obsidian_time: Optional[float] = None
                if updated:
                    added += 1
                    rec = db.get_last_add_result() or {}
                    ch = rec.get('content_checksum') or rec.get('content_hash')
                    txt = rec.get('text')
                    if ch and txt is not None:
                        try:
                            prog.update(f"{f.name} ‚Ä¢ obsidian", advance=0)
                            obs_start = time.perf_counter()
                            vault.write_doc_text(ch, f, txt, keep=docs_keep)
                            obsidian_time = time.perf_counter() - obs_start
                        except Exception:
                            obsidian_time = None
                else:
                    skipped += 1

                prog.update(f.name, advance=1)
                summaries.append({
                    "path": f,
                    "status": "updated" if updated else "unchanged",
                    "hash": hash_times.get(f),
                    "extract": extract_time,
                    "embed": rec_timings.get('embed'),
                    "db": rec_timings.get('db_update'),
                    "chunks": rec_timings.get('chunks'),
                    "obsidian": obsidian_time,
                })
            prog.update("DB update complete", advance=0)

        for f in pre_skipped:
            summaries.append({
                "path": f,
                "status": "cached",
                "hash": hash_times.get(f),
                "extract": None,
                "embed": None,
                "db": None,
                "chunks": None,
                "obsidian": None,
            })

        db_payload: Optional[Dict[str, Any]] = None
        try:
            stats = db.get_stats()
            if stats:
                db_payload = {
                    "database": stats.get("database"),
                    "collection": stats.get("collection"),
                    "total_files": stats.get("total_files"),
                    "total_bytes": stats.get("total_bytes"),
                }
        except Exception:
            stats = None
        payload = {
            "mode": "index",
            "requested": [str(p) for p in files],
            "added": added,
            "skipped": skipped,
            "errors": errors,
            "files": [
                {
                    "path": str(entry["path"]),
                    "status": entry["status"],
                    "timings": {
                        "hash": entry.get("hash"),
                        "extract": entry.get("extract"),
                        "embed": entry.get("embed"),
                        "db": entry.get("db"),
                        "chunks": entry.get("chunks"),
                        "obsidian": entry.get("obsidian"),
                    },
                }
                for entry in summaries
            ],
        }
        if db_payload:
            payload["database"] = db_payload
        _maybe_write_json(args, payload)

        print(f"Indexed {added} file(s), skipped {skipped}, errors {errors}")

        if summaries:
            _render_timing_summary(summaries, display_mode, _fmt_duration)
        if db_payload:
            total_bytes = db_payload.get("total_bytes")
            if total_bytes is not None:
                print(
                    f"DB: {db_payload['database']}.{db_payload['collection']} total_files={db_payload['total_files']} total_bytes={total_bytes}"
                )
            else:
                print(
                    f"DB: {db_payload['database']}.{db_payload['collection']} total_files={db_payload['total_files']}"
                )
        return 0
    idx.set_defaults(func=_index_cmd)

    # Related command: find semantically similar documents
    rel = sub.add_parser("related", help="Find semantically similar documents")
    rel.add_argument("path", help="Reference file to find similar documents for")
    rel.add_argument("--limit", type=int, default=10, help="Maximum number of results (default: 10)")
    rel.add_argument("--min-similarity", type=float, default=0.0, help="Minimum similarity threshold 0.0-1.0 (default: 0.0)")
    rel.add_argument("--format", choices=["table", "json"], default="table", help="Output format (default: table)")
    def _related_cmd(args: argparse.Namespace) -> int:
        """Find semantically similar documents."""
        from pathlib import Path

        display = args.display_obj

        # Parse input path
        query_path = Path(args.path).expanduser().resolve()
        if not query_path.exists():
            display.error(f"File not found: {query_path}")
            return 2

        # Load similarity DB
        display.status("Loading similarity database...")
        try:
            db, _ = _load_similarity_required()
            display.success("Connected to database")
        except SystemExit as e:
            return e.code if isinstance(e.code, int) else 1
        except Exception as e:
            display.error(f"Error loading similarity database: {e}")
            return 2

        # Find similar documents
        display.status(f"Finding similar documents to: {query_path.name}")
        try:
            results = db.find_similar(
                query_path=query_path,
                limit=args.limit,
                min_similarity=args.min_similarity,
                mode="file"
            )
        except Exception as e:
            display.error(f"Error finding similar documents: {e}")
            return 2
        finally:
            try:
                db.client.close()
            except Exception:
                pass

        # Format output
        if args.format == "json" or args.display == "mcp":
            # JSON output
            output = []
            for path_uri, similarity in results:
                # Convert file:// URI to path if needed
                if path_uri.startswith("file://"):
                    from urllib.parse import unquote, urlparse
                    parsed = urlparse(path_uri)
                    display_path = Path(unquote(parsed.path or ""))
                else:
                    display_path = Path(path_uri)

                output.append({
                    "path": str(display_path),
                    "similarity": round(similarity, 3)
                })
            display.json_output(output)
        else:
            # Table format
            if not results:
                display.info(f"No similar documents found for: {query_path}")
                return 0

            # Prepare table data
            table_data = []
            for path_uri, similarity in results:
                # Convert file:// URI to path if needed
                if path_uri.startswith("file://"):
                    from urllib.parse import unquote, urlparse
                    parsed = urlparse(path_uri)
                    display_path = Path(unquote(parsed.path or ""))
                else:
                    display_path = Path(path_uri)

                # Format similarity as percentage
                sim_pct = similarity * 100
                table_data.append({
                    "Similarity": f"{sim_pct:5.1f}%",
                    "Path": str(display_path)
                })

            display.table(table_data, title=f"Similar to: {query_path}")

        return 0
    rel.set_defaults(func=_related_cmd)

    # DB command: simple query passthrough and stats
    dbp = sub.add_parser("db", help="Database helpers: query and stats")
    dbsub = dbp.add_subparsers(dest="db_cmd")
    dbq = dbsub.add_parser("query", help="Run a JSON query against the selected DB space")
    # Choose logical DB space: --space (embeddings) or --time (snapshots)
    scope = dbq.add_mutually_exclusive_group(required=True)
    scope.add_argument("--space", action="store_true", help="Query the space DB (embeddings)")
    scope.add_argument("--time", action="store_true", help="Query the time DB (snapshots)")
    dbq.add_argument('--filter', default='{}', help='JSON filter, e.g. {"path": {"$regex": "2025-WKS"}}')
    dbq.add_argument('--projection', default=None, help='JSON projection, e.g. {"path":1,"timestamp":1}')
    dbq.add_argument("--limit", type=int, default=20)
    dbq.add_argument("--sort", default=None, help="Sort spec 'field:asc|desc' (optional)")
    def _db_query(args: argparse.Namespace) -> int:
        cfg_local = load_config()
        space_tag, time_tag = resolve_db_compatibility(cfg_local)
        pkg_version = get_package_version()
        try:
            client, mongo_cfg = _mongo_client_params(cfg=cfg_local)
        except Exception as e:
            print(f"DB connection failed: {e}")
            return 2
        try:
            import json as _json
            filt = _json.loads(args.filter or "{}")
            proj = _json.loads(args.projection) if args.projection else None
        except Exception as e:
            print(f"Invalid JSON: {e}")
            return 2
        # Decide target collection by logical DB
        if args.space:
            coll_name = mongo_cfg['space_collection']
            scope_label = 'space'
            try:
                ensure_db_compat(
                    client,
                    mongo_cfg['space_database'],
                    scope_label,
                    space_tag,
                    product_version=pkg_version,
                )
            except IncompatibleDatabase as exc:
                print(exc)
                return 2
            coll = client[mongo_cfg['space_database']][coll_name]
        else:
            coll_name = mongo_cfg['time_collection']
            scope_label = 'time'
            try:
                ensure_db_compat(
                    client,
                    mongo_cfg['time_database'],
                    scope_label,
                    time_tag,
                    product_version=pkg_version,
                )
            except IncompatibleDatabase as exc:
                print(exc)
                return 2
            coll = client[mongo_cfg['time_database']][coll_name]
        try:
            cur = coll.find(filt, proj)
        except Exception as e:
            print(f"DB query failed: {e}")
            return 1
        if args.sort:
            try:
                fld, dirspec = args.sort.split(':',1)
                direction = 1 if dirspec.lower().startswith('asc') else -1
                cur = cur.sort(fld, direction)
            except Exception:
                pass
        if args.limit:
            cur = cur.limit(int(args.limit))
        try:
            record_db_activity(f"db.query.{scope_label}", args.filter or "{}")
        except Exception:
            pass
        docs = list(cur)
        payload = {
            "scope": scope_label,
            "collection": coll_name,
            "count": len(docs),
            "documents": docs,
        }
        _maybe_write_json(args, payload)
        if not _display_enabled(args.display):
            return 0
        if args.display == "markdown":
            rows = [
                _json_dumps(doc) if not isinstance(doc, str) else doc
                for doc in docs
            ]
            print(
                render_template(
                    DB_QUERY_MARKDOWN_TEMPLATE,
                    {"scope": scope_label, "collection": coll_name, "rows": rows},
                )
            )
            return 0
        use_rich = args.display in {"rich", "plain"}
        if use_rich:
            try:
                from rich import box
                from rich.console import Console
                from rich.table import Table

                table_box = box.SQUARE if args.display == "rich" else box.SIMPLE
                header_style = "bold" if args.display == "rich" else ""
                table = Table(
                    title=f"[{scope_label}] {coll_name} query",
                    header_style=header_style,
                    box=table_box,
                    expand=False,
                    pad_edge=False,
                )
                table.add_column("#", justify="right", no_wrap=True)
                table.add_column("Document", overflow="fold")
                for idx, doc in enumerate(docs, 1):
                    table.add_row(str(idx), str(doc))
                Console().print(table)
                return 0
            except Exception:
                use_rich = False
        if not use_rich:
            for idx, doc in enumerate(docs, 1):
                print(f"[{idx}] {doc}")
        return 0
    dbq.set_defaults(func=_db_query)

    def _db_info(args: argparse.Namespace) -> int:
        # Use a lightweight, fast Mongo client for stats (no model/docling startup)
        cfg_local = load_config()
        ts_format = timestamp_format(cfg_local)
        display_mode = args.display

        reference_input = getattr(args, 'reference', None)
        reference_uri: Optional[str] = None
        reference_path: Optional[Path] = None
        reference_info: Optional[Dict[str, Any]] = None
        db_idx = None
        if reference_input:
            from urllib.parse import unquote, urlparse

            if reference_input.startswith("file://"):
                parsed = urlparse(reference_input)
                reference_uri = reference_input
                reference_path = Path(unquote(parsed.path or "")).expanduser()
            else:
                reference_path = Path(reference_input).expanduser()
                reference_uri = _as_file_uri_local(reference_path)
            if not reference_path.exists():
                print(f"Reference file not found: {reference_path}")
                return 2
            try:
                db_idx, _ = _load_similarity_required()
                extractor = _build_extractor(cfg_local)
                checksum_val = None
                size_val = None
                try:
                    checksum_val = _file_checksum(reference_path)
                except Exception:
                    checksum_val = None
                try:
                    size_val = reference_path.stat().st_size
                except Exception:
                    size_val = None
                extraction = extractor.extract(reference_path, persist=True)
                db_idx.add_file(
                    reference_path,
                    extraction=extraction,
                    file_checksum=checksum_val,
                    file_bytes=size_val,
                )
            except SystemExit:
                raise
            except Exception as exc:
                print(f"Failed to index reference: {exc}")
                return 2
            finally:
                if db_idx is not None:
                    try:
                        db_idx.client.close()
                    except Exception:
                        pass
            reference_info = {"uri": reference_uri, "path": reference_path}
        else:
            reference_info = None

        try:
            client, mongo_cfg = _mongo_client_params(server_timeout=300, connect_timeout=300, cfg=cfg_local)
        except Exception as e:
            try:
                mongoctl.ensure_mongo_running(_default_mongo_uri(), record_start=True)
                client, mongo_cfg = _mongo_client_params(server_timeout=300, connect_timeout=300, cfg=cfg_local)
            except Exception:
                print(f"DB connection failed: {e}")
                return 2
        try:
            client.admin.command('ping')
        except Exception as e:
            print(f"DB unreachable: {e}")
            return 1
        space_tag, time_tag = resolve_db_compatibility(cfg_local)
        pkg_version = get_package_version()
        info_scope = "time" if getattr(args, "time", False) else "space"
        target_db = mongo_cfg['time_database'] if info_scope == "time" else mongo_cfg['space_database']
        compat_tag = time_tag if info_scope == "time" else space_tag
        try:
            ensure_db_compat(
                client,
                target_db,
                info_scope,
                compat_tag,
                product_version=pkg_version,
            )
        except IncompatibleDatabase as exc:
            print(exc)
            return 2
        coll_space = mongo_cfg['space_collection']
        coll_time = mongo_cfg['time_collection']
        if reference_info and getattr(args, 'time', False):
            print("Reference comparisons are only available for the space database.")
            return 2
        # Helper to format timestamps consistently
        from datetime import datetime as _dt
        def _fmt_ts(ts):
            if not ts:
                return ""
            try:
                if isinstance(ts, str):
                    s = ts.replace('Z','+00:00') if ts.endswith('Z') else ts
                    dt = _dt.fromisoformat(s)
                else:
                    dt = _dt.fromtimestamp(float(ts))
                return dt.strftime(ts_format)
            except Exception:
                return str(ts)

        def _fmt_bytes(value: Optional[Any]) -> str:
            try:
                if value is None:
                    return "-"
                bytes_val = float(value)
            except Exception:
                return "-"
            units = ['B', 'kB', 'MB', 'GB', 'TB']
            i = 0
            while bytes_val >= 1024.0 and i < len(units) - 1:
                bytes_val /= 1024.0
                i += 1
            return f"{bytes_val:7.2f} {units[i]:>2}"

        def _fmt_uri(doc: Dict[str, Any]) -> str:
            uri = doc.get('path')
            if uri:
                return str(uri)
            local = doc.get('path_local')
            if not local:
                return ""
            try:
                return Path(local).expanduser().resolve().as_uri()
            except Exception:
                return str(local)

        def _fmt_checksum(value: Optional[Any]) -> str:
            if value is None or value == "":
                return "-"
            text = str(value)
            if len(text) <= 10:
                return text
            return f"{text[:10]} ‚Ä¶"

        def _parse_ts_value(ts: Any) -> Optional[_dt]:
            if not ts:
                return None
            try:
                if isinstance(ts, str):
                    s = ts.replace("Z", "+00:00") if ts.endswith("Z") else ts
                    return _dt.fromisoformat(s)
                if isinstance(ts, (int, float)):
                    return _dt.fromtimestamp(float(ts))
            except Exception:
                return None
            return None

        def _fmt_tdelta(delta: Optional[Any]) -> str:
            if delta is None:
                return "-"
            if not isinstance(delta, (int, float)):
                total_seconds = int(delta.total_seconds())
            else:
                total_seconds = int(delta)
            sign = "+" if total_seconds >= 0 else "-"
            total_seconds = abs(total_seconds)
            days, rem = divmod(total_seconds, 86400)
            hours, rem = divmod(rem, 3600)
            minutes, seconds = divmod(rem, 60)
            if days:
                return f"{sign}{days}d {hours:02d}:{minutes:02d}:{seconds:02d}"
            return f"{sign}{hours:02d}:{minutes:02d}:{seconds:02d}"

        def _fmt_size_delta(delta: Optional[Any]) -> str:
            if delta is None:
                return "-"
            try:
                value = float(delta)
            except Exception:
                return "-"
            sign = "+" if value >= 0 else "-"
            value = abs(value)
            units = ['B', 'kB', 'MB', 'GB', 'TB']
            i = 0
            while value >= 1024.0 and i < len(units) - 1:
                value /= 1024.0
                i += 1
            return f"{sign}{value:7.2f} {units[i]:>2}"

        def _fmt_angle_delta(delta: Optional[float]) -> str:
            if delta is None:
                return "-"
            return f"{delta:6.2f}¬∞"

        def _angle_between(vec_a: Optional[List[Any]], vec_b: Optional[List[Any]]) -> Optional[float]:
            if not vec_a or not vec_b:
                return None
            try:
                pairs = list(zip(vec_a, vec_b))
                if not pairs:
                    return None
                dot = sum(float(a) * float(b) for a, b in pairs)
                norm_a = math.sqrt(sum(float(a) * float(a) for a in vec_a))
                norm_b = math.sqrt(sum(float(b) * float(b) for b in vec_b))
                denom = norm_a * norm_b
                if denom <= 0:
                    return None
                cosv = max(-1.0, min(1.0, dot / denom))
                return math.degrees(math.acos(cosv))
            except Exception:
                return None

        def _format_cell(text: str, width: int, align: str) -> str:
            if align == "right":
                return text.rjust(width)
            if align == "center":
                return text.center(width)
            return text.ljust(width)

        def _render_plain_boxed_table_simple(
            title: str,
            header: List[str],
            rows: List[List[str]],
            align: List[str],
            limits: Optional[Dict[int, int]] = None,
        ) -> None:
            if not rows:
                return
            limits = limits or {}
            processed: List[List[str]] = []
            for row in rows:
                processed_row: List[str] = []
                for idx, cell in enumerate(row):
                    text = str(cell)
                    limit = limits.get(idx)
                    if limit and len(text) > limit:
                        if limit == 1:
                            text = text[:1]
                        else:
                            text = text[: limit - 1] + "‚Ä¶"
                    processed_row.append(text)
                processed.append(processed_row)
            widths: List[int] = []
            for idx, head in enumerate(header):
                col_width = len(str(head))
                for row in processed:
                    col_width = max(col_width, len(row[idx]))
                limit = limits.get(idx)
                if limit:
                    col_width = min(col_width, limit)
                widths.append(col_width)
            total_width = sum(widths) + 3 * (len(widths) - 1)
            border = "+" + "-" * (total_width + 2) + "+"
            print()
            print(border)
            print("| " + title.center(total_width) + " |")
            print(border)
            header_line = " | ".join(
                _format_cell(str(head), widths[idx], "center" if align[idx] == "right" else align[idx])
                for idx, head in enumerate(header)
            )
            print("| " + header_line + " |")
            separator = "-+-".join("-" * width for width in widths)
            print("| " + separator + " |")
            for row in processed:
                line = " | ".join(
                    _format_cell(cell, widths[idx], align[idx]) for idx, cell in enumerate(row)
                )
                print("| " + line + " |")
            print(border)

        if args.time:
            coll = client[mongo_cfg['time_database']][coll_time]
            total = coll.count_documents({})
            if display_mode != 'json':
                print({"database": mongo_cfg['time_database'], "collection": coll_time, "total_docs": total})
            try:
                record_db_activity("db.info.time", f"total={total}")
            except Exception:
                pass
            n = int(getattr(args, 'latest', 0) or 0)
            if n > 0:
                cur = coll.find(
                    {},
                    {"path": 1, "t_new": 1, "checksum_new": 1, "size_bytes_new": 1, "bytes_delta": 1},
                ).sort("t_new_epoch", -1).limit(n)
                docs = list(cur)
                if display_mode == 'json':
                    latest_payload = []
                    for doc in docs:
                        latest_payload.append({
                            "timestamp": _fmt_ts(doc.get('t_new', '')),
                            "uri": _fmt_uri(doc),
                            "checksum": _fmt_checksum(doc.get('checksum_new') or doc.get('content_hash')),
                            "size_bytes": doc.get('size_bytes_new'),
                            "bytes_delta": doc.get('bytes_delta'),
                        })
                    payload = {
                        "database": mongo_cfg['time_database'],
                        "collection": coll_time,
                        "total_docs": total,
                        "latest": latest_payload,
                    }
                    print(json.dumps(payload, indent=2))
                    return 0
                use_rich = (display_mode == 'rich') or (display_mode == 'auto')
                try:
                    from rich.console import Console
                    from rich.table import Table
                    if use_rich:
                        t = Table(title=f"[time] latest {n} snapshots")
                        t.add_column("#", justify="right", no_wrap=True, overflow="ignore")
                        t.add_column("t_new", no_wrap=True, overflow="ignore", min_width=19)
                        t.add_column("path", overflow="fold")
                        t.add_column("checksum", no_wrap=True, overflow="ignore", min_width=14)
                        t.add_column("bytes", justify="right", no_wrap=True, overflow="ignore", min_width=10)
                        t.add_column("Œî bytes", justify="right", no_wrap=True, overflow="ignore", min_width=10)
                        for i, doc in enumerate(docs, 1):
                            checksum = _fmt_checksum(doc.get('checksum_new') or doc.get('content_hash'))
                            size_disp = _fmt_bytes(doc.get('size_bytes_new'))
                            delta = doc.get('bytes_delta')
                            delta_disp = f"{delta:+}" if isinstance(delta, (int, float)) else "-"
                            t.add_row(
                                str(i),
                                _fmt_ts(doc.get('t_new', '')),
                                _fmt_uri(doc),
                                checksum,
                                size_disp,
                                delta_disp,
                            )
                        Console().print(t)
                    else:
                        for i, doc in enumerate(docs, 1):
                            checksum = _fmt_checksum(doc.get('checksum_new') or doc.get('content_hash'))
                            size_disp = _fmt_bytes(doc.get('size_bytes_new'))
                            delta = doc.get('bytes_delta')
                            delta_disp = f"{delta:+}" if isinstance(delta, (int, float)) else "-"
                            print(
                                f"[{i}] {_fmt_ts(doc.get('t_new',''))} checksum={checksum} size={size_disp} delta={delta_disp} {_fmt_uri(doc)}"
                            )
                except Exception:
                    for i, doc in enumerate(docs, 1):
                        checksum = _fmt_checksum(doc.get('checksum_new') or doc.get('content_hash'))
                        size_disp = _fmt_bytes(doc.get('size_bytes_new'))
                        delta = doc.get('bytes_delta')
                        delta_disp = f"{delta:+}" if isinstance(delta, (int, float)) else "-"
                        print(
                            f"[{i}] {_fmt_ts(doc.get('t_new',''))} checksum={checksum} size={size_disp} delta={delta_disp} {_fmt_uri(doc)}"
                        )
            elif display_mode == 'json':
                payload = {
                    "database": mongo_cfg['time_database'],
                    "collection": coll_time,
                    "total_docs": total,
                    "latest": [],
                }
                print(json.dumps(payload, indent=2))
                return 0
            return 0
        else:
            coll = client[mongo_cfg['space_database']][coll_space]
            total = coll.count_documents({})
            if display_mode != 'json':
                print(f"tracked files: {total}")
            try:
                record_db_activity("db.info.space", f"total={total}")
            except Exception:
                pass
            if reference_info:
                uri = reference_info.get("uri")
                ref_path_obj = reference_info.get("path")
                ref_doc = None
                if uri:
                    ref_doc = coll.find_one({"path": uri})
                if ref_doc is None and ref_path_obj is not None:
                    ref_doc = coll.find_one({"path_local": str(ref_path_obj.resolve())})
                if not ref_doc:
                    print("Reference document not found in the database.")
                    return 2
                ref_embedding = ref_doc.get("embedding")
                if not ref_embedding:
                    print("Reference document is missing embedding data; re-index and retry.")
                    return 2
                ref_ts = _parse_ts_value(ref_doc.get("timestamp"))
                ref_bytes = ref_doc.get("bytes")
                ref_checksum = ref_doc.get("checksum")

                entries: List[Dict[str, Any]] = []
                cursor = coll.find({}, {"path": 1, "path_local": 1, "timestamp": 1, "checksum": 1, "bytes": 1, "embedding": 1})
                for doc in cursor:
                    emb = doc.get("embedding")
                    angle_delta = _angle_between(ref_embedding, emb)
                    if angle_delta is None:
                        continue
                    other_ts = _parse_ts_value(doc.get("timestamp"))
                    t_delta = None
                    if other_ts is not None and ref_ts is not None:
                        t_delta = (other_ts - ref_ts).total_seconds()
                    size_delta = None
                    bytes_val = doc.get("bytes")
                    if bytes_val is not None and ref_bytes is not None:
                        try:
                            size_delta = int(bytes_val) - int(ref_bytes)
                        except Exception:
                            size_delta = None
                    checksum_same = False
                    if ref_checksum is not None:
                        checksum_same = doc.get("checksum") == ref_checksum
                    entries.append({
                        "doc": doc,
                        "angle_delta": angle_delta,
                        "time_delta": t_delta,
                        "size_delta": size_delta,
                        "checksum_same": checksum_same,
                    })

                entries.sort(key=lambda e: (abs(e['angle_delta']) if e['angle_delta'] is not None else float('inf'), e['doc'].get('path', '')))
                limit_n = int(getattr(args, 'latest', 0) or 10)
                if limit_n > 0:
                    entries = entries[:limit_n]

                try:
                    record_db_activity("db.info.reference", title_label)
                except Exception:
                    pass

                use_rich = False
                try:
                    if display_mode == 'rich':
                        use_rich = True
                    elif display_mode in (None, 'auto'):
                        use_rich = sys.stdout.isatty()
                except Exception:
                    use_rich = False

                title_label = uri or (str(ref_path_obj.resolve()) if ref_path_obj is not None else "reference")
                if display_mode == 'json':
                    json_entries: List[Dict[str, Any]] = []
                    for entry in entries:
                        doc = entry['doc']
                        json_entries.append({
                            "uri": _fmt_uri(doc),
                            "timestamp": _fmt_ts(doc.get('timestamp', '')),
                            "angle_delta": entry['angle_delta'],
                            "checksum_same": bool(entry['checksum_same']),
                            "size_delta": entry['size_delta'],
                            "time_delta_secs": entry['time_delta'],
                        })
                    payload = {
                        "reference": title_label,
                        "tracked_files": total,
                        "entries": json_entries,
                    }
                    print(json.dumps(payload, indent=2))
                    return 0
                if use_rich:
                    try:
                        from rich import box
                        from rich.console import Console
                        from rich.panel import Panel
                        from rich.table import Table
                    except Exception:
                        use_rich = False
                if use_rich:
                    console = Console()
                    table = Table(show_header=True, header_style="bold", box=box.SQUARE, expand=False, pad_edge=False)
                    table.add_column("#", justify="right", width=3, no_wrap=True)
                    table.add_column("Œî time", justify="right", no_wrap=True)
                    table.add_column("Checksum", justify="left", no_wrap=True)
                    table.add_column("Œî size", justify="right", no_wrap=True)
                    table.add_column("Œî angle", justify="right", no_wrap=True)
                    table.add_column("uri", overflow="fold")
                    for idx, entry in enumerate(entries, 1):
                        checksum_label = "same" if entry['checksum_same'] else "diff"
                        checksum_display = f"[green]{checksum_label}[/]" if entry['checksum_same'] else f"[red]{checksum_label}[/]"
                        table.add_row(
                            str(idx),
                            _fmt_tdelta(entry['time_delta']) if entry['time_delta'] is not None else "-",
                            checksum_display,
                            _fmt_size_delta(entry['size_delta']),
                            _fmt_angle_delta(entry['angle_delta']),
                            _fmt_uri(entry['doc'])
                        )
                    console.print(Panel.fit(table, title=f"Reference: {title_label}", border_style="dim"))
                    return 0

                header = ["#", "Œî time", "Checksum", "Œî size", "Œî angle", "uri"]
                align_plain = ["right", "right", "left", "right", "right", "left"]
                limits_plain = {5: 80}
                rows_plain: List[List[str]] = []
                for idx, entry in enumerate(entries, 1):
                    rows_plain.append([
                        str(idx),
                        _fmt_tdelta(entry['time_delta']) if entry['time_delta'] is not None else "-",
                        "same" if entry['checksum_same'] else "diff",
                        _fmt_size_delta(entry['size_delta']),
                        _fmt_angle_delta(entry['angle_delta']),
                        _fmt_uri(entry['doc']),
                    ])
                _render_plain_boxed_table_simple(f"Reference: {title_label}", header, rows_plain, align_plain, limits_plain)
                return 0

            n = int(getattr(args, 'latest', 0) or 0)
            if n > 0:
                cur = coll.find({}, {"path": 1, "path_local": 1, "timestamp": 1, "checksum": 1, "bytes": 1, "angle": 1}).sort("timestamp", -1).limit(n)
                docs = list(cur)
                latest_docs_payload = [
                    {
                        "timestamp": _fmt_ts(doc.get('timestamp', '')),
                        "checksum": _fmt_checksum(doc.get('checksum') or doc.get('content_hash')),
                        "bytes": doc.get('bytes'),
                        "angle": doc.get('angle'),
                        "uri": _fmt_uri(doc),
                    }
                    for doc in docs
                ]
                if display_mode == 'json':
                    payload = {
                        "tracked_files": total,
                        "latest": latest_docs_payload,
                    }
                    print(json.dumps(payload, indent=2))
                    return 0
                use_rich = (display_mode == 'rich') or (display_mode == 'auto')
                try:
                    from rich.console import Console
                    from rich.table import Table
                    if use_rich:
                        t = Table(title=f"[space] latest {n} files")
                        t.add_column("#", justify="right", no_wrap=True, overflow="ignore")
                        t.add_column("timestamp", no_wrap=True, overflow="ignore", min_width=19)
                        t.add_column("checksum", no_wrap=True, overflow="ignore", min_width=14)
                        t.add_column("size", justify="right", no_wrap=True, overflow="ignore", min_width=10)
                        t.add_column("angle", justify="right", no_wrap=True, overflow="ignore", min_width=8)
                        t.add_column("uri", overflow="fold")
                        for i, doc in enumerate(docs, 1):
                            checksum = _fmt_checksum(doc.get('checksum') or doc.get('content_hash'))
                            size_disp = _fmt_bytes(doc.get('bytes'))
                            angle = doc.get('angle')
                            angle_disp = f"{float(angle):6.2f}¬∞" if isinstance(angle, (int, float)) else "-"
                            t.add_row(
                                str(i),
                                _fmt_ts(doc.get('timestamp','')),
                                checksum,
                                size_disp,
                                angle_disp,
                                _fmt_uri(doc),
                            )
                        Console().print(t)
                    else:
                        for i, doc in enumerate(docs, 1):
                            checksum = _fmt_checksum(doc.get('checksum') or doc.get('content_hash'))
                            size_disp = _fmt_bytes(doc.get('bytes'))
                            angle = doc.get('angle')
                            angle_disp = f"{float(angle):6.2f}¬∞" if isinstance(angle, (int, float)) else "-"
                            print(
                                f"[{i}] {_fmt_ts(doc.get('timestamp',''))} checksum={checksum} size={size_disp} angle={angle_disp} {_fmt_uri(doc)}"
                            )
                except Exception:
                    for i, doc in enumerate(docs, 1):
                        checksum = _fmt_checksum(doc.get('checksum') or doc.get('content_hash'))
                        size_disp = _fmt_bytes(doc.get('bytes'))
                        angle = doc.get('angle')
                        angle_disp = f"{float(angle):6.2f}¬∞" if isinstance(angle, (int, float)) else "-"
                        print(
                            f"[{i}] {_fmt_ts(doc.get('timestamp',''))} checksum={checksum} size={size_disp} angle={angle_disp} {_fmt_uri(doc)}"
                        )
            elif display_mode == 'json':
                payload = {
                    "tracked_files": total,
                    "latest": [],
                }
                print(json.dumps(payload, indent=2))
                return 0
            return 0

    dbinfo = dbsub.add_parser("info", help="Print tracked file count and latest files")
    scope_info = dbinfo.add_mutually_exclusive_group()
    scope_info.add_argument("--space", action="store_true", help="Stats for the space DB")
    scope_info.add_argument("--time", action="store_true", help="Stats for the time DB")
    dbinfo.add_argument("-n", "--latest", type=int, default=10, help="Show the most recent N records (default 10)")
    dbinfo.add_argument("--reference", help="File path or file:// URI to compare against")
    dbinfo.set_defaults(func=_db_info)
    # DB reset: drop Mongo database and remove local mongod files (if any)
    dbr = dbsub.add_parser("reset", help="Drop WKS Mongo databases and local DB files (space/time)")
    def _db_reset(args: argparse.Namespace) -> int:
        cfg = load_config()
        mongo_cfg = mongo_settings(cfg)
        uri = mongo_cfg['uri']
        space_db = mongo_cfg['space_database']
        time_db = mongo_cfg['time_database']
        _stop_managed_mongo()
        # Try to drop DB via pymongo
        try:
            client = pymongo.MongoClient(uri, serverSelectionTimeoutMS=3000)
            try:
                client.admin.command('ping')
                dropped = set()
                client.drop_database(space_db)
                dropped.add(space_db)
                if time_db not in dropped:
                    client.drop_database(time_db)
                    dropped.add(time_db)
                print(f"Dropped database(s): {', '.join(sorted(dropped))}")
                try:
                    record_db_activity("db.reset", ",".join(sorted(dropped)))
                except Exception:
                    pass
            except Exception as e:
                print(f"DB drop skipped (unreachable or error): {e}")
        except Exception as e:
            print(f"Mongo client unavailable: {e}")
        # Stop local mongod on 27027 (best-effort)
        try:
            import shutil as _sh, subprocess as _sp
            if _sh.which('pkill'):
                _sp.run(['pkill','-f','mongod.*27027'], check=False)
        except Exception:
            pass
        # Remove local DB files
        try:
            import shutil as _sh
            dbroot = Path.home()/WKS_HOME_EXT/'mongodb'
            if dbroot.exists():
                _sh.rmtree(dbroot, ignore_errors=True)
                print(f"Removed local DB files: {dbroot}")
        except Exception:
            pass
        # Ensure Mongo comes back up so the service can reconnect immediately
        try:
            mongoctl.ensure_mongo_running(uri, record_start=True)
        except SystemExit:
            raise
        except Exception:
            pass
        return 0
    dbr.set_defaults(func=_db_reset)

    # MCP server command
    mcp = sub.add_parser("mcp", help="MCP (Model Context Protocol) server for AI integration")
    mcpsub = mcp.add_subparsers(dest="mcp_cmd")

    mcprun = mcpsub.add_parser("run", help="Start MCP server (stdio transport)")
    def _mcp_run(args: argparse.Namespace) -> int:
        """Start MCP server for AI integration."""
        from .mcp_server import main as mcp_main
        mcp_main()
        return 0
    mcprun.set_defaults(func=_mcp_run)

    # Simplified CLI ‚Äî top-level groups: config/service/monitor/extract/index/related/db/mcp

    args = parser.parse_args(argv)

    # Get display instance based on mode
    args.display_obj = get_display(args.display)

    if not hasattr(args, "func"):
        # If a group was selected without subcommand, show that group's help
        try:
            cmd = getattr(args, 'cmd', None)
            if cmd == 'service':
                svc.print_help()
                return 2
            if cmd == 'monitor':
                mon.print_help()
                return 2
            if cmd == 'db':
                dbp.print_help()
                return 2
            if cmd == 'mcp':
                mcp.print_help()
                return 2
        except Exception:
            pass
        parser.print_help()
        return 2

    res = args.func(args)
    return 0 if res is None else res


if __name__ == "__main__":
    raise SystemExit(main())
</file>

<file path="wks/links.py">
"""
URL organization utilities for WKS.

Parses Markdown files with bare URLs and rewrites them as categorized
Markdown links with readable titles and short blurbs.
"""

from __future__ import annotations

import re
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Tuple
from urllib.parse import urlparse, parse_qs, urlunparse


@dataclass
class LinkInfo:
    url: str
    host: str
    path: str
    title: str
    category: str
    blurb: str


_CATEGORY_RULES: List[Tuple[re.Pattern, str, str]] = [
    (re.compile(r"(^|\.)sciencedirect\.com$", re.I), "Research Papers", "ScienceDirect article"),
    (re.compile(r"(^|\.)inldigitallibrary\.inl\.gov$", re.I), "Research Papers", "INL digital library document"),
    (re.compile(r"(^|\.)inis\.iaea\.org$", re.I), "Research Papers", "IAEA INIS record"),
    (re.compile(r"(^|\.)readthedocs\.io$", re.I), "Docs & Guides", "Documentation"),
    (re.compile(r"(^|\.)docs\.github\.com$", re.I), "Docs & Guides", "GitHub Docs"),
    (re.compile(r"(^|\.)huggingface\.co$", re.I), "AI/ML Tools", "Model/space on Hugging Face"),
    (re.compile(r"(^|\.)ollama\.com$", re.I), "AI/ML Tools", "Ollama model catalog"),
    (re.compile(r"(^|\.)superuser\.com$", re.I), "Q&A / How‚ÄëTo", "Superuser Q&A"),
    (re.compile(r"(^|\.)microsoftonline\.com$", re.I), "Accounts / Internal", "Microsoft login/OAuth"),
    (re.compile(r"(^|\.)powerbigov\.us$", re.I), "Accounts / Internal", "Power BI Gov"),
    (re.compile(r"(^|\.)localhost$", re.I), "Local / Dev", "Localhost"),
]


def _short_text_from_path(path: str) -> str:
    # Prefer last non-empty path component
    segs = [s for s in path.split('/') if s]
    if not segs:
        return "/"
    last = segs[-1]
    # Strip common GUID-ish or hashy tails
    last = re.sub(r"[?#].*$", "", last)
    last = re.sub(r"%[0-9A-Fa-f]{2}", " ", last)
    # Replace separators with spaces
    disp = re.sub(r"[-_]+", " ", last)
    # Trim very long segments
    return (disp[:60] + "‚Ä¶") if len(disp) > 60 else disp


def _strip_tracking(url: str, host: str) -> str:
    # Keep full URL, but remove obviously useless trackers from common sites
    p = urlparse(url)
    if not p.query:
        return url
    q = parse_qs(p.query, keep_blank_values=True)
    drop_keys = {"utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content"}
    # For sciencedirect and similar, preserve query (often required), so skip
    if re.search(r"sciencedirect|microsoftonline|powerbi", host, re.I):
        return url
    new_query = "&".join(
        f"{k}={v[0]}" if v else k for k, v in q.items() if k not in drop_keys
    )
    return urlunparse((p.scheme, p.netloc, p.path, p.params, new_query, p.fragment))


def categorize(url: str) -> LinkInfo:
    p = urlparse(url)
    host = p.hostname or ""
    cleaned = _strip_tracking(url, host)
    # Category/blurb by host rules
    category = "Other"
    blurb = "Link"
    for pat, cat, default_blurb in _CATEGORY_RULES:
        if pat.search(host):
            category = cat
            blurb = default_blurb
            break
    # Derive a readable title
    base = host.replace("www.", "")
    tail = _short_text_from_path(p.path)
    title = base if tail in {"/", ""} else f"{base}: {tail}"
    # Extra heuristics
    if host.endswith("sciencedirect.com") and "/science/article/" in p.path:
        title = f"ScienceDirect article ({p.path.split('/')[-1]})"
    if host.endswith("inis.iaea.org") and "RN:" in cleaned:
        m = re.search(r"RN:(\d+)", cleaned)
        if m:
            title = f"IAEA INIS record RN:{m.group(1)}"
    if cleaned.lower().endswith(".pdf"):
        blurb = "PDF document"
    return LinkInfo(url=cleaned, host=host, path=p.path, title=title, category=category, blurb=blurb)


def extract_urls_from_markdown(text: str) -> List[str]:
    urls: List[str] = []
    # Match http/https URLs in lines; ignore those already formatted as [text](url)
    for line in text.splitlines():
        # Bullets with bare URL
        m = re.findall(r"https?://[^\s)]+", line)
        for u in m:
            urls.append(u.strip())
    return urls


def tidy_markdown_links(md_path: Path) -> str:
    src = md_path.read_text(encoding="utf-8")
    urls = extract_urls_from_markdown(src)
    if not urls:
        return src
    # Dedup while preserving input order
    seen: Dict[str, bool] = {}
    uniq: List[str] = []
    for u in urls:
        if u not in seen:
            uniq.append(u)
            seen[u] = True
    infos = [categorize(u) for u in uniq]
    # Group by category
    by_cat: Dict[str, List[LinkInfo]] = {}
    for info in infos:
        by_cat.setdefault(info.category, []).append(info)
    order = [
        "Research Papers",
        "Docs & Guides",
        "AI/ML Tools",
        "Q&A / How‚ÄëTo",
        "Accounts / Internal",
        "Local / Dev",
        "Other",
    ]
    # Build output
    lines: List[str] = []
    # Preserve the original top header if present
    header_done = False
    for line in src.splitlines():
        if line.strip().startswith("# ") and not header_done:
            lines.append(line)
            lines.append("")
            header_done = True
            break
    if not header_done:
        lines.append("# Curated Links")
        lines.append("")
    lines.append("Organized by category with short descriptions. Original sources consolidated.")
    lines.append("")
    for cat in order:
        items = by_cat.get(cat) or []
        if not items:
            continue
        lines.append(f"## {cat}")
        for info in items:
            # Markdown link with title; include blurb as dash detail
            lines.append(f"- [{info.title}]({info.url}) ‚Äî {info.blurb}")
        lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def write_tidy_markdown(md_path: Path) -> Path:
    out = tidy_markdown_links(md_path)
    md_path.write_text(out, encoding="utf-8")
    return md_path
</file>

<file path="wks/mongo_retry.py">
"""Retry logic for MongoDB operations."""

import logging
import time
from functools import wraps
from typing import Any, Callable, TypeVar, Optional

import pymongo.errors

logger = logging.getLogger(__name__)

T = TypeVar('T')


def mongo_retry(
    max_attempts: int = 3,
    delay_secs: float = 0.5,
    backoff_multiplier: float = 2.0,
    exceptions: tuple = (pymongo.errors.ConnectionFailure, pymongo.errors.ServerSelectionTimeoutError)
):
    """
    Decorator to retry MongoDB operations on transient failures.

    Args:
        max_attempts: Maximum number of retry attempts
        delay_secs: Initial delay between retries in seconds
        backoff_multiplier: Multiplier for exponential backoff
        exceptions: Tuple of exception types to retry on

    Example:
        @mongo_retry(max_attempts=3, delay_secs=0.5)
        def get_document(collection, doc_id):
            return collection.find_one({"_id": doc_id})
    """
    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @wraps(func)
        def wrapper(*args, **kwargs) -> T:
            attempt = 0
            current_delay = delay_secs

            while attempt < max_attempts:
                try:
                    return func(*args, **kwargs)
                except exceptions as exc:
                    attempt += 1
                    if attempt >= max_attempts:
                        logger.error(
                            f"MongoDB operation failed after {max_attempts} attempts: {func.__name__}",
                            exc_info=True,
                            extra={"operation": func.__name__, "attempts": attempt}
                        )
                        raise

                    logger.warning(
                        f"MongoDB operation failed (attempt {attempt}/{max_attempts}): {func.__name__}. "
                        f"Retrying in {current_delay}s...",
                        extra={"operation": func.__name__, "attempt": attempt, "delay": current_delay}
                    )

                    time.sleep(current_delay)
                    current_delay *= backoff_multiplier

            # Should never reach here, but for type safety
            raise RuntimeError(f"Unexpected exit from retry loop: {func.__name__}")

        return wrapper
    return decorator


class MongoRetryWrapper:
    """
    Wrapper class that adds retry logic to MongoDB collection methods.

    Example:
        collection = db["my_collection"]
        safe_collection = MongoRetryWrapper(collection)
        doc = safe_collection.find_one({"_id": "123"})  # Automatically retries on failure
    """

    def __init__(self, collection, max_attempts: int = 3, delay_secs: float = 0.5):
        """
        Args:
            collection: PyMongo collection object
            max_attempts: Maximum retry attempts for operations
            delay_secs: Initial delay between retries
        """
        self._collection = collection
        self._max_attempts = max_attempts
        self._delay_secs = delay_secs

    def __getattr__(self, name: str) -> Any:
        """
        Wrap collection methods with retry logic.

        Read operations (find_one, find, count_documents) get retry logic.
        Write operations (insert_one, update_one, etc.) pass through without retry
        to avoid duplicate writes on transient failures.
        """
        attr = getattr(self._collection, name)

        if not callable(attr):
            return attr

        # Only retry read operations
        read_ops = {
            "find_one", "find", "count_documents", "estimated_document_count",
            "distinct", "aggregate", "find_one_and_update", "find_one_and_replace",
            "find_one_and_delete"
        }

        if name in read_ops:
            return mongo_retry(max_attempts=self._max_attempts, delay_secs=self._delay_secs)(attr)

        return attr

    @property
    def name(self) -> str:
        """Pass through collection name."""
        return self._collection.name

    def __repr__(self) -> str:
        return f"MongoRetryWrapper({self._collection!r}, max_attempts={self._max_attempts})"
</file>

<file path="wks/priority.py">
"""Priority calculation for file importance scoring."""

import math
from pathlib import Path
from typing import Dict, Optional, Tuple


def find_managed_directory(
    path: Path,
    managed_dirs: Dict[str, int]
) -> Tuple[Optional[Path], int]:
    """Find the deepest matching managed directory for a path.

    Args:
        path: File path (should be resolved/absolute)
        managed_dirs: Dict mapping directory paths to base priorities

    Returns:
        Tuple of (matched_directory_path, base_priority)
        Returns (None, 100) if no match (default priority)
    """
    # Resolve path to absolute
    path = path.resolve()

    # Convert managed_dirs keys to resolved Paths
    resolved_managed = {
        Path(k).expanduser().resolve(): v
        for k, v in managed_dirs.items()
    }

    # Find all ancestors of the path
    ancestors = [path] + list(path.parents)

    # Find deepest (most specific) match
    best_match = None
    best_priority = 100  # Default

    for ancestor in ancestors:
        if ancestor in resolved_managed:
            # Deeper match wins (first match is deepest)
            if best_match is None or len(ancestor.parts) > len(best_match.parts):
                best_match = ancestor
                best_priority = resolved_managed[ancestor]

    return best_match, best_priority


def count_leading_underscores(name: str) -> int:
    """Count leading underscores in a name.

    Args:
        name: Path component name

    Returns:
        Number of leading underscores (0 if none)
    """
    count = 0
    for char in name:
        if char == "_":
            count += 1
        else:
            break
    return count


def calculate_underscore_penalty(name: str) -> float:
    """Calculate penalty multiplier for underscore prefixes.

    Rules:
    - Single "_" component: √∑64
    - Each leading "_": √∑2

    Args:
        name: Path component name

    Returns:
        Penalty multiplier (e.g., 0.5 for one underscore, 0.25 for two)
    """
    if name == "_":
        return 1.0 / 64.0

    underscore_count = count_leading_underscores(name)
    if underscore_count == 0:
        return 1.0

    # Each underscore divides by 2
    return 1.0 / (2 ** underscore_count)


def calculate_priority(
    path: Path,
    managed_dirs: Dict[str, int],
    priority_config: Dict
) -> int:
    """Calculate priority score for a file.

    Algorithm:
    1. Find managed directory (deepest match)
    2. Start with base priority from that directory
    3. For each path component after base:
       - Apply depth multiplier (default 0.9)
       - Apply underscore penalty (√∑2 per _, √∑64 for single _)
    4. Apply extension weight
    5. Round to integer (minimum 1)

    Args:
        path: File path (will be resolved)
        managed_dirs: Dict mapping directory paths to base priorities
        priority_config: Dict with keys:
            - depth_multiplier: float (default 0.9)
            - underscore_divisor: int (default 2)
            - single_underscore_divisor: int (default 64)
            - extension_weights: dict mapping extensions to weights

    Returns:
        Priority score (integer, minimum 1)

    Examples:
        >>> # /Users/ww5/Documents/reports/_old/draft.pdf
        >>> # Managed: ~/Documents (100), relative: reports/_old/draft.pdf
        >>> # reports: 100 * 0.9 = 90
        >>> # _old: 90 * 0.9 * 0.5 = 40.5
        >>> # Extension .pdf: 40.5 * 1.1 = 44.55
        >>> # Result: 45
    """
    # Resolve path
    path = path.resolve()

    # Find managed directory
    managed_dir, base_priority = find_managed_directory(path, managed_dirs)

    # Get config values
    depth_multiplier = priority_config.get("depth_multiplier", 0.9)
    extension_weights = priority_config.get("extension_weights", {})
    default_weight = extension_weights.get("default", 1.0)

    # Start with base priority
    score = float(base_priority)

    # Calculate relative path from managed directory
    if managed_dir:
        try:
            relative_parts = path.relative_to(managed_dir).parts
        except ValueError:
            # Path not under managed_dir (shouldn't happen)
            relative_parts = path.parts
    else:
        # No managed directory match, use full path
        relative_parts = path.parts

    # Process each directory component (exclude filename)
    for component in relative_parts[:-1]:
        # Apply depth penalty
        score *= depth_multiplier

        # Apply underscore penalty
        underscore_penalty = calculate_underscore_penalty(component)
        score *= underscore_penalty

    # Process filename stem ONLY if it has leading underscores
    filename_stem = path.stem
    if filename_stem and (filename_stem.startswith("_") or filename_stem == "_"):
        # Apply depth penalty for filename level
        score *= depth_multiplier

        # Apply underscore penalty to stem
        underscore_penalty = calculate_underscore_penalty(filename_stem)
        score *= underscore_penalty

    # Apply extension weight to final score
    extension = path.suffix.lower()
    weight = extension_weights.get(extension, default_weight)
    score *= weight

    # Round to integer (round half up), minimum 1
    # Use int(score + 0.5) for round-half-up behavior instead of Python's round-half-to-even
    return max(1, int(score + 0.5))


def priority_examples():
    """Return example priority calculations for testing/documentation."""
    managed_dirs = {
        "~/Desktop": 150,
        "~/deadlines": 120,
        "~": 100,
        "~/Documents": 100,
        "~/Pictures": 80,
        "~/Downloads": 50,
    }

    priority_config = {
        "depth_multiplier": 0.9,
        "underscore_divisor": 2,
        "single_underscore_divisor": 64,
        "extension_weights": {
            ".docx": 1.3,
            ".pptx": 1.3,
            ".pdf": 1.1,
            "default": 1.0
        }
    }

    home = Path.home()

    examples = [
        # Example 1: Deep nesting with underscores
        (
            home / "Documents/my/full/_path/__file.txt",
            "~/Documents/my/full/_path/__file.txt",
            8
        ),
        # Example 2: Single underscore directory
        (
            home / "Documents/my/_/path/file.txt",
            "~/Documents/my/_/path/file.txt",
            1
        ),
        # Example 3: DOCX with high weight
        (
            home / "Documents/reports/2025/annual_report.docx",
            "~/Documents/reports/2025/annual_report.docx",
            105
        ),
        # Example 4: Project with archive
        (
            home / "2025-Project/_old/draft.pdf",
            "~/2025-Project/_old/draft.pdf",
            45
        ),
        # Example 5: Pictures directory (lower base)
        (
            home / "Pictures/2025-Memes/funny_diagram.png",
            "~/Pictures/2025-Memes/funny_diagram.png",
            72
        ),
        # Example 6: Downloads
        (
            home / "Downloads/report.docx",
            "~/Downloads/report.docx",
            65
        ),
        # Example 7: Deadlines (high base)
        (
            home / "deadlines/2025_12_15-Proposal/draft_v3.pdf",
            "~/deadlines/2025_12_15-Proposal/draft_v3.pdf",
            119
        ),
        # Example 8: Downloads archive
        (
            home / "Downloads/_archive/old_file.txt",
            "~/Downloads/_archive/old_file.txt",
            23
        ),
    ]

    results = []
    for path, display_path, expected in examples:
        calculated = calculate_priority(path, managed_dirs, priority_config)
        results.append({
            "path": display_path,
            "expected": expected,
            "calculated": calculated,
            "match": calculated == expected
        })

    return results
</file>

<file path="wks/status.py">
from __future__ import annotations

import json
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

from .constants import WKS_HOME_EXT

_DB_ACTIVITY_DIR = Path.home() / WKS_HOME_EXT
_DB_ACTIVITY_SUMMARY = _DB_ACTIVITY_DIR / "db_activity.json"
_DB_ACTIVITY_HISTORY = _DB_ACTIVITY_DIR / "db_activity_history.json"
_MAX_HISTORY_SECONDS = 24 * 60 * 60  # keep last 24 hours
_MAX_HISTORY_ENTRIES = 2000


def _now() -> float:
    return time.time()


def record_db_activity(operation: str, detail: Optional[str] = None) -> None:
    """Persist a database activity event for service status metrics."""
    try:
        ts = _now()
        entry: Dict[str, Any] = {
            "timestamp": ts,
            "timestamp_iso": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(ts)),
            "operation": operation,
        }
        if detail:
            entry["detail"] = detail

        _DB_ACTIVITY_DIR.mkdir(parents=True, exist_ok=True)

        history: List[Dict[str, Any]] = []
        if _DB_ACTIVITY_HISTORY.exists():
            try:
                history = json.loads(_DB_ACTIVITY_HISTORY.read_text())
            except Exception:
                history = []

        # Drop entries older than max age or malformed ones
        cutoff = ts - _MAX_HISTORY_SECONDS
        filtered: List[Dict[str, Any]] = []
        for item in history:
            try:
                it_ts = float(item.get("timestamp", 0))
            except Exception:
                continue
            if it_ts >= cutoff:
                filtered.append(item)
        filtered.append(entry)
        if len(filtered) > _MAX_HISTORY_ENTRIES:
            filtered = filtered[-_MAX_HISTORY_ENTRIES:]

        _DB_ACTIVITY_HISTORY.write_text(json.dumps(filtered, indent=2))
        _DB_ACTIVITY_SUMMARY.write_text(json.dumps(entry, indent=2))
    except Exception:
        # Best-effort; swallow all errors.
        pass


def load_db_activity_summary() -> Dict[str, Any]:
    """Return the latest DB activity event (may be empty)."""
    try:
        if _DB_ACTIVITY_SUMMARY.exists():
            return json.loads(_DB_ACTIVITY_SUMMARY.read_text())
    except Exception:
        return {}
    return {}


def load_db_activity_history(max_age_secs: Optional[int] = None) -> List[Dict[str, Any]]:
    """Return recent DB activity history, optionally filtered by max_age_secs."""
    try:
        if not _DB_ACTIVITY_HISTORY.exists():
            return []
        history = json.loads(_DB_ACTIVITY_HISTORY.read_text())
        if not isinstance(history, list):
            return []
        if max_age_secs is None:
            return history
        cutoff = _now() - float(max_age_secs)
        filtered = []
        for item in history:
            try:
                ts = float(item.get("timestamp", 0))
            except Exception:
                continue
            if ts >= cutoff:
                filtered.append(item)
        return filtered
    except Exception:
        return []
</file>

<file path="wks/templating.py">
"""Thin wrapper around Jinja2 for rendering CLI outputs."""

from __future__ import annotations

from typing import Any, Dict

from jinja2 import Environment, BaseLoader, StrictUndefined

_ENV = Environment(loader=BaseLoader(), trim_blocks=True, lstrip_blocks=True, undefined=StrictUndefined)


def render_template(template: str, context: Dict[str, Any]) -> str:
    tmpl = _ENV.from_string(template)
    return tmpl.render(**context)
</file>

<file path="wks/uri_utils.py">
"""Path and URI conversion utilities for WKS.

The monitor database stores paths as URIs (file:///...) while configuration
uses regular paths (~/obsidian, /Users/ww5/obsidian). These utilities handle
conversion between the two forms.
"""

from pathlib import Path
from urllib.parse import unquote


def path_to_uri(path: Path) -> str:
    """Convert Path to file:// URI.

    Args:
        path: Path object

    Returns:
        URI string like 'file:///Users/ww5/file.txt'
    """
    return path.resolve().as_uri()


def uri_to_path(uri: str) -> Path:
    """Convert file:// URI to Path.

    Args:
        uri: URI string like 'file:///Users/ww5/file.txt'

    Returns:
        Path object
    """
    if uri.startswith('file://'):
        # Remove file:// prefix
        path_str = uri[7:]
        # URL decode any percent-encoded characters
        path_str = unquote(path_str)
        return Path(path_str)
    return Path(uri)
</file>

<file path="CONTRIBUTING.md">
# Repository Guidelines

## Project Structure & Module Organization
- Source: `wks/` (Python package)
  - `cli.py` (console entry `wks`), `daemon.py` (background agent), `obsidian.py` (vault ops), `monitor.py` (FS watch), `similarity.py` (embeddings/MongoDB), `links.py`, `activity.py`, `__main__.py`.
- Packaging: `setup.py` (installs `wks` console script).
- Docs: `README.md`, `SPEC.md`, `ROADMAP.md`, `TASKS.md`.
- Scripts: `scripts/` (utility placeholders).

## Build, Test, and Development Commands
- Create env: `python -m venv venv && source venv/bin/activate`
- Install editable: `pip install -e .`
- CLI help: `wks -h`
- Daemon: `wks service start | stop | status | restart`
- Analyze/similarity: `wks analyze index <paths...>`, `wks analyze query ...`, `wks analyze route --path <file>`

## Coding Style & Naming Conventions
- Python ‚â• 3.8; PEP 8; 4 spaces; type hints preferred.
- `snake_case` (functions/vars), `PascalCase` (classes), `UPPER_SNAKE` (constants).
- Keep modules focused; avoid cross‚Äëcutting side effects.
- If you use Black/Ruff locally, avoid unrelated mass reformatting.

## Testing Guidelines
- Use `pytest`; add tests under `tests/`, named `test_*.py`.
- Prefer pure functions; mock filesystem/IO; avoid touching `~/obsidian` or `~/.wks`.
- Aim for reasonable coverage on changed code; document a brief test plan in PRs.

## Commit & Pull Request Guidelines
- Conventional Commits (`feat:`, `fix:`, `chore:`). Keep scope small.
- PRs include: summary, motivation, linked issues/tasks, and CLI examples; screenshots optional for vault artifacts.
- Avoid destructive defaults; gate with flags and document safeguards.
- Update relevant docs (`SPEC.md`, `TASKS.md`) when changing behavior/workflows.

## Security & Configuration Tips
- User config: `~/.wks/config.json` (e.g., `obsidian.base_dir`, monitor paths, similarity settings). Never commit secrets.
- Default Mongo: `mongodb://localhost:27027/` (auto‚Äëstart if available). Support offline models where possible.
- Respect ignore settings; never write outside the configured vault base dir.
</file>

<file path="guides/CLAUDE.md">
# Wieselquist Knowledge System (WKS)

**Version:** 1.1
**Updated:** 2025-10-28
**Author:** William Wieselquist
**Purpose:** Complete specification and AI assistant instructions for the Wieselquist Knowledge System

## Table of Contents

1. [System Overview](#system-overview)
2. [Core Principles](#core-principles)
3. [Critical Rules](#critical-rules)
4. [File System Structure](#file-system-structure)
5. [Naming Conventions](#naming-conventions)
6. [Special Directory Structures](#special-directory-structures)
7. [Obsidian Vault Organization](#obsidian-vault-organization)
8. [Common Workflows](#common-workflows)
9. [AI Agent Guidelines](#ai-agent-guidelines)

---

## System Overview

The Wieselquist Knowledge System (WKS) is an AI-assisted file organization and knowledge management system that uses Obsidian as an intelligent layer for maintaining connections and organization across a structured file system.

## Core Principles

1. **Date-based naming** encodes temporal scope and relevance
2. **Hierarchical archiving** (`_old/YYYY/`) keeps workspaces clean
3. **Obsidian as intelligent layer** maintains connections as files move
4. **Desktop as focus workspace** - curated via symlinks
5. **AI agent maintains coherence** between filesystem and knowledge graph

## Critical Rules

### Obsidian Linking Policy

**NEVER link to obsidian from outside obsidian. Obsidian manages the links of our data.**

- External files (in `~/YYYY-projectname/`, `~/Documents/`, etc.) should NOT contain links to `~/obsidian/` files
- Only Obsidian vault files may link to other Obsidian vault files
- Links flow: Obsidian ‚Üí Filesystem (one-way)
- Obsidian notes can reference filesystem locations
- Filesystem content remains link-free

### Naming Convention Rules

**Year-only directories DO NOT use `_XX` suffix:**

- ‚ùå Wrong: `2025_XX-NRC_Misc`
- ‚úÖ Correct: `2025-NRC_Misc`

**Month/Day directories use underscores in date portion:**

- Month-scoped: `YYYY_MM-DocumentName` (e.g., `2024_08-QuarterlyReport`)
- Day-scoped: `YYYY_MM_DD-DeadlineName` (e.g., `2025_03_15-ProposalDue`)

**Name format guidelines:**

- Single hyphen (`-`) separates date from name
- No additional hyphens within namestring
- Use underscores or PascalCase for multi-word names
- Example: `2025-SCALE_Validation` or `2025-ScaleValidation`

## Special Directory Structures

### Presentation Archives

Location: `~/Documents/YYYY-PresentationArchive/`

**Structure:**
```
~/Documents/2024-PresentationArchive/
  Presentation_Name/
    Presentation_Name.pptx
    Presentation_Name.md

~/Documents/2025-PresentationArchive/
  Presentation_Name/
    Presentation_Name.pptx
    Presentation_Name.md
```

**Rules:**
- Flat structure: each presentation in its own directory directly under year archive
- Directory name = presentation name (short, descriptive)
- Markdown conversion: Use `docling file.pptx --to md` to convert to markdown
- Store markdown alongside original presentation with same basename
- Extract author and title from markdown (typically in first 10 lines)
- Create People pages in `~/obsidian/People/` for each author
- Link presentations to authors' People pages

**Pre-2024 archives:**
- Move to `~/_old/YYYY-PresentationArchive/`
- Keeps current archives focused on recent work

### Email Archives

Location: `~/Documents/2025-Emails/`

**Subdirectories:**
- `NRC/` - Nuclear Regulatory Commission correspondence
- `ORNL/` - Internal ORNL communications
- `DNCSH/` - DOE-NE Domestic Nuclear Capabilities Strategic Hub
- `Personal/` - Professional personal correspondence
- `Conferences/` - Conference and meeting related emails

**Naming:** `YYYY_MM_DD-Subject-Correspondent.pdf`

### Meeting Transcripts

Location: `~/Documents/2025-MeetingTranscripts/`

**Purpose:** AI-generated meeting transcripts from Granola and similar services

**Naming:** `YYYY_MM_DD-Meeting_Title-Attendees.format`

**Formats:** PDF (preferred), Markdown, DOCX, TXT, JSON

## Obsidian Vault Organization

### Structure

- **Projects/** - One note per `~/YYYY-projectname/` directory
- **People/** - Collaborators, contacts, professional relationships
- **Topics/** - Technical domains (reactor physics, nuclear data, SCALE)
- **Organizations/** - ORNL, NRC, DOE, universities
- **Records/** - Trip reports, performance reviews, meetings
- **Index.md** - Main dashboard

### Obsidian Vault (`~/obsidian/`)

Knowledge graph and organizational layer containing:
- Project descriptions and status
- Links to filesystem locations
- Relationships between projects/people/topics
- Ideas that may become projects
- Meeting notes and trip reports

**Key capability:** Maintains coherence as filesystem evolves

**Symlink structure:** `~/obsidian/_links/` mirrors home directory, containing symlinks to selected files. Internal to vault for managing embedded content.

**Note on refreshing:** External file modifications may require manual refresh (Cmd+R) in Obsidian. Edits within Obsidian refresh automatically.

### Linking Strategy

- **Bidirectional links** between related concepts
- **MOCs (Maps of Content)** for major areas
- **Tags** for cross-cutting themes (#validation, #proposal, #publication)
- **Dataview queries** for dynamic collections

### Page Templates

**People Pages** (`~/obsidian/People/`)

```markdown
# [Full Name]
**Position:** [Title]
**Organization:** [ORNL/NRC/etc.]
**Expertise:** [Primary areas]

## Collaboration
[Projects worked on together]

## Related
- [[Projects/ProjectName]]
```

Create when: author on presentations, collaborator on projects, frequent correspondent.

**Projects Pages** (`~/obsidian/Projects/`)

```markdown
# [Project Name]
**Status:** Active/Completed/On Hold
**Started:** YYYY-MM
**Directory:** `~/YYYY-projectname/`

## Overview
[Purpose and goals]

## People
- [[People/PersonName]]

## Topics
- [[Topics/TopicName]]
```

**Topics Pages** (`~/obsidian/Topics/`)

Create for: technical subjects spanning multiple projects, domain knowledge areas, cross-cutting methodologies.

---

## AI Agent Guidelines

### File Operation Preferences

**ALWAYS prefer editing existing files over creating new ones**

Exceptions: User explicitly requests new file, no relevant file exists, creating structured Obsidian content.

**Use specialized tools:**
- **Read** not `cat`, `head`, `tail`
- **Edit** not `sed`, `awk`
- **Write** not `echo >` or `cat <<EOF`
- **Glob** not `find`, `ls`
- **Grep** not `grep`, `rg`

Reserve Bash for actual system commands (git, npm, docker).

### Task Management

**Use TodoWrite tool for:**
- Multi-step tasks (3+ steps)
- Complex/non-trivial operations
- User provides multiple tasks
- Tracking progress across session

**Task states:** `pending`, `in_progress` (only ONE at a time), `completed`

**Critical:** Mark tasks completed IMMEDIATELY upon finishing, not in batches.

**Don't use for:** Single straightforward tasks, trivial operations, informational requests.

### Agent Responsibilities

**Filesystem ‚Üí Obsidian:**
- Detect new projects ‚Üí create Project notes
- Monitor file movements ‚Üí update links
- Identify related content ‚Üí suggest connections

**Obsidian ‚Üí Filesystem:**
- New idea note with detail ‚Üí create project directory
- Deadline approaching ‚Üí surface on Desktop
- Project archived ‚Üí consider filesystem archival

**Maintenance:**
- Suggest archiving stale content to `_old/YYYY/`
- Identify misplaced files
- Maintain Index.md dashboard
- Clean up broken links

**Knowledge Discovery:**
- Find connections between new and old work
- Surface relevant archived projects
- Suggest collaborators based on topic overlap

## Common Workflows

### New Project Setup

1. Create directory: `~/YYYY-ProjectName/`
2. Create Obsidian note: `~/obsidian/Projects/YYYY-ProjectName.md`
3. Link to related People and Topics
4. Update `~/obsidian/Index.md` if major project
5. Consider Desktop symlink if immediate focus

### Document Organization

1. Verify document is finalized (not working draft)
2. Place in `~/Documents/YYYY_MM-DocumentName/`
3. Working drafts/code go in project directories
4. Old/reference-only material ‚Üí `~/_old/`

### Archiving Content

When project/content becomes inactive:
1. Create `_old/YYYY/` in appropriate location
2. Move content maintaining structure
3. Update Obsidian links if referenced
4. Don't break existing paths

### Desktop Curation

Desktop contains only:
- 3-5 symlinks to active projects
- Symlinks to imminent deadlines
- Critical files needing immediate attention

Remove symlinks when focus shifts.

### Git Commits

Only create commits when user explicitly requests.

**Format:**
```
Brief summary

- file1: changes
- file2: changes

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
```

**Always use HEREDOC:**
```bash
git commit -m "$(cat <<'EOF'
Summary
- Details
ü§ñ Generated with [Claude Code](https://claude.com/claude-code)
Co-Authored-By: Claude <noreply@anthropic.com>
EOF
)"
```

### Large File Handling

Large files may exceed Read tool limits (256KB):
- Use `offset` and `limit` parameters with Read
- Use Grep to extract specific sections
- Author info typically in first 10-30 lines

---

*Operational guidelines for AI assistants working with the Wieselquist Knowledge System*
- any files named ~/Downloads/extractXX.txt where XX is a number I want you to read and add in the proper places in ~/obsidian
</file>

<file path="tests/test_daemon_maintenance.py">
import time


def test_daemon_background_maintenance(monkeypatch, tmp_path):
    home = tmp_path / "home"
    home.mkdir()
    (home / ".wks").mkdir()
    monkeypatch.setenv("HOME", str(home))

    from wks import daemon as daemon_mod

    class DummyVault:
        def __init__(self, *args, **kwargs):
            self.active_files_max_rows = kwargs.get("active_files_max_rows", 50)

        def ensure_structure(self):
            pass

        def update_active_files(self, *args, **kwargs):
            pass

        def log_file_operation(self, *args, **kwargs):
            pass

        def update_link_on_move(self, *args, **kwargs):
            pass

        def update_vault_links_on_move(self, *args, **kwargs):
            pass

        def create_project_note(self, *args, **kwargs):
            pass

        def write_health_page(self, *args, **kwargs):
            pass

        def mark_reference_deleted(self, *args, **kwargs):
            pass

        def write_doc_text(self, *args, **kwargs):
            pass

    class DummyActivity:
        def __init__(self, *args, **kwargs):
            pass

        def record_event(self, *args, **kwargs):
            pass

        def refresh_angles_all(self):
            pass

        def get_top_active_files(self, *args, **kwargs):
            return []

    monkeypatch.setattr(daemon_mod, "ObsidianVault", DummyVault)

    class DummySimilarity:
        def __init__(self):
            self.audit_calls = 0
            self.closed = False

        def audit_documents(self, remove_missing=True, fix_missing_metadata=True):
            self.audit_calls += 1
            return {"removed": 0, "updated": 0}

        def close(self):
            self.closed = True

    daemon = daemon_mod.WKSDaemon(
        config={},
        vault_path=tmp_path / "vault",
        base_dir="WKS",
        obsidian_log_max_entries=10,
        obsidian_active_files_max_rows=5,
        obsidian_source_max_chars=10,
        obsidian_destination_max_chars=10,
        obsidian_docs_keep=3,
        monitor_paths=[tmp_path],
        maintenance_interval_secs=0.1,
    )

    sim = DummySimilarity()
    daemon.similarity = sim
    daemon._maintenance_interval_secs = 0.05

    daemon._start_maintenance_thread()
    try:
        time.sleep(0.15)
        assert sim.audit_calls >= 1
    finally:
        daemon._stop_maintenance_thread()
        daemon.stop()

    assert sim.closed
</file>

<file path="tests/test_extractor.py">
import hashlib
from pathlib import Path

import pytest

from wks.constants import WKS_EXTRACT_EXT
from wks.extractor import Extractor


def test_docling_fallback_to_builtin_text(tmp_path, monkeypatch):
    sample = tmp_path / "script.py"
    content = "print('hello world')\n"
    sample.write_text(content, encoding="utf-8")

    extractor = Extractor(engine="docling")

    def _fail_docling(self, source: Path):
        raise RuntimeError("docling failed")

    monkeypatch.setattr(Extractor, "_docling_convert", _fail_docling, raising=False)

    result = extractor.extract(sample)

    assert result.text.strip() == content.strip()
    assert result.content_path is not None
    assert result.content_path.suffix == ".py"
    assert result.content_path.parent == sample.parent / WKS_EXTRACT_EXT
    assert result.content_path.exists()

    checksum = hashlib.sha256(content.encode("utf-8")).hexdigest()
    assert result.content_path.name.startswith(checksum)


def test_extractor_uses_repo_root_wks0(tmp_path):
    repo = tmp_path / "repo"
    repo.mkdir()
    (repo / ".git").mkdir()
    subdir = repo / "src"
    subdir.mkdir()
    target = subdir / "example.txt"
    target.write_text("hello world", encoding="utf-8")

    extractor = Extractor(engine="builtin")
    result = extractor.extract(target)

    assert result.content_path is not None
    assert result.content_path.parent == tmp_path / WKS_EXTRACT_EXT
    assert (tmp_path / WKS_EXTRACT_EXT).exists()
    # Ensure no nested .wkso under repo or subdir
    assert not (repo / WKS_EXTRACT_EXT).exists()
    assert not (subdir / WKS_EXTRACT_EXT).exists()
</file>

<file path="tests/test_monitor_controller.py">
"""
Unit tests for MonitorController.

Tests the core business logic for monitor operations without
requiring CLI or display infrastructure.
"""

import unittest
from pathlib import Path
from wks.monitor_controller import MonitorController


class TestMonitorController(unittest.TestCase):
    """Test MonitorController methods."""

    def test_get_status_basic(self):
        """Test get_status with minimal config."""
        config = {
            "monitor": {
                "include_paths": ["~"],
                "exclude_paths": [],
                "managed_directories": {"~/Documents": 100},
                "ignore_dirnames": ["node_modules"],
                "ignore_globs": ["*.tmp"],
                "database": "wks.monitor"
            },
            "mongo": {"uri": "mongodb://localhost:27017/"}
        }

        result = MonitorController.get_status(config)

        self.assertIsNotNone(result.tracked_files)
        self.assertIsNotNone(result.managed_directories)
        self.assertIsNotNone(result.include_paths)
        self.assertIsNotNone(result.exclude_paths)
        self.assertIsInstance(result.issues, list)
        self.assertIsInstance(result.redundancies, list)

    def test_get_status_detects_vault_redundancy(self):
        """Test that vault_path in exclude_paths triggers redundancy warning."""
        config = {
            "vault_path": "~/obsidian",
            "monitor": {
                "include_paths": ["~"],
                "exclude_paths": ["~/obsidian"],  # Redundant - vault auto-excluded
                "managed_directories": {},
                "ignore_dirnames": [],
                "ignore_globs": [],
                "database": "wks.monitor"
            }
        }

        result = MonitorController.get_status(config)

        # Should detect redundancy
        self.assertTrue(any("vault_path is automatically ignored" in r for r in result.redundancies))

    def test_get_status_detects_wks_home_redundancy(self):
        """Test that ~/.wks in exclude_paths triggers redundancy warning."""
        config = {
            "monitor": {
                "include_paths": ["~"],
                "exclude_paths": ["~/.wks"],  # Redundant - WKS home auto-excluded
                "managed_directories": {},
                "ignore_dirnames": [],
                "ignore_globs": [],
                "database": "wks.monitor"
            }
        }

        result = MonitorController.get_status(config)

        # Should detect redundancy
        self.assertTrue(any("WKS home is automatically ignored" in r for r in result.redundancies))

    def test_validate_config_no_issues(self):
        """Test validate_config with clean configuration."""
        config = {
            "monitor": {
                "include_paths": ["~"],
                "exclude_paths": ["~/Downloads"],
                "managed_directories": {"~/Documents": 100},
                "ignore_dirnames": [],
                "ignore_globs": [],
                "database": "wks.monitor"
            }
        }

        result = MonitorController.validate_config(config)

        self.assertEqual(len(result.issues), 0)
        self.assertEqual(len(result.redundancies), 0)

    def test_validate_config_detects_conflicts(self):
        """Test that paths in both include and exclude are detected."""
        config = {
            "monitor": {
                "include_paths": ["~/Documents"],
                "exclude_paths": ["~/Documents"],  # Conflict!
                "managed_directories": {},
                "ignore_dirnames": [],
                "ignore_globs": [],
                "database": "wks.monitor"
            }
        }

        result = MonitorController.validate_config(config)

        self.assertGreater(len(result.issues), 0)
        self.assertTrue(any("both include and exclude" in issue for issue in result.issues))

    def test_check_path_included(self):
        """Test check_path for an included path."""
        config = {
            "monitor": {
                "include_paths": ["~/Documents"],
                "exclude_paths": [],
                "managed_directories": {"~/Documents": 100},
                "ignore_dirnames": [],
                "ignore_globs": [],
                "database": "wks.monitor",
                "priority": {
                    "depth_multiplier": 0.9,
                    "underscore_divisor": 2,
                    "single_underscore_divisor": 64,
                    "extension_weights": {"default": 1.0}
                }
            }
        }

        result = MonitorController.check_path(config, "~/Documents/test.txt")

        self.assertTrue(result["is_monitored"])
        self.assertIsNotNone(result["priority"])
        self.assertGreater(len(result["decisions"]), 0)

    def test_check_path_excluded(self):
        """Test check_path for an excluded path."""
        config = {
            "monitor": {
                "include_paths": ["~"],
                "exclude_paths": ["~/Library"],
                "managed_directories": {"~": 100},
                "ignore_dirnames": [],
                "ignore_globs": [],
                "database": "wks.monitor",
                "priority": {}
            }
        }

        result = MonitorController.check_path(config, "~/Library/test.txt")

        self.assertFalse(result["is_monitored"])
        self.assertIn("exclude_paths", result["reason"])
        self.assertIsNone(result["priority"])

    def test_check_path_ignored_dirname(self):
        """Test check_path for path with ignored dirname."""
        config = {
            "monitor": {
                "include_paths": ["~"],
                "exclude_paths": [],
                "managed_directories": {"~": 100},
                "ignore_dirnames": ["node_modules"],
                "ignore_globs": [],
                "database": "wks.monitor",
                "priority": {}
            }
        }

        result = MonitorController.check_path(config, "~/project/node_modules/test.js")

        self.assertFalse(result["is_monitored"])
        self.assertIn("ignored dirname", result["reason"])

    def test_check_path_ignored_glob(self):
        """Test check_path for path matching ignore_globs."""
        config = {
            "monitor": {
                "include_paths": ["~"],
                "exclude_paths": [],
                "managed_directories": {"~": 100},
                "ignore_dirnames": [],
                "ignore_globs": ["*.tmp"],
                "database": "wks.monitor",
                "priority": {}
            }
        }

        result = MonitorController.check_path(config, "~/test.tmp")

        self.assertFalse(result["is_monitored"])
        self.assertIn("ignore_globs", result["reason"])


if __name__ == "__main__":
    unittest.main()
</file>

<file path="wks/cli/commands/index.py">
"""Index and extract commands (Semantic Engines layer - higher than Monitor)."""

import argparse
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pymongo

from ..dataclasses import DatabaseSummary, FileSummary, FileTimings, IndexResult
from ..helpers import (
    as_file_uri_local,
    build_extractor,
    file_checksum,
    iter_files,
    make_progress,
    maybe_write_json,
)
from ...config import load_config, mongo_settings
from ...constants import WKS_HOME_DISPLAY, MAX_DISPLAY_WIDTH
from ...dbmeta import IncompatibleDatabase, resolve_db_compatibility
from ...status import record_db_activity
from ...utils import get_package_version
from ... import mongoctl


# Similarity loading helpers
def handle_similarity_import_error(e: Exception, require_enabled: bool) -> Tuple[None, None]:
    """Handle import errors for similarity dependencies."""
    error_msg = str(e).lower()
    if "sentence" in error_msg or "transformers" in error_msg:
        from ...error_messages import missing_dependency_error
        missing_dependency_error("sentence-transformers", e)
    elif "docling" in error_msg:
        from ...error_messages import missing_dependency_error
        missing_dependency_error("docling", e)
    else:
        print(f"\nSimilarity features not available: {e}")
        print("Install with: pip install -e '.[all]'\n")
    if require_enabled:
        raise SystemExit(2)
    return None, None


def try_build_similarity(cfg: Dict[str, Any], require_enabled: bool) -> Tuple[Any, Any]:
    """Try to build similarity DB from config."""
    try:
        from ...similarity import build_similarity_from_config  # type: ignore
    except ImportError as e:
        return handle_similarity_import_error(e, require_enabled)
    except Exception as e:
        print(f"Similarity not available: {e}")
        if require_enabled:
            raise SystemExit(2)
        return None, None

    space_tag, _ = resolve_db_compatibility(cfg)
    pkg_version = get_package_version()
    try:
        db, sim_cfg = build_similarity_from_config(
            cfg,
            require_enabled=require_enabled,
            compatibility_tag=space_tag,
            product_version=pkg_version,
        )
        return db, sim_cfg
    except IncompatibleDatabase as exc:
        print(exc)
        if require_enabled:
            raise SystemExit(2)
        return None, None
    except Exception as e:
        return try_auto_start_mongod_and_build(cfg, require_enabled, space_tag, pkg_version, e)


def try_auto_start_mongod_and_build(
    cfg: Dict[str, Any],
    require_enabled: bool,
    space_tag: str,
    pkg_version: str,
    original_error: Exception
) -> Tuple[Any, Any]:
    """Try to auto-start mongod and then build similarity."""
    import shutil
    import subprocess
    from ...constants import WKS_HOME_EXT

    mongo_uri = mongo_settings(cfg)["uri"]
    if not (mongo_uri.startswith("mongodb://localhost:27027") and shutil.which("mongod")):
        print(f"Failed to initialize similarity DB: {original_error}")
        if require_enabled:
            raise SystemExit(2)
        return None, None

    dbroot = Path.home() / WKS_HOME_EXT / "mongodb"
    dbpath = dbroot / "db"
    logfile = dbroot / "mongod.log"
    dbpath.mkdir(parents=True, exist_ok=True)
    try:
        subprocess.check_call([
            "mongod", "--dbpath", str(dbpath), "--logpath", str(logfile),
            "--fork", "--bind_ip", "127.0.0.1", "--port", "27027"
        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        from ...similarity import build_similarity_from_config  # type: ignore
        db, sim_cfg = build_similarity_from_config(
            cfg,
            require_enabled=require_enabled,
            compatibility_tag=space_tag,
            product_version=pkg_version,
        )
        return db, sim_cfg
    except IncompatibleDatabase as exc2:
        print(exc2)
        if require_enabled:
            raise SystemExit(2)
        return None, None
    except Exception as e2:
        print(f"Failed to auto-start local mongod: {e2}")
        if require_enabled:
            raise SystemExit(2)
        return None, None


def build_similarity_from_config(require_enabled: bool = True):
    """Build similarity database from config."""
    cfg = load_config()
    return try_build_similarity(cfg, require_enabled)


def load_similarity_required() -> Tuple[Any, Dict[str, Any]]:
    """Load similarity DB, raising SystemExit if unavailable."""
    db, sim_cfg = build_similarity_from_config(require_enabled=True)
    if db is None or sim_cfg is None:
        raise SystemExit(2)
    return db, sim_cfg


def mongo_client_params(
    server_timeout: int = 500,
    connect_timeout: int = 500,
    cfg: Optional[Dict[str, Any]] = None,
    *,
    ensure_running: bool = True,
) -> Tuple[pymongo.MongoClient, Dict[str, str]]:
    """Return (client, normalized mongo settings)."""
    if cfg is None:
        cfg = load_config()
    mongo_cfg = mongo_settings(cfg)
    client = mongoctl.create_client(
        mongo_cfg['uri'],
        server_timeout=server_timeout,
        connect_timeout=connect_timeout,
        ensure_running=ensure_running,
    )
    return client, mongo_cfg


def load_vault() -> Any:
    """Load Obsidian vault from config."""
    from ...obsidian import ObsidianVault  # lazy import
    cfg = load_config()
    vault_path = cfg.get('vault_path')
    if not vault_path:
        print(f"Fatal: 'vault_path' is required in {WKS_HOME_DISPLAY}/config.json")
        raise SystemExit(2)
    obs = cfg.get('obsidian', {})
    base_dir = obs.get('base_dir')
    if not base_dir:
        print(f"Fatal: 'obsidian.base_dir' is required in {WKS_HOME_DISPLAY}/config.json (e.g., 'WKS')")
        raise SystemExit(2)
    # Require explicit logging caps/widths
    for k in ["log_max_entries", "active_files_max_rows", "source_max_chars", "destination_max_chars"]:
        if k not in obs:
            print(f"Fatal: missing required config key: obsidian.{k}")
            raise SystemExit(2)
    vault = ObsidianVault(
        Path(vault_path).expanduser(),
        base_dir=base_dir,
        log_max_entries=int(obs["log_max_entries"]),
        active_files_max_rows=int(obs["active_files_max_rows"]),
        source_max_chars=int(obs["source_max_chars"]),
        destination_max_chars=int(obs["destination_max_chars"]),
    )
    return vault


# Extract command
def extract_cmd(args: argparse.Namespace) -> int:
    """Extract document text using the configured pipeline."""
    cfg = load_config()
    include_exts = [e.lower() for e in (cfg.get('similarity', {}).get('include_extensions') or [])]
    files = iter_files(args.paths, include_exts, cfg)
    if not files:
        print("No files to extract (check paths/extensions)")
        return 0
    extractor = build_extractor(cfg)
    extracted = 0
    skipped = 0
    errors = 0
    outputs: List[Tuple[Path, Path]] = []
    with make_progress(total=len(files), display=args.display) as prog:
        for f in files:
            prog.update(f.name, advance=0)
            try:
                result = extractor.extract(f, persist=True)
                if result.content_path:
                    extracted += 1
                    outputs.append((f, Path(result.content_path)))
                else:
                    skipped += 1
            except Exception:
                errors += 1
            finally:
                prog.update(f.name, advance=1)
    for src, artefact in outputs:
        print(f"{src} -> {artefact}")
    print(f"Extracted {extracted} file(s), skipped {skipped}, errors {errors}")
    return 0


# Index command helpers
def handle_untrack_mode(files: List[Path], args: argparse.Namespace) -> int:
    """Handle untrack mode - remove files from similarity DB."""
    removed = 0
    missing = 0
    errors = 0
    outcomes: List[Dict[str, Any]] = []
    with make_progress(total=len(files), display=args.display) as prog:
        prog.update("Connecting to DB‚Ä¶", advance=0)
        db, _ = load_similarity_required()
        for f in files:
            prog.update(f"{f.name} ‚Ä¢ untrack", advance=0)
            try:
                if db.remove_file(f):
                    removed += 1
                    outcomes.append({"path": str(f), "status": "removed"})
                else:
                    missing += 1
                    outcomes.append({"path": str(f), "status": "not_tracked"})
            except Exception as exc:
                errors += 1
                outcomes.append({"path": str(f), "status": f"error: {exc}"})
            finally:
                prog.update(f.name, advance=1)
        prog.update("Untracking complete", advance=0)
    payload = {
        "mode": "untrack",
        "requested": [str(p) for p in files],
        "removed": removed,
        "missing": missing,
        "errors": errors,
        "files": outcomes,
    }
    maybe_write_json(args, payload)
    print(f"Untracked {removed} file(s), missing {missing}, errors {errors}")
    return 0


def get_database_summary_for_cached(cfg: Dict[str, Any]) -> Optional[DatabaseSummary]:
    """Get database summary when all files are cached."""
    try:
        client, mongo_cfg = mongo_client_params(
            server_timeout=300,
            connect_timeout=300,
            cfg=cfg,
            ensure_running=False,
        )
        coll = client[mongo_cfg['space_database']][mongo_cfg['space_collection']]
        total_files = coll.count_documents({})
        client.close()
        return DatabaseSummary(
            database=mongo_cfg['space_database'],
            collection=mongo_cfg['space_collection'],
            total_files=total_files,
        )
    except Exception:
        return None


def handle_all_cached_case(
    files: List[Path],
    pre_skipped: List[Path],
    hash_times: Dict[Path, Optional[float]],
    cfg: Dict[str, Any],
    display_mode: str,
    args: argparse.Namespace
) -> int:
    """Handle case where all files are already cached."""
    skipped = len(pre_skipped)
    cached_summaries = [
        create_file_summary(
            path=f,
            status="cached",
            hash_time=hash_times.get(f),
        )
        for f in pre_skipped
    ]

    db_summary_obj = get_database_summary_for_cached(cfg)

    result = IndexResult(
        mode="index",
        requested=[str(p) for p in files],
        added=0,
        skipped=skipped,
        errors=0,
        files=cached_summaries,
        database=db_summary_obj,
    )
    maybe_write_json(args, result.to_dict())

    print("Nothing to index; all files already current.")
    print(f"Indexed 0 file(s), skipped {skipped}, errors 0")
    if db_summary_obj:
        print(
            f"DB: {db_summary_obj.database}.{db_summary_obj.collection} total_files={db_summary_obj.total_files}"
        )
    if cached_summaries:
        render_timing_summary_updated(cached_summaries, display_mode)
    return 0


def extract_file_content(extractor: Any, f: Path, prog: Any) -> Tuple[Any, float]:
    """Extract file content. Returns (extraction, extract_time)."""
    prog.update(f"{f.name} ‚Ä¢ extract", advance=0)
    extract_start = time.perf_counter()
    extraction = extractor.extract(f, persist=True)
    extract_time = time.perf_counter() - extract_start
    return extraction, extract_time


def embed_file_to_db(
    db: Any,
    f: Path,
    extraction: Any,
    checksums: Dict[Path, Optional[str]],
    file_sizes: Dict[Path, Optional[int]],
    prog: Any
) -> Tuple[bool, Dict[str, float]]:
    """Embed file to database. Returns (updated, rec_timings)."""
    prog.update(f"{f.name} ‚Ä¢ embed", advance=0)
    kwargs: Dict[str, Any] = {}
    checksum_value = checksums.get(f)
    if checksum_value is not None:
        kwargs['file_checksum'] = checksum_value
    size_value = file_sizes.get(f)
    if size_value is not None:
        kwargs['file_bytes'] = size_value

    updated = db.add_file(f, extraction=extraction, **kwargs)
    rec = db.get_last_add_result() or {}
    rec_timings = rec.get('timings') or {}
    return updated, rec_timings


def process_files_for_indexing(
    files_to_process: List[Path],
    pre_skipped: List[Path],
    hash_times: Dict[Path, Optional[float]],
    checksums: Dict[Path, Optional[str]],
    file_sizes: Dict[Path, Optional[int]],
    cfg: Dict[str, Any],
    docs_keep: int,
    display_mode: str
) -> Tuple[List[FileSummary], int, int, int, Any]:
    """Process files through indexing pipeline.

    Returns:
        Tuple of (summaries, added, skipped, errors, db)
    """
    added = 0
    skipped = len(pre_skipped)
    errors = 0
    summaries: List[FileSummary] = []

    with make_progress(total=len(files_to_process), display=display_mode) as prog:
        prog.update(f"Pre-checking {len(files_to_process) + len(pre_skipped)} file(s)‚Ä¶", advance=0)
        prog.update("Connecting to DB‚Ä¶", advance=0)
        db, _ = load_similarity_required()
        extractor = build_extractor(cfg)
        vault = load_vault()

        for f in files_to_process:
            summary, updated, error_occurred = process_single_file(
                f, hash_times, checksums, file_sizes, extractor, db, vault, docs_keep, prog
            )
            summaries.append(summary)
            if error_occurred:
                errors += 1
            elif updated:
                added += 1
            else:
                skipped += 1

        prog.update("DB update complete", advance=0)

    # Add cached files to summaries
    for f in pre_skipped:
        summaries.append(create_file_summary(
            path=f,
            status="cached",
            hash_time=hash_times.get(f),
        ))

    return summaries, added, skipped, errors, db


def write_to_obsidian(vault: Any, db: Any, f: Path, docs_keep: int, prog: Any) -> Optional[float]:
    """Write file to Obsidian vault. Returns obsidian_time or None."""
    rec = db.get_last_add_result() or {}
    ch = rec.get('content_checksum') or rec.get('content_hash')
    txt = rec.get('text')
    if not (ch and txt is not None):
        return None

    try:
        prog.update(f"{f.name} ‚Ä¢ obsidian", advance=0)
        obs_start = time.perf_counter()
        vault.write_doc_text(ch, f, txt, keep=docs_keep)
        return time.perf_counter() - obs_start
    except Exception:
        return None


def process_single_file(
    f: Path,
    hash_times: Dict[Path, Optional[float]],
    checksums: Dict[Path, Optional[str]],
    file_sizes: Dict[Path, Optional[int]],
    extractor: Any,
    db: Any,
    vault: Any,
    docs_keep: int,
    prog: Any,
) -> Tuple[FileSummary, bool, bool]:
    """Process a single file through extract, embed, and obsidian stages.

    Returns:
        Tuple of (FileSummary, updated, error_occurred)
    """
    try:
        extraction, extract_time = extract_file_content(extractor, f, prog)
    except Exception as exc:
        prog.update(f.name, advance=1)
        return create_file_summary(
            path=f,
            status=f"error: {exc}",
            hash_time=hash_times.get(f),
        ), False, True

    try:
        updated, rec_timings = embed_file_to_db(db, f, extraction, checksums, file_sizes, prog)
    except Exception:
        prog.update(f.name, advance=1)
        return create_file_summary(
            path=f,
            status="error",
            hash_time=hash_times.get(f),
            extract_time=extract_time,
        ), False, True

    obsidian_time = write_to_obsidian(vault, db, f, docs_keep, prog) if updated else None
    prog.update(f.name, advance=1)

    summary = create_file_summary(
        path=f,
        status="updated" if updated else "unchanged",
        hash_time=hash_times.get(f),
        extract_time=extract_time,
        embed_time=rec_timings.get('embed'),
        db_time=rec_timings.get('db_update'),
        chunks_time=rec_timings.get('chunks'),
        obsidian_time=obsidian_time,
    )
    return summary, updated, False


def compute_file_checksums(files: List[Path]) -> Tuple[Dict[Path, Optional[float]], Dict[Path, Optional[str]], Dict[Path, Optional[int]]]:
    """Compute checksums, hash times, and file sizes for files."""
    hash_times: Dict[Path, Optional[float]] = {}
    checksums: Dict[Path, Optional[str]] = {}
    file_sizes: Dict[Path, Optional[int]] = {}
    for f in files:
        try:
            h_start = time.perf_counter()
            checksum = file_checksum(f)
            hash_times[f] = time.perf_counter() - h_start
            checksums[f] = checksum
        except Exception:
            hash_times[f] = None
            checksums[f] = None
        try:
            file_sizes[f] = f.stat().st_size
        except Exception:
            file_sizes[f] = None
    return hash_times, checksums, file_sizes


def precheck_files(
    files: List[Path],
    checksums: Dict[Path, Optional[str]],
    cfg: Dict[str, Any]
) -> Tuple[List[Path], List[Path]]:
    """Precheck files against database to find which need processing.

    Returns:
        Tuple of (files_to_process, pre_skipped)
    """
    pre_skipped: List[Path] = []
    files_to_process = list(files)

    try:
        client, mongo_cfg = mongo_client_params(
            server_timeout=300,
            connect_timeout=300,
            cfg=cfg,
            ensure_running=False,
        )
    except Exception:
        return files_to_process, pre_skipped

    try:
        coll = client[mongo_cfg['space_database']][mongo_cfg['space_collection']]
        to_process: List[Path] = []
        for f in files:
            checksum = checksums.get(f)
            if checksum is None:
                to_process.append(f)
                continue
            doc = coll.find_one({"path": as_file_uri_local(f)})
            try:
                record_db_activity("index.precheck", str(f))
            except Exception:
                pass
            if not doc:
                doc = coll.find_one({"path_local": str(f.resolve())})
            if doc and doc.get("checksum") == checksum:
                pre_skipped.append(f)
            else:
                to_process.append(f)
        files_to_process = to_process
    finally:
        try:
            client.close()
        except Exception:
            pass
    return files_to_process, pre_skipped


def create_file_summary(
    path: Path,
    status: str,
    hash_time: Optional[float] = None,
    extract_time: Optional[float] = None,
    embed_time: Optional[float] = None,
    db_time: Optional[float] = None,
    chunks_time: Optional[float] = None,
    obsidian_time: Optional[float] = None,
) -> FileSummary:
    """Create a FileSummary from timing data."""
    timings = FileTimings(
        hash=hash_time,
        extract=extract_time,
        embed=embed_time,
        db=db_time,
        chunks=chunks_time,
        obsidian=obsidian_time,
    )
    return FileSummary(path=path, status=status, timings=timings)


# Timing summary rendering
def fmt_duration(seconds: Optional[float]) -> str:
    """Format duration in seconds as human-readable string."""
    if seconds is None:
        return "‚Äî"
    if seconds >= 1:
        return f"{seconds:6.2f} {'s':>2}"
    return f"{seconds * 1000:6.1f} {'ms':>2}"


def get_stage_labels() -> List[Tuple[str, str]]:
    """Get stage labels for timing summary."""
    return [
        ("hash", "Hash"),
        ("extract", "Extract"),
        ("embed", "Embed"),
        ("db", "DB"),
        ("chunks", "Chunks"),
        ("obsidian", "Obsidian"),
    ]


def calculate_timing_totals(entries: List[FileSummary]) -> Tuple[Dict[str, float], Dict[str, int]]:
    """Calculate totals and counts for timing stages."""
    stage_labels = get_stage_labels()
    totals: Dict[str, float] = {key: 0.0 for key, _ in stage_labels}
    counts: Dict[str, int] = {key: 0 for key, _ in stage_labels}
    for entry in entries:
        timings = entry.timings
        for key, _ in stage_labels:
            val = getattr(timings, key)
            if isinstance(val, (int, float)):
                totals[key] += val
                counts[key] += 1
    return totals, counts


def create_rich_timing_console():
    """Create Rich Console for timing table."""
    from rich.console import Console
    return Console(
        force_terminal=True,
        color_system="standard",
        markup=True,
        highlight=False,
        soft_wrap=False,
    )


def create_rich_timing_table():
    """Create Rich Table for timing summary."""
    from rich import box
    from rich.table import Table

    stage_labels = get_stage_labels()
    detail = Table(
        show_header=True,
        header_style="bold",
        expand=False,
        box=box.SQUARE,
        pad_edge=False,
    )
    detail.add_column("#", justify="right", no_wrap=True, overflow="ignore", min_width=2, max_width=3)
    detail.add_column(
        "File",
        style="cyan",
        no_wrap=False,
        overflow="fold",
        min_width=12,
        max_width=28,
    )
    detail.add_column(
        "Status",
        style="magenta",
        no_wrap=False,
        overflow="fold",
        min_width=8,
        max_width=20,
    )
    for _, label in stage_labels:
        detail.add_column(label, justify="right", no_wrap=True, overflow="ignore", min_width=9, max_width=11)
    return detail


def add_timing_table_rows(
    table: Any,
    entries: List[FileSummary],
    totals: Dict[str, float],
    counts: Dict[str, int]
) -> None:
    """Add data rows to timing table."""
    stage_labels = get_stage_labels()
    for idx, entry in enumerate(entries, 1):
        row = [str(idx), entry.path.name, entry.status]
        for key, _ in stage_labels:
            val = getattr(entry.timings, key)
            row.append(fmt_duration(val))
        table.add_row(*row)
    total_files = max(counts.values()) if counts.values() else len(entries)
    total_row = ["-", "Totals", f"{total_files} file(s)"]
    for key, _ in stage_labels:
        total_row.append(fmt_duration(totals[key] if counts.get(key) else None))
    table.add_row(*total_row, style="bold")


def render_rich_timing_table(entries: List[FileSummary], totals: Dict[str, float], counts: Dict[str, int]) -> None:
    """Render timing summary using Rich table."""
    from rich.panel import Panel

    console = create_rich_timing_console()
    table = create_rich_timing_table()

    console.print()
    add_timing_table_rows(table, entries, totals, counts)

    console.print(Panel.fit(table, title="Timing Details", border_style="dim"), width=MAX_DISPLAY_WIDTH)


def render_plain_timing_table(entries: List[FileSummary], totals: Dict[str, float], counts: Dict[str, int]) -> None:
    """Render timing summary using plain text table."""
    stage_labels = get_stage_labels()
    header = ["#", "File", "Status"] + [label for _, label in stage_labels]

    # Build rows
    details_rows: List[List[str]] = []
    for idx, entry in enumerate(entries, 1):
        row = [str(idx), entry.path.name[:32], entry.status[:20]]
        for key, _ in stage_labels:
            val = getattr(entry.timings, key)
            row.append(fmt_duration(val))
        details_rows.append(row)

    total_files = max(counts.values()) if counts.values() else len(entries)
    totals_row = ["-", "Totals", f"{total_files} file(s)"]
    for key, _ in stage_labels:
        totals_row.append(fmt_duration(totals[key] if counts.get(key) else None))
    details_rows.append(totals_row)

    # Simple table output
    print("\nTiming Details")
    print("=" * 80)
    print(" | ".join(h.ljust(10) for h in header))
    print("-" * 80)
    for row in details_rows:
        print(" | ".join(str(cell).ljust(10)[:10] for cell in row))
    print("=" * 80)


def render_timing_summary_updated(entries: List[FileSummary], display_mode: str) -> None:
    """Render timing summary using FileSummary dataclasses."""
    if not entries:
        return

    totals, counts = calculate_timing_totals(entries)

    if display_mode == "cli":
        render_rich_timing_table(entries, totals, counts)
    else:
        render_plain_timing_table(entries, totals, counts)


# Index command
def index_cmd(args: argparse.Namespace) -> int:
    """Index files or directories (recursive) into similarity DB with progress."""
    cfg = load_config()
    include_exts = [e.lower() for e in (cfg.get('similarity', {}).get('include_extensions') or [])]
    files = iter_files(args.paths, include_exts, cfg)
    if not files:
        print("No files to process (check paths/extensions)")
        return 0

    display_mode = args.display

    if args.untrack:
        return handle_untrack_mode(files, args)

    # Compute checksums and file sizes
    hash_times, checksums, file_sizes = compute_file_checksums(files)

    # Precheck files against database
    files_to_process, pre_skipped = precheck_files(files, checksums, cfg)

    if not files_to_process:
        return handle_all_cached_case(files, pre_skipped, hash_times, cfg, display_mode, args)

    # Process files through indexing pipeline
    docs_keep = int((cfg.get('obsidian') or {}).get('docs_keep', 99))
    summaries, added, skipped, errors, db = process_files_for_indexing(
        files_to_process, pre_skipped, hash_times, checksums, file_sizes, cfg, docs_keep, args.display
    )

    # Get database summary and output results
    db_summary_obj = get_database_summary_from_db(db)
    output_index_results(args, files, added, skipped, errors, summaries, db_summary_obj, display_mode)
    return 0


def get_database_summary_from_db(db: Any) -> Optional[DatabaseSummary]:
    """Get database summary from similarity DB."""
    try:
        stats = db.get_stats()
        if stats:
            return DatabaseSummary(
                database=stats.get("database", ""),
                collection=stats.get("collection", ""),
                total_files=stats.get("total_files", 0),
                total_bytes=stats.get("total_bytes"),
            )
    except Exception:
        pass
    return None


def output_index_results(
    args: argparse.Namespace,
    files: List[Path],
    added: int,
    skipped: int,
    errors: int,
    summaries: List[FileSummary],
    db_summary_obj: Optional[DatabaseSummary],
    display_mode: str
) -> None:
    """Output index command results."""
    result = IndexResult(
        mode="index",
        requested=[str(p) for p in files],
        added=added,
        skipped=skipped,
        errors=errors,
        files=summaries,
        database=db_summary_obj,
    )
    maybe_write_json(args, result.to_dict())

    print(f"Indexed {added} file(s), skipped {skipped}, errors {errors}")

    if summaries:
        render_timing_summary_updated(summaries, display_mode)

    if db_summary_obj:
        print_database_summary(db_summary_obj)


def print_database_summary(db_summary_obj: DatabaseSummary) -> None:
    """Print database summary information."""
    if db_summary_obj.total_bytes is not None:
        print(
            f"DB: {db_summary_obj.database}.{db_summary_obj.collection} total_files={db_summary_obj.total_files} total_bytes={db_summary_obj.total_bytes}"
        )
    else:
        print(
            f"DB: {db_summary_obj.database}.{db_summary_obj.collection} total_files={db_summary_obj.total_files}"
        )


def setup_index_parser(subparsers) -> None:
    """Setup index and extract command parsers."""
    # Extract command
    extp = subparsers.add_parser("extract", help="Extract document text using the configured pipeline")
    extp.add_argument("paths", nargs="+", help="Files or directories to extract")
    extp.set_defaults(func=extract_cmd)

    # Index command
    idx = subparsers.add_parser("index", help="Index files or directories (recursive) into similarity DB with progress")
    idx.add_argument("--untrack", action="store_true", help="Remove tracked entries (and artefacts) instead of indexing")
    idx.add_argument("paths", nargs="+", help="Files or directories to process")
    idx.set_defaults(func=index_cmd)
</file>

<file path="wks/__init__.py">
"""
Wieselquist Knowledge System (WKS)

An AI-assisted file organization and knowledge management system.
"""

__version__ = "0.2.5"
</file>

<file path="wks/constants.py">
"""Shared constants for WKS dot-directories and artefact locations."""

WKS_HOME_EXT = ".wks"   # user-level state/config directory suffix
WKS_EXTRACT_EXT = ".wkso"  # per-directory extraction artefact folder
WKS_HOME_DISPLAY = f"~/{WKS_HOME_EXT}"  # user-readable path hint

# Convenience set for directories that should be auto-ignored in scans
WKS_DOT_DIRS = {WKS_HOME_EXT, WKS_EXTRACT_EXT}

# Display width constant - standardize to 80 characters max
MAX_DISPLAY_WIDTH = 80
</file>

<file path="wks/dbmeta.py">
"""
Database compatibility helpers.

We persist a tiny metadata document inside each Mongo database so upgrades
can detect whether an existing dataset is compatible with the current CLI.
"""

from __future__ import annotations

from datetime import datetime, timezone
from typing import Dict, Tuple, Any

SPACE_COMPAT_DEFAULT = "space-v1"
TIME_COMPAT_DEFAULT = "time-v1"
META_COLLECTION = "_wks_meta"
_SCOPE_KEYS = {
    "space": "mongo.compatibility.space",
    "time": "mongo.compatibility.time",
}


class IncompatibleDatabase(RuntimeError):
    """Raised when the stored compatibility tag does not match expectations."""

    def __init__(self, scope: str, stored_tag: str, expected_tag: str):
        config_key = _SCOPE_KEYS.get(scope, f"mongo.compatibility.{scope}")
        message = (
            f"Incompatible {scope} database: stored compatibility tag '{stored_tag or '?'}' "
            f"does not match expected '{expected_tag}'. "
            f"Update {config_key} in ~/.wks/config.json to '{stored_tag}' to reuse the existing data, "
            "or run `wks0 db reset` to rebuild the database."
        )
        super().__init__(message)
        self.scope = scope
        self.stored_tag = stored_tag
        self.expected_tag = expected_tag


def _utc_now() -> str:
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")


def _normalize_tag(value: Any, default: str) -> str:
    if isinstance(value, str):
        text = value.strip()
        if text:
            return text
    return default


def resolve_db_compatibility(cfg: Dict[str, Any]) -> Tuple[str, str]:
    """Return (space_tag, time_tag) strings using config overrides when present."""
    compat_cfg = (cfg.get("mongo") or {}).get("compatibility") or {}
    space_tag = _normalize_tag(compat_cfg.get("space"), SPACE_COMPAT_DEFAULT)
    time_tag = _normalize_tag(compat_cfg.get("time"), TIME_COMPAT_DEFAULT)
    return space_tag, time_tag


def ensure_db_compat(
    client,
    database_name: str,
    scope: str,
    expected_tag: str,
    *,
    product_version: str | None = None,
) -> str:
    """
    Ensure the given database stores the expected compatibility tag.

    Returns the stored tag (which matches expected_tag) or raises IncompatibleDatabase.
    """
    db = client[database_name]
    meta = db[META_COLLECTION]
    now = _utc_now()
    doc = meta.find_one({"_id": scope})
    if doc is None:
        meta.replace_one(
            {"_id": scope},
            {
                "compat_tag": expected_tag,
                "created_at": now,
                "last_used_at": now,
                "product_version": product_version,
            },
            upsert=True,
        )
        return expected_tag
    stored_tag = _normalize_tag(doc.get("compat_tag"), "")
    if stored_tag != expected_tag:
        raise IncompatibleDatabase(scope, stored_tag, expected_tag)
    meta.update_one(
        {"_id": scope},
        {
            "$set": {
                "last_used_at": now,
                "product_version": product_version or doc.get("product_version"),
            }
        },
    )
    return stored_tag
</file>

<file path="wks/error_messages.py">
"""Helpful error messages for common WKS failure modes."""

import sys
from typing import NoReturn


def mongodb_connection_error(mongo_uri: str, original_error: Exception) -> NoReturn:
    """Print helpful message for MongoDB connection failures and exit.

    Args:
        mongo_uri: The MongoDB URI that failed to connect
        original_error: The original exception
    """
    print("\n" + "=" * 70)
    print("MONGODB CONNECTION FAILED")
    print("=" * 70)
    print(f"\nCouldn't connect to MongoDB at: {mongo_uri}")
    print(f"\nOriginal error: {original_error}")
    print("\nPossible solutions:")
    print("  1. Start MongoDB if it's not running:")
    print("     - macOS: brew services start mongodb-community")
    print("     - Linux: sudo systemctl start mongod")
    print("     - Or use: wks0 daemon start (starts MongoDB automatically)")
    print("\n  2. Check if MongoDB is running:")
    print("     - macOS: brew services list | grep mongodb")
    print("     - Linux: sudo systemctl status mongod")
    print("\n  3. Verify the mongo.uri in ~/.wks/config.json")
    print("     - Default: mongodb://localhost:27017/")
    print("=" * 70 + "\n")
    sys.exit(1)


def missing_dependency_error(package_name: str, import_error: Exception) -> NoReturn:
    """Print helpful message for missing dependencies and exit.

    Args:
        package_name: Name of the missing package
        import_error: The original ImportError
    """
    print("\n" + "=" * 70)
    print(f"MISSING DEPENDENCY: {package_name}")
    print("=" * 70)
    print(f"\nCouldn't import required package: {package_name}")
    print(f"\nOriginal error: {import_error}")
    print("\nSolution:")
    print(f"  Install WKS with all dependencies:")
    print(f"     pip install -e '.[all]'")
    print("\n  Or install specific package:")
    print(f"     pip install {package_name}")
    print("=" * 70 + "\n")
    sys.exit(1)


def file_permission_error(file_path: str, operation: str, original_error: Exception) -> NoReturn:
    """Print helpful message for file permission errors and exit.

    Args:
        file_path: Path to the file that caused the error
        operation: Description of the operation (e.g., "read", "write", "delete")
        original_error: The original PermissionError
    """
    print("\n" + "=" * 70)
    print("FILE PERMISSION ERROR")
    print("=" * 70)
    print(f"\nCouldn't {operation}: {file_path}")
    print(f"\nOriginal error: {original_error}")
    print("\nPossible solutions:")
    print("  1. Check file permissions:")
    print(f"     ls -la '{file_path}'")
    print("\n  2. Grant read/write permissions:")
    print(f"     chmod u+rw '{file_path}'")
    print("\n  3. Check directory permissions:")
    print(f"     ls -la $(dirname '{file_path}')")
    print("=" * 70 + "\n")
    sys.exit(1)


def vault_path_error(vault_path: str) -> NoReturn:
    """Print helpful message when Obsidian vault path is invalid.

    Args:
        vault_path: The invalid vault path
    """
    print("\n" + "=" * 70)
    print("INVALID OBSIDIAN VAULT PATH")
    print("=" * 70)
    print(f"\nVault path does not exist: {vault_path}")
    print("\nPlease verify:")
    print("  1. The vault_path in ~/.wks/config.json points to your Obsidian vault")
    print("  2. The directory exists and is accessible")
    print("\nExample config.json:")
    print('  {')
    print('    "vault_path": "~/obsidian",')
    print('    ...')
    print('  }')
    print("=" * 70 + "\n")
    sys.exit(1)


def model_download_error(model_name: str, original_error: Exception) -> NoReturn:
    """Print helpful message for model download failures.

    Args:
        model_name: Name of the model that failed to download
        original_error: The original exception
    """
    print("\n" + "=" * 70)
    print("MODEL DOWNLOAD FAILED")
    print("=" * 70)
    print(f"\nCouldn't download model: {model_name}")
    print(f"\nOriginal error: {original_error}")
    print("\nPossible solutions:")
    print("  1. Check internet connection")
    print("  2. Verify HuggingFace is accessible:")
    print("     curl -I https://huggingface.co")
    print("\n  3. Try downloading manually:")
    print(f"     python -c 'from sentence_transformers import SentenceTransformer; SentenceTransformer(\"{model_name}\")'")
    print("\n  4. If behind proxy, set environment variables:")
    print("     export HTTP_PROXY=http://proxy:port")
    print("     export HTTPS_PROXY=http://proxy:port")
    print("=" * 70 + "\n")
    sys.exit(1)
</file>

<file path="wks/logging_config.py">
"""Centralized logging configuration for WKS."""

import logging
import sys
from pathlib import Path
from typing import Optional

from .utils import wks_home_path


def setup_logging(
    level: int = logging.INFO,
    log_file: Optional[Path] = None,
    format_string: Optional[str] = None
) -> None:
    """
    Configure structured logging for WKS.

    Args:
        level: Logging level (default INFO)
        log_file: Optional path to log file (default ~/.wks/wks.log)
        format_string: Optional custom format string
    """
    if format_string is None:
        format_string = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

    if log_file is None:
        log_file = wks_home_path("wks.log")

    # Ensure log directory exists
    log_file.parent.mkdir(parents=True, exist_ok=True)

    # Configure root logger
    logging.basicConfig(
        level=level,
        format=format_string,
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stderr)
        ]
    )

    # Suppress noisy third-party loggers
    logging.getLogger('sentence_transformers').setLevel(logging.WARNING)
    logging.getLogger('docling').setLevel(logging.WARNING)
    logging.getLogger('pymongo').setLevel(logging.WARNING)


def get_logger(name: str) -> logging.Logger:
    """Get a logger instance for a module."""
    return logging.getLogger(f"wks.{name}")
</file>

<file path="tests/test_cli_db_info.py">
import io
import json
from contextlib import redirect_stdout

import mongomock
import pytest


@pytest.fixture()
def singleton_client():
    # One shared in-memory client for the test
    return mongomock.MongoClient()


def test_db_info_json_lists_tracked_and_latest(monkeypatch, tmp_path, singleton_client):
    monkeypatch.setattr('pathlib.Path.home', lambda: tmp_path)
    # Minimal config for space DB
    def _cfg():
        return {
            'similarity': {
                'mongo_uri': 'mongodb://localhost:27027/',
                'database': 'wks_similarity',
                'collection': 'file_embeddings',
            }
        }
    monkeypatch.setattr('wks.cli.load_config', _cfg)

    # Seed in-memory DB
    client = singleton_client
    coll = client['wks_similarity']['file_embeddings']
    coll.insert_many([
        {'path': 'file:///tmp/x.txt', 'timestamp': '2025-10-28T10:00:00.000000', 'checksum': '0123456789abcdef', 'bytes': 1024, 'angle': 2.5},
        {'path': 'file:///tmp/y.txt', 'timestamp': '2025-10-28T11:00:00.000000', 'checksum': 'fedcba9876543210', 'bytes': 2048, 'angle': 5.0},
    ])

    # Ensure the CLI uses the same singleton client
    monkeypatch.setattr('pymongo.MongoClient', lambda *a, **k: client)

    # Run CLI
    from wks.cli import main
    buf = io.StringIO()
    with redirect_stdout(buf):
        rc = main(['--display','json','db','info','-n','2','--space'])
    out = buf.getvalue()
    data = json.loads(out)
    assert rc == 0
    assert data.get('tracked_files') == 2 or data.get('total_docs') == 2
    latest = data.get('latest') or data.get('records') or []
    assert len(latest) >= 1
    assert any('file:///tmp/x.txt' in (item.get('uri') or '') or 'file:///tmp/y.txt' in (item.get('uri') or '') for item in latest)


def test_db_info_uses_top_level_mongo(monkeypatch, tmp_path, singleton_client):
    monkeypatch.setattr('pathlib.Path.home', lambda: tmp_path)
    def _cfg():
        return {
            'mongo': {
                'uri': 'mongodb://localhost:27027/',
                'space_database': 'wks_similarity',
                'space_collection': 'file_embeddings',
            }
        }
    monkeypatch.setattr('wks.cli.load_config', _cfg)
    client = singleton_client
    coll = client['wks_similarity']['file_embeddings']
    coll.insert_one({'path': 'file:///tmp/z.txt', 'timestamp': '2025-10-28T12:00:00.000000', 'checksum': 'abcdef1234567890', 'bytes': 512, 'angle': 1.0})
    monkeypatch.setattr('pymongo.MongoClient', lambda *a, **k: client)
    from wks.cli import main
    buf = io.StringIO()
    with redirect_stdout(buf):
        rc = main(['--display','json','db','info','--space'])
    assert rc == 0
    output = buf.getvalue()
    data = json.loads(output)
    latest = data.get('latest') or []
    assert any(item.get('uri') == 'file:///tmp/z.txt' for item in latest)


def test_db_info_respects_timestamp_format(monkeypatch, tmp_path, singleton_client):
    monkeypatch.setattr('pathlib.Path.home', lambda: tmp_path)
    def _cfg():
        return {
            'display': {'timestamp_format': '%m/%d/%Y %H:%M'},
            'mongo': {
                'uri': 'mongodb://localhost:27027/',
                'space_database': 'wks_similarity',
                'space_collection': 'file_embeddings',
            }
        }
    monkeypatch.setattr('wks.cli.load_config', _cfg)
    client = singleton_client
    coll = client['wks_similarity']['file_embeddings']
    coll.insert_one({'path': 'file:///tmp/f.txt', 'timestamp': '2025-10-28T08:15:00.000000', 'checksum': '0123456789abcdef', 'bytes': 256, 'angle': 0.0})
    monkeypatch.setattr('pymongo.MongoClient', lambda *a, **k: client)
    from wks.cli import main
    buf = io.StringIO()
    with redirect_stdout(buf):
        rc = main(['--display','json','db','info','--space'])
    assert rc == 0
    output = buf.getvalue()
    data = json.loads(output)
    latest = data.get('latest') or []
    assert any('10/28/2025 08:15' in (item.get('timestamp') or '') for item in latest)


def test_db_info_reference_differences(monkeypatch, tmp_path, singleton_client):
    ref_file = tmp_path / 'ref.txt'
    ref_file.write_text('reference content', encoding='utf-8')
    ref_uri = ref_file.resolve().as_uri()

    def _cfg():
        return {
            'similarity': {
                'mongo_uri': 'mongodb://localhost:27027/',
                'database': 'wks_similarity',
                'collection': 'file_embeddings',
            },
            'extract': {
                'engine': 'builtin',
                'ocr': False,
                'timeout_secs': 30,
            }
        }

    monkeypatch.setattr('wks.cli.load_config', _cfg)

    class FakeDB:
        def __init__(self):
            class _Client:
                def close(self_inner):
                    pass
            self.client = _Client()
        def add_file(self, *a, **k):
            return True

    monkeypatch.setattr('wks.cli._load_similarity_required', lambda: (FakeDB(), {}))

    class DummyExtractor:
        def extract(self, path, persist=True):
            from types import SimpleNamespace
            return SimpleNamespace(
                text='content',
                content_path=None,
                content_checksum='hash',
                content_bytes=10,
            )

    monkeypatch.setattr('wks.cli._build_extractor', lambda cfg: DummyExtractor())

    client = singleton_client
    coll = client['wks_similarity']['file_embeddings']
    coll.insert_many([
        {
            'path': ref_uri,
            'path_local': str(ref_file.resolve()),
            'timestamp': '2025-10-30T00:00:00Z',
            'checksum': 'aaaa',
            'bytes': 100,
            'embedding': [1.0, 0.0, 0.0],
        },
        {
            'path': 'file:///tmp/other.txt',
            'path_local': '/tmp/other.txt',
            'timestamp': '2025-10-30T01:00:00Z',
            'checksum': 'bbbb',
            'bytes': 200,
            'embedding': [0.0, 1.0, 0.0],
        },
    ])

    monkeypatch.setattr('pymongo.MongoClient', lambda *a, **k: client)

    from wks.cli import main
    buf = io.StringIO()
    with redirect_stdout(buf):
        rc = main([
            '--display', 'json',
            'db', 'info', '--space',
            '--reference', str(ref_file),
            '--latest', '2',
        ])

    out = buf.getvalue()
    assert rc == 0
    data = json.loads(out)
    assert data['reference'].startswith('file://')
    entries = data['entries']
    assert len(entries) == 2
    assert any(entry['checksum_same'] is True for entry in entries)
    assert any(entry['checksum_same'] is False for entry in entries)


def test_db_info_incompatible_db_requires_override(monkeypatch, tmp_path, singleton_client):
    monkeypatch.setattr('pathlib.Path.home', lambda: tmp_path)

    def _cfg():
        return {
            'mongo': {
                'uri': 'mongodb://localhost:27027/',
                'space_database': 'wks_similarity',
                'space_collection': 'file_embeddings',
            }
        }

    monkeypatch.setattr('wks.cli.load_config', _cfg)
    client = singleton_client
    client['wks_similarity']['_wks_meta'].insert_one({'_id': 'space', 'compat_tag': 'legacy-space'})
    monkeypatch.setattr('pymongo.MongoClient', lambda *a, **k: client)
    from wks.cli import main
    buf = io.StringIO()
    with redirect_stdout(buf):
        rc = main(['--display', 'plain', 'db', 'info', '--space'])
    assert rc == 2
    assert "Incompatible space database" in buf.getvalue()


def test_db_info_respects_compat_override(monkeypatch, tmp_path, singleton_client):
    monkeypatch.setattr('pathlib.Path.home', lambda: tmp_path)

    def _cfg():
        return {
            'mongo': {
                'uri': 'mongodb://localhost:27027/',
                'space_database': 'wks_similarity',
                'space_collection': 'file_embeddings',
                'compatibility': {'space': 'legacy-space'},
            }
        }

    monkeypatch.setattr('wks.cli.load_config', _cfg)
    client = singleton_client
    client['wks_similarity']['_wks_meta'].insert_one({'_id': 'space', 'compat_tag': 'legacy-space'})
    coll = client['wks_similarity']['file_embeddings']
    coll.insert_one({'path': 'file:///tmp/a.txt', 'timestamp': '2025-11-03T00:00:00Z'})
    monkeypatch.setattr('pymongo.MongoClient', lambda *a, **k: client)
    from wks.cli import main
    buf = io.StringIO()
    with redirect_stdout(buf):
        rc = main(['--display', 'json', 'db', 'info', '--space'])
    assert rc == 0
    data = json.loads(buf.getvalue())
    assert data.get('tracked_files') == 1 or data.get('total_docs') == 1
</file>

<file path="tests/test_config_validator.py">
"""Tests for config validation."""

import pytest
from pathlib import Path

from wks.config_validator import validate_config, validate_and_raise, ConfigValidationError


def test_valid_minimal_config(tmp_path):
    """Test that minimal valid config passes validation."""
    vault_path = tmp_path / "vault"
    vault_path.mkdir()

    include_path = tmp_path / "include"
    include_path.mkdir()

    cfg = {
        "vault": {
            "base_dir": str(vault_path),
            "wks_dir": "WKS",
        },
        "monitor": {
            "include_paths": [str(include_path)],
            "exclude_paths": [],
            "ignore_dirnames": [".git"],
            "ignore_globs": ["*.tmp"],
            "touch_weight": 0.2,
        },
    }

    errors = validate_config(cfg)
    assert errors == []


def test_missing_vault():
    """Test that missing vault section is caught."""
    cfg = {
        "monitor": {},
    }

    errors = validate_config(cfg)
    # Config validator may still check for vault_path for backward compat
    # or may check for vault.base_dir - both are acceptable
    assert any("vault" in e.lower() or "vault_path" in e.lower() for e in errors)


def test_nonexistent_vault_path():
    """Test that non-existent vault path is caught."""
    cfg = {
        "vault": {
            "base_dir": "/nonexistent/path/to/vault",
            "wks_dir": "WKS",
        },
        "monitor": {},
    }

    errors = validate_config(cfg)
    # May check vault.base_dir or vault_path (backward compat)
    assert any("does not exist" in e or "nonexistent" in e.lower() for e in errors)


def test_missing_obsidian_keys():
    """Test that missing obsidian keys are caught (backward compat)."""
    cfg = {
        "vault": {"base_dir": "~"},
        "obsidian": {},
        "monitor": {},
    }

    errors = validate_config(cfg)
    # May check obsidian.base_dir for backward compat, or may not require it if vault section exists
    # Just verify validation runs without crashing
    assert isinstance(errors, list)


def test_invalid_numeric_values():
    """Test that invalid numeric values are caught."""
    cfg = {
        "vault": {"base_dir": "~", "wks_dir": "WKS"},
        "obsidian": {
            "base_dir": "WKS",
            "log_max_entries": -1,  # Invalid: must be positive
            "active_files_max_rows": "not_a_number",  # Invalid: not an int
            "source_max_chars": 40,
            "destination_max_chars": 40,
        },
        "monitor": {
            "include_paths": [],
            "exclude_paths": [],
            "ignore_dirnames": [],
            "ignore_globs": [],
            "touch_weight": 0.2,
        },
    }

    errors = validate_config(cfg)
    # May check obsidian keys if obsidian section exists
    if any("obsidian" in e for e in errors):
        assert any("log_max_entries must be positive" in e or "log_max_entries" in e for e in errors)
        assert any("active_files_max_rows must be an integer" in e or "active_files_max_rows" in e for e in errors)


def test_touch_weight_validation(tmp_path):
    """monitor.touch_weight must be numeric and within range."""
    vault_path = tmp_path / "vault"
    vault_path.mkdir()

    include_path = tmp_path / "include"
    include_path.mkdir()

    cfg = {
        "vault": {
            "base_dir": str(vault_path),
            "wks_dir": "WKS",
        },
        "monitor": {
            "include_paths": [str(include_path)],
            "exclude_paths": [],
            "ignore_dirnames": [],
            "ignore_globs": [],
            "touch_weight": 0.0,
        },
    }

    errors = validate_config(cfg)
    assert any("touch_weight" in e for e in errors)

    cfg["monitor"]["touch_weight"] = "not-a-number"
    errors = validate_config(cfg)
    assert any("touch_weight" in e for e in errors)

    cfg["monitor"]["touch_weight"] = 0.0005
    errors = validate_config(cfg)
    assert any("touch_weight" in e for e in errors)

    cfg["monitor"]["touch_weight"] = 0.2
    errors = validate_config(cfg)
    assert not any("touch_weight" in e for e in errors)

def test_similarity_validation():
    """Test similarity config validation."""
    cfg = {
        "vault": {"base_dir": "~", "wks_dir": "WKS"},
        "monitor": {
            "include_paths": ["~"],
            "exclude_paths": [],
            "ignore_dirnames": [],
            "ignore_globs": [],
            "touch_weight": 0.2,
        },
        "similarity": {
            "enabled": True,
            # Missing: model, include_extensions
            "min_chars": 1000,
            "max_chars": 500,  # Invalid: min > max
        },
    }

    errors = validate_config(cfg)
    assert any("similarity.model is required" in e for e in errors)
    assert any("similarity.include_extensions is required" in e for e in errors)
    assert any("min_chars must be less than max_chars" in e for e in errors)


def test_validate_and_raise_success(tmp_path):
    """Test that validate_and_raise doesn't raise on valid config."""
    vault_path = tmp_path / "vault"
    vault_path.mkdir()

    cfg = {
        "vault": {
            "base_dir": str(vault_path),
            "wks_dir": "WKS",
        },
        "monitor": {
            "include_paths": [str(tmp_path)],
            "exclude_paths": [],
            "ignore_dirnames": [],
            "ignore_globs": [],
            "touch_weight": 0.2,
        },
    }

    # Should not raise
    validate_and_raise(cfg)


def test_validate_and_raise_failure():
    """Test that validate_and_raise raises on invalid config."""
    cfg = {
        "vault": {"base_dir": ""},  # Invalid: empty
        "monitor": {},  # Missing required keys
    }

    with pytest.raises(ConfigValidationError) as exc_info:
        validate_and_raise(cfg)

    # May check vault_path or vault.base_dir
    assert "vault" in str(exc_info.value).lower() or "vault_path" in str(exc_info.value).lower()
</file>

<file path="wks/cli/commands/config.py">
"""Config command implementation."""

import argparse
import json
from typing import Any, Dict, List

from ...config import get_config_path
from ...constants import MAX_DISPLAY_WIDTH
from ...display.context import get_display


def _format_config_value(value: Any) -> str:
    """Format a config value for display."""
    if isinstance(value, list):
        if value and isinstance(value[0], str):
            return ", ".join(value)
        return str(value)
    elif isinstance(value, dict):
        return str(value)
    elif isinstance(value, bool):
        return "true" if value else "false"
    else:
        return str(value)


def _add_config_section_items(table_data: List[Dict[str, str]], section_data: Dict[str, Any]) -> None:
    """Add items from a config section to table data."""
    for key, value in sorted(section_data.items()):
        if isinstance(value, dict):
            # Nested dict - add as subsection
            table_data.append({"Key": f"  {key}", "Value": ""})
            for subkey, subvalue in sorted(value.items()):
                if isinstance(subvalue, dict):
                    # Deeply nested - show as string
                    table_data.append({"Key": f"    {subkey}", "Value": _format_config_value(subvalue)})
                else:
                    table_data.append({"Key": f"    {subkey}", "Value": _format_config_value(subvalue)})
        else:
            table_data.append({"Key": f"  {key}", "Value": _format_config_value(value)})


def _build_config_table_data(config_data: Dict[str, Any]) -> List[Dict[str, str]]:
    """Build table data from config with sections.

    NOTE TO DEVELOPERS: This output is intentionally simple and straightforward.
    Do not add complexity, fancy formatting, or additional features. Keep it as-is.
    """
    table_data = []

    # Define section order and names (matching SPEC.md architecture)
    sections = [
        ("Monitor", "monitor"),
        ("Vault", "vault"),
        ("DB", "db"),
        ("Extract", "extract"),
        ("Diff", "diff"),
        ("Related", "related"),
        ("Index", "index"),
        ("Search", "search"),
        ("Display", "display"),
    ]

    for section_name, section_key in sections:
        if section_key in config_data:
            # Add section header
            table_data.append({"Key": section_name, "Value": ""})
            _add_config_section_items(table_data, config_data[section_key])

    # Add any remaining top-level keys not in our section list
    known_keys = {key for _, key in sections}
    remaining = {k: v for k, v in config_data.items() if k not in known_keys}
    if remaining:
        table_data.append({"Key": "Other", "Value": ""})
        for key, value in sorted(remaining.items()):
            table_data.append({"Key": f"  {key}", "Value": _format_config_value(value)})

    return table_data


def _style_config_table_data(table_data: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """Apply styling to config table data."""
    formatted_data = []
    for row in table_data:
        key = row["Key"]
        value = row["Value"]
        # Style section headers (empty value and not indented)
        if value == "" and not key.startswith("  "):
            formatted_data.append({"Key": f"[bold yellow]{key}[/bold yellow]", "Value": value})
        else:
            formatted_data.append({"Key": key, "Value": value})
    return formatted_data


def show_config(args: argparse.Namespace) -> int:
    """Show config file - table in CLI mode, JSON in MCP mode."""
    # Import from wks.cli to allow monkeypatching in tests
    from ...cli import load_config

    config_path = get_config_path()
    display = getattr(args, "display_obj", None) or get_display(getattr(args, "display", None))
    display_mode = getattr(args, "display", None)

    # Load config (allows monkeypatching in tests)
    try:
        config_data = load_config()
    except Exception as e:
        display.error(f"Failed to load config file: {config_path}", details=str(e))
        return 2

    # MCP mode: output raw JSON (not wrapped in MCP format)
    if display_mode == "mcp":
        # Output raw JSON for MCP mode (tests expect this)
        print(json.dumps(config_data, indent=2))
        return 0

    # CLI mode: show as table with sections
    table_data = _build_config_table_data(config_data)
    formatted_data = _style_config_table_data(table_data)

    display.table(
        formatted_data,
        title=f"WKS Configuration ({config_path})",
        column_justify={"Key": "left", "Value": "left"},
        show_header=False
    )

    return 0


def setup_config_parser(subparsers) -> None:
    """Setup config command parser."""
    cfg = subparsers.add_parser("config", help="Show configuration file")
    cfg.set_defaults(func=show_config)
</file>

<file path="wks/cli/commands/monitor.py">
"""Monitor commands - filesystem monitoring status and configuration (Monitor Layer)."""

import argparse
import json
import math
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

from ...config import get_config_path, load_config
from ...constants import MAX_DISPLAY_WIDTH
from ...monitor_controller import MonitorController, MonitorValidator
from ...utils import wks_home_path


# Helper functions for monitor status display
def path_matches_message(path: str, message: str) -> bool:
    """Check if path appears in message."""
    return f"'{path}'" in message or f" {path}" in message or message.endswith(path)


def extract_paths_from_messages(messages: List[str], all_paths: List[str]) -> Set[str]:
    """Extract paths that appear in messages."""
    matched = set()
    for message in messages:
        for path in all_paths:
            if path_matches_message(path, message):
                matched.add(path)
    return matched


def build_problematic_paths(issues: List[str], redundancies: List[str],
                            managed_dirs: Dict, include_paths: set, exclude_paths: set) -> Tuple[set, set]:
    """Build sets of problematic paths for coloring."""
    all_paths = list(managed_dirs.keys()) + list(include_paths) + list(exclude_paths)
    red_paths = extract_paths_from_messages(issues, all_paths)
    yellow_paths = extract_paths_from_messages(redundancies, all_paths)
    return red_paths, yellow_paths


def calculate_priority_alignment(managed_dirs_dict: Dict) -> Tuple[int, int]:
    """Calculate max pip count and number width for priority alignment."""
    max_pip_count = 0
    max_num_width = 0
    for path_info in managed_dirs_dict.values():
        priority = path_info.priority
        pip_count = 1 if priority <= 1 else int(math.log10(priority)) + 1
        max_pip_count = max(max_pip_count, pip_count)
        max_num_width = max(max_num_width, len(str(priority)))
    return max_pip_count, max_num_width


def style_setting_column(text: str) -> str:
    """Style setting column text (background is applied at column level)."""
    # Background is now applied at the column level, so just return the text
    return text


def build_managed_dirs_rows(managed_dirs_dict: Dict, max_pip_count: int, max_num_width: int) -> List[Dict]:
    """Build table rows for managed directories."""
    rows = []
    for path, path_info in sorted(managed_dirs_dict.items(), key=lambda x: -x[1].priority):
        priority = path_info.priority
        is_valid = path_info.valid
        error_msg = path_info.error

        pip_count = 1 if priority <= 1 else int(math.log10(priority)) + 1
        pips = "‚ñ™" * pip_count
        status_symbol = MonitorValidator.status_symbol(error_msg, is_valid)
        priority_display = f"{pips.ljust(max_pip_count)} {str(priority).rjust(max_num_width)} {status_symbol}"

        styled_path = style_setting_column(f"  {path}")
        rows.append({"Setting": styled_path, "Value": priority_display})
    return rows


def build_path_list_rows(paths: set, red_paths: set, yellow_paths: set, label: str) -> List[Dict]:
    """Build table rows for include/exclude paths."""
    styled_label = style_setting_column(f"[bold cyan]{label}[/bold cyan]")
    rows = [{"Setting": styled_label, "Value": str(len(paths))}]
    for path in sorted(paths):
        error_msg = None if path not in (red_paths | yellow_paths) else "issue"
        is_valid = path not in red_paths
        styled_path = style_setting_column(f"  {path}")
        rows.append({"Setting": styled_path, "Value": MonitorValidator.status_symbol(error_msg, is_valid)})
    return rows


def build_ignore_rules_rows(status_data) -> List[Dict[str, str]]:
    """Build table rows for ignore rules (ignore_dirnames and ignore_globs)."""
    rows = []
    ignore_dirnames = status_data.ignore_dirnames
    ignore_globs = status_data.ignore_globs

    styled_ignore_dirnames = style_setting_column("[bold cyan]ignore_dirnames[/bold cyan]")
    rows.append({
        "Ignore Rule": styled_ignore_dirnames,
        "Count": str(len(ignore_dirnames))
    })
    rows.append({"Ignore Rule": "", "Count": ""})

    for dirname in ignore_dirnames:
        validation_info = status_data.ignore_dirname_validation.get(dirname, {})
        error_msg = validation_info.get("error")
        is_valid = validation_info.get("valid", True)
        styled_dirname = style_setting_column(f"  {dirname}")
        rows.append({
            "Ignore Rule": styled_dirname,
            "Count": MonitorValidator.status_symbol(error_msg, is_valid)
        })

    rows.append({"Ignore Rule": "", "Count": ""})
    styled_ignore_globs = style_setting_column("[bold cyan]ignore_globs[/bold cyan]")
    rows.append({
        "Ignore Rule": styled_ignore_globs,
        "Count": str(len(ignore_globs))
    })

    for glob_pattern in ignore_globs:
        validation_info = status_data.ignore_glob_validation.get(glob_pattern, {})
        error_msg = validation_info.get("error")
        is_valid = validation_info.get("valid", True)
        styled_pattern = style_setting_column(f"  {glob_pattern}")
        rows.append({
            "Ignore Rule": styled_pattern,
            "Count": MonitorValidator.status_symbol(error_msg, is_valid)
        })

    return rows


def load_monitor_config() -> Dict[str, Any]:
    """Load monitor configuration."""
    return load_config()


def extract_monitor_status_data(status_data: Any) -> Tuple[int, List[str], List[str], Dict[str, int], Set[str], Set[str]]:
    """Extract data from monitor status.

    Returns:
        Tuple of (total_files, issues, redundancies, managed_dirs_dict, include_paths, exclude_paths)
    """
    total_files = status_data.tracked_files
    issues = status_data.issues
    redundancies = status_data.redundancies
    managed_dirs_dict = status_data.managed_directories
    include_paths = set(status_data.include_paths)
    exclude_paths = set(status_data.exclude_paths)
    return total_files, issues, redundancies, managed_dirs_dict, include_paths, exclude_paths


def get_last_touch_time(config: Dict[str, Any]) -> Optional[str]:
    """Get the most recent timestamp from the monitor database."""
    try:
        from ...config import mongo_settings
        from pymongo import MongoClient
        from ...monitor_controller import MonitorConfig

        monitor_cfg = MonitorConfig.from_config_dict(config)
        mongo_config = mongo_settings(config)

        client = MongoClient(mongo_config["uri"], serverSelectionTimeoutMS=5000)
        client.server_info()
        db_name, coll_name = monitor_cfg.database.split(".", 1)
        db = client[db_name]
        collection = db[coll_name]

        # Find the document with the most recent timestamp
        latest_doc = collection.find_one(
            {"timestamp": {"$exists": True}},
            sort=[("timestamp", -1)]
        )
        client.close()

        if latest_doc and latest_doc.get("timestamp"):
            # Parse ISO timestamp and format it nicely
            from datetime import datetime
            try:
                dt = datetime.fromisoformat(latest_doc["timestamp"])
                return dt.strftime("%Y-%m-%d %H:%M:%S")
            except (ValueError, TypeError):
                return latest_doc["timestamp"]
        return None
    except Exception:
        return None


def build_monitor_status_table_data(
    total_files: int,
    managed_dirs_dict: Dict[str, Any],  # Dict[str, ManagedDirectoryInfo]
    issues: List[str],
    redundancies: List[str],
    include_paths: Set[str],
    exclude_paths: Set[str],
    status_data: Any,
    config: Dict[str, Any]
) -> List[Tuple[str, str]]:
    """Build simple key/value pairs for monitor status display.

    Returns:
        List of (key, value) tuples for monitor status data, including files and last touch
    """
    # Color constants for consistent styling
    COLOR_FILES_KEY = "bold purple"
    COLOR_FILES_VALUE = "purple"
    COLOR_LAST_TOUCH_KEY = "bold green"
    COLOR_LAST_TOUCH_VALUE = "green"
    COLOR_HEADING = "bold cyan"
    COLOR_PATH_BG = "on gray30"

    # Get last touch time
    last_touch = get_last_touch_time(config) or "Never"

    rows = []
    # Add files and last touch at the top
    rows.append((f"[{COLOR_FILES_KEY}]files[/{COLOR_FILES_KEY}]", f"[{COLOR_FILES_VALUE}]{str(total_files)}[/{COLOR_FILES_VALUE}]"))
    rows.append((f"[{COLOR_LAST_TOUCH_KEY}]last touch[/{COLOR_LAST_TOUCH_KEY}]", f"[{COLOR_LAST_TOUCH_VALUE}]{last_touch}[/{COLOR_LAST_TOUCH_VALUE}]"))
    rows.append(("", ""))  # Empty row separator

    # Managed directories
    rows.append((f"[{COLOR_HEADING}]managed_directories[/{COLOR_HEADING}]", str(len(managed_dirs_dict))))
    red_paths, yellow_paths = build_problematic_paths(issues, redundancies, managed_dirs_dict, include_paths, exclude_paths)
    max_pip_count, max_num_width = calculate_priority_alignment(managed_dirs_dict)

    # Ensure we have minimum values to avoid division by zero
    max_pip_count = max(max_pip_count, 1)
    max_num_width = max(max_num_width, 1)

    for path, path_info in sorted(managed_dirs_dict.items(), key=lambda x: -x[1].priority):
        priority = path_info.priority
        is_valid = path_info.valid
        error_msg = path_info.error

        pip_count = 1 if priority <= 1 else int(math.log10(priority)) + 1
        pips = "‚ñ™" * pip_count
        status_symbol = MonitorValidator.status_symbol(error_msg, is_valid)
        priority_display = f"{pips.ljust(max_pip_count)} {str(priority).rjust(max_num_width)} {status_symbol}"
        rows.append((f"  [{COLOR_PATH_BG}]{path}[/{COLOR_PATH_BG}]", priority_display))

    rows.append(("", ""))

    # Include paths
    rows.append((f"[{COLOR_HEADING}]include_paths[/{COLOR_HEADING}]", str(len(include_paths))))
    for path in sorted(include_paths):
        error_msg = None if path not in (red_paths | yellow_paths) else "issue"
        is_valid = path not in red_paths
        rows.append((f"  [{COLOR_PATH_BG}]{path}[/{COLOR_PATH_BG}]", MonitorValidator.status_symbol(error_msg, is_valid)))

    rows.append(("", ""))

    # Exclude paths
    rows.append((f"[{COLOR_HEADING}]exclude_paths[/{COLOR_HEADING}]", str(len(exclude_paths))))
    for path in sorted(exclude_paths):
        error_msg = None if path not in (red_paths | yellow_paths) else "issue"
        is_valid = path not in red_paths
        rows.append((f"  [{COLOR_PATH_BG}]{path}[/{COLOR_PATH_BG}]", MonitorValidator.status_symbol(error_msg, is_valid)))

    rows.append(("", ""))

    # Ignore rules
    rows.append((f"[{COLOR_HEADING}]ignore_dirnames[/{COLOR_HEADING}]", str(len(status_data.ignore_dirnames))))
    for dirname in status_data.ignore_dirnames:
        validation_info = status_data.ignore_dirname_validation.get(dirname, {})
        error_msg = validation_info.get("error")
        is_valid = validation_info.get("valid", True)
        rows.append((f"  [{COLOR_PATH_BG}]{dirname}[/{COLOR_PATH_BG}]", MonitorValidator.status_symbol(error_msg, is_valid)))

    rows.append(("", ""))
    rows.append((f"[{COLOR_HEADING}]ignore_globs[/{COLOR_HEADING}]", str(len(status_data.ignore_globs))))
    for glob_pattern in status_data.ignore_globs:
        validation_info = status_data.ignore_glob_validation.get(glob_pattern, {})
        error_msg = validation_info.get("error")
        is_valid = validation_info.get("valid", True)
        rows.append((f"  [{COLOR_PATH_BG}]{glob_pattern}[/{COLOR_PATH_BG}]", MonitorValidator.status_symbol(error_msg, is_valid)))

    return rows


def display_monitor_status_table(
    display: Any,
    monitor_status_rows: List[Tuple[str, str]]
) -> None:
    """Display monitor status rows using the shared two-column table helper."""
    from ..helpers import display_status_table

    display_status_table(display, monitor_status_rows, title="Monitor Status")


def display_monitor_status_issues_panel(display: Any, issues: List[str], redundancies: List[str]) -> None:
    """Display issues and redundancies in a panel on STDOUT."""
    content_lines = []

    if issues:
        content_lines.append(f"[bold red]Inconsistencies ({len(issues)}):[/bold red]")
        for issue in issues:
            content_lines.append(f"  ‚Ä¢ {issue}")
        if redundancies:
            content_lines.append("")  # Blank line between sections

    if redundancies:
        content_lines.append(f"[bold yellow]Redundancies ({len(redundancies)}):[/bold yellow]")
        for redund in redundancies:
            content_lines.append(f"  ‚Ä¢ {redund}")

    if content_lines:
        content = "\n".join(content_lines)
        title = "Configuration Issues" if issues else "Configuration Warnings"
        border_style = "red" if issues else "yellow"
        display.panel(content, title=title, border_style=border_style, width=MAX_DISPLAY_WIDTH)


# Monitor command implementations
def monitor_status_cmd(args: argparse.Namespace) -> int:
    """Show monitoring statistics.

    Follows the 4-step process:
    1. Say what you're doing on STDERR
    2. Start progress bar on STDERR
    3. Say what you did and if there were problems on STDERR
    4. Display output on STDOUT
    """
    # Live mode requires CLI display
    live = getattr(args, "live", False)
    if live:
        args.display = "cli"
        from ...display.context import get_display
        args.display_obj = get_display("cli")
        return _monitor_status_live(args)

    # Step 1: Say what you're doing on STDERR (include config file path)
    config_file = get_config_path()
    print(f"Loading monitor configuration from {config_file}...", file=sys.stderr)

    # Step 2: Start progress bar on STDERR
    display = args.display_obj
    progress_handle = display.progress_start(total=3, description="Loading monitor status")

    try:
        # Load config (no informational output to STDOUT)
        cfg = load_monitor_config()
        display.progress_update(progress_handle, advance=1, description="Gathering monitor status")

        status_data = MonitorController.get_status(cfg)
        display.progress_update(progress_handle, advance=1, description="Building status display")

        total_files, issues, redundancies, managed_dirs_dict, include_paths, exclude_paths = extract_monitor_status_data(status_data)

        monitor_status_rows = build_monitor_status_table_data(
            total_files, managed_dirs_dict, issues, redundancies, include_paths, exclude_paths, status_data, cfg
        )

        display.progress_update(progress_handle, advance=1, description="Complete")
        display.progress_finish(progress_handle)

        # Step 3: Say what you did and if there were problems on STDERR
        problems = len(issues) + len(redundancies)
        if problems > 0:
            print(f"Monitor status loaded with {problems} issue(s) found", file=sys.stderr)
        else:
            print("Monitor status loaded successfully", file=sys.stderr)

        # Step 4: Display output on STDOUT (table and issues panel)
        display_monitor_status_table(display, monitor_status_rows)

        # Display issues/redundancies panel after the table
        if issues or redundancies:
            display_monitor_status_issues_panel(display, issues, redundancies)

    except Exception as e:
        display.progress_finish(progress_handle)
        import traceback
        print(f"Error loading monitor status: {e}", file=sys.stderr)
        print(traceback.format_exc(), file=sys.stderr)
        return 1

    return 0


def _monitor_status_live(args: argparse.Namespace) -> int:
    """Render live-updating monitor status display."""
    from rich.console import Console
    from rich.live import Live
    from rich.panel import Panel
    from ...constants import MAX_DISPLAY_WIDTH
    from ..helpers import display_status_table

    console = Console(width=MAX_DISPLAY_WIDTH)

    def _render_status() -> Panel:
        """Render current status as a Rich panel with unified table."""
        cfg = load_monitor_config()
        status_data = MonitorController.get_status(cfg)
        total_files, issues, redundancies, managed_dirs_dict, include_paths, exclude_paths = extract_monitor_status_data(status_data)

        monitor_status_rows = build_monitor_status_table_data(
            total_files, managed_dirs_dict, issues, redundancies, include_paths, exclude_paths, status_data, cfg
        )

        # Create a mock display object for the unified function
        class MockDisplay:
            def __init__(self, console):
                self.console = console

        mock_display = MockDisplay(console)

        # Build the panel manually since we need to return it
        from rich.table import Table
        from rich.columns import Columns

        key_width = 22
        value_width = 10
        row_tables = []
        for key, value in monitor_status_rows:
            row_table = Table(show_header=False, box=None, padding=(0, 1))
            row_table.add_column("Key", justify="left", width=key_width)
            row_table.add_column("Value", justify="right", width=value_width)
            row_table.add_row(key, value)
            row_tables.append(row_table)

        columns = Columns(row_tables, equal=True, column_first=True)
        return Panel.fit(columns, title="Monitor Status (Live)", border_style="cyan", width=MAX_DISPLAY_WIDTH)

    try:
        with Live(_render_status(), refresh_per_second=0.5, screen=False, console=console) as live:
            while True:
                time.sleep(2.0)
                try:
                    live.update(_render_status())
                except Exception as update_exc:
                    console.print(f"[yellow]Warning: {update_exc}[/yellow]", end="")
    except KeyboardInterrupt:
        console.print("\n[dim]Stopped monitoring.[/dim]")
        return 0
    except Exception as exc:
        console.print(f"[red]Error in live mode: {exc}[/red]")
        return 2


# Validation helpers
def check_path_conflicts(include_paths: Set[str], exclude_paths: Set[str]) -> List[str]:
    """Check for paths in both include and exclude."""
    issues = []
    conflicts = include_paths & exclude_paths
    for path in conflicts:
        issues.append(f"Path in both include and exclude: {path}")
    return issues


def check_duplicate_managed_dirs(managed_dirs: Set[str]) -> List[str]:
    """Check for duplicate managed directories with same resolved path."""
    warnings = []
    managed_list = list(managed_dirs)
    for i, dir1 in enumerate(managed_list):
        p1 = Path(dir1).expanduser().resolve()
        for dir2 in managed_list[i + 1:]:
            p2 = Path(dir2).expanduser().resolve()
            try:
                if p1 == p2:
                    warnings.append(f"Duplicate managed directories: {dir1} and {dir2} resolve to same path")
            except BaseException:
                pass
    return warnings


def display_validation_results(display: Any, issues: List[str], warnings: List[str]) -> int:
    """Display validation results and return exit code."""
    if not issues and not warnings:
        display.success("No configuration issues found")
        return 0

    if issues:
        display.error(f"Found {len(issues)} error(s):")
        for issue in issues:
            display.error(f"  ‚Ä¢ {issue}")

    if warnings:
        display.warning(f"Found {len(warnings)} warning(s):")
        for warning in warnings:
            display.warning(f"  ‚Ä¢ {warning}")

    return 1 if issues else 0


def monitor_validate_cmd(args: argparse.Namespace) -> int:
    """Check for configuration inconsistencies."""
    display = args.display_obj
    cfg = load_config()
    monitor_config = cfg.get("monitor", {})

    include_paths = set(monitor_config.get("include_paths", []))
    exclude_paths = set(monitor_config.get("exclude_paths", []))
    managed_dirs = set(monitor_config.get("managed_directories", {}).keys())

    issues = check_path_conflicts(include_paths, exclude_paths)
    warnings = check_duplicate_managed_dirs(managed_dirs)

    return display_validation_results(display, issues, warnings)


def monitor_check_cmd(args: argparse.Namespace) -> int:
    """Check if a path would be monitored."""
    display = args.display_obj
    cfg = load_config()

    # Use MonitorController to check the path
    result = MonitorController.check_path(cfg, args.path)

    # Display results
    if result["is_monitored"]:
        display.success(f"Path WOULD be monitored: {result['path']}")
        if result.get("priority"):
            display.info(f"Priority: {result['priority']}")
    else:
        display.error(f"Path would NOT be monitored: {result['path']}")
        if result.get("reason"):
            display.error(f"Reason: {result['reason']}")

    # Show decision chain
    display.info("\nDecision chain:")
    for decision in result.get("decisions", []):
        symbol = decision.get("symbol", "‚Ñπ")
        message = decision.get("message", "")
        if symbol == "‚úì":
            display.success(f"  {message}")
        elif symbol == "‚úó":
            display.error(f"  {message}")
        elif symbol == "‚ö†":
            display.warning(f"  {message}")
        else:
            display.info(f"  {message}")

    return 0 if result["is_monitored"] else 1


# List management helpers
def normalize_path_for_list(value: str, resolve_path: bool) -> Tuple[str, str]:
    """Normalize path for list operations.

    Returns:
        Tuple of (value_resolved, value_to_store)
    """
    if resolve_path:
        value_resolved = str(Path(value).expanduser().resolve())
        home_dir = str(Path.home())
        if value_resolved.startswith(home_dir):
            value_to_store = "~" + value_resolved[len(home_dir):]
        else:
            value_to_store = value_resolved
        return value_resolved, value_to_store
    return value, value


def find_existing_entry_in_list(
    cfg: Dict[str, Any],
    list_name: str,
    value: str,
    value_resolved: str,
    resolve_path: bool
) -> Optional[str]:
    """Find existing entry in list by comparing resolved paths."""
    if resolve_path:
        for entry in cfg["monitor"][list_name]:
            entry_resolved = str(Path(entry).expanduser().resolve())
            if entry_resolved == value_resolved:
                return entry
        return None
    return value if value in cfg["monitor"][list_name] else None


def validate_before_add(
    cfg: Dict[str, Any],
    list_name: str,
    value: str,
    value_resolved: str,
    resolve_path: bool,
    display: Any
) -> int:
    """Validate before adding to list. Returns 0 if valid, 1 if invalid."""
    if list_name == "ignore_dirnames":
        ignore_globs = cfg["monitor"].get("ignore_globs", [])
        is_valid, error_msg = MonitorValidator.validate_ignore_dirname(
            value_resolved if not resolve_path else value,
            ignore_globs
        )
        if not is_valid:
            display.error(error_msg)
            return 1
    return 0


def perform_list_add(
    cfg: Dict[str, Any],
    list_name: str,
    value_to_store: str,
    existing_entry: Optional[str],
    display: Any
) -> int:
    """Perform add operation. Returns exit code."""
    if existing_entry:
        display.warning(f"Already in {list_name}: {existing_entry}")
        return 0
    cfg["monitor"][list_name].append(value_to_store)
    display.success(f"Added to {list_name}: {value_to_store}")
    return 0


def perform_list_remove(
    cfg: Dict[str, Any],
    list_name: str,
    value: str,
    existing_entry: Optional[str],
    display: Any
) -> int:
    """Perform remove operation. Returns exit code."""
    if not existing_entry:
        display.warning(f"Not in {list_name}: {value}")
        return 0
    cfg["monitor"][list_name].remove(existing_entry)
    display.success(f"Removed from {list_name}: {existing_entry}")
    return 0


def modify_monitor_list(display, list_name: str, value: str, operation: str, resolve_path: bool = True) -> int:
    """Modify a monitor config list (add/remove)."""
    config_path = get_config_path()

    if not config_path.exists():
        display.error(f"Config file not found: {config_path}")
        return 2

    # Read current config
    with open(config_path) as f:
        cfg = json.load(f)

    # Get monitor section
    if "monitor" not in cfg:
        cfg["monitor"] = {}

    if list_name not in cfg["monitor"]:
        cfg["monitor"][list_name] = []

    # Normalize path for comparison if needed
    value_resolved, value_to_store = normalize_path_for_list(value, resolve_path)
    existing_entry = find_existing_entry_in_list(cfg, list_name, value, value_resolved, resolve_path)

    # Perform operation
    if operation == "add":
        exit_code = validate_before_add(cfg, list_name, value, value_resolved, resolve_path, display)
        if exit_code != 0:
            return exit_code
        exit_code = perform_list_add(cfg, list_name, value_to_store, existing_entry, display)
        if exit_code != 0:
            return exit_code
    elif operation == "remove":
        exit_code = perform_list_remove(cfg, list_name, value, existing_entry, display)
        if exit_code != 0:
            return exit_code

    # Write back
    with open(config_path, "w") as f:
        json.dump(cfg, f, indent=4)

    display.info("Restart the monitor service for changes to take effect")
    return 0


def show_monitor_list(display, list_name: str, title: str) -> int:
    """Show contents of a monitor config list with validation status."""
    cfg = load_config()
    monitor_config = cfg.get("monitor", {})
    items = monitor_config.get(list_name, [])

    if not items:
        display.info(f"No {list_name} configured")
        return 0

    # Get other config items for validation
    ignore_globs = monitor_config.get("ignore_globs", [])
    ignore_dirnames = monitor_config.get("ignore_dirnames", [])
    include_paths = monitor_config.get("include_paths", [])
    exclude_paths = monitor_config.get("exclude_paths", [])
    managed_dirs = monitor_config.get("managed_directories", {})

    table_data = []
    for i, item in enumerate(items, 1):
        is_valid, error_msg = True, None

        if list_name == "ignore_dirnames":
            is_valid, error_msg = MonitorValidator.validate_ignore_dirname(item, ignore_globs)
        elif list_name == "ignore_globs":
            is_valid, error_msg = MonitorValidator.validate_ignore_glob(item)
        elif list_name in ("include_paths", "exclude_paths"):
            try:
                path_obj = Path(item).expanduser().resolve()
                if not path_obj.exists():
                    is_valid = list_name == "exclude_paths"  # Warning for exclude, error for include
                    error_msg = "Path does not exist" + (" (will be ignored if created)" if is_valid else "")
                elif not path_obj.is_dir():
                    is_valid, error_msg = False, "Not a directory"
            except Exception as e:
                is_valid, error_msg = False, f"Invalid path: {e}"

        table_data.append({"#": str(i), "Value": item, "Status": MonitorValidator.status_symbol(error_msg, is_valid)})

    display.table(table_data, title=title)
    return 0


# Command handlers for list operations
def monitor_include_default(args: argparse.Namespace) -> int:
    """Default action for include_paths: show list."""
    if not args.include_paths_op:
        return show_monitor_list(args.display_obj, "include_paths", "Include Paths")
    return 0


def monitor_include_add(args: argparse.Namespace) -> int:
    """Add path(s) to include_paths."""
    for path in args.paths:
        result = modify_monitor_list(args.display_obj, "include_paths", path, "add", resolve_path=True)
        if result != 0:
            return result
    return 0


def monitor_include_remove(args: argparse.Namespace) -> int:
    """Remove path(s) from include_paths."""
    for path in args.paths:
        result = modify_monitor_list(args.display_obj, "include_paths", path, "remove", resolve_path=True)
        if result != 0:
            return result
    return 0


def monitor_exclude_default(args: argparse.Namespace) -> int:
    """Default action for exclude_paths: show list."""
    if not args.exclude_paths_op:
        return show_monitor_list(args.display_obj, "exclude_paths", "Exclude Paths")
    return 0


def monitor_exclude_add(args: argparse.Namespace) -> int:
    """Add path(s) to exclude_paths."""
    for path in args.paths:
        result = modify_monitor_list(args.display_obj, "exclude_paths", path, "add", resolve_path=True)
        if result != 0:
            return result
    return 0


def monitor_exclude_remove(args: argparse.Namespace) -> int:
    """Remove path(s) from exclude_paths."""
    for path in args.paths:
        result = modify_monitor_list(args.display_obj, "exclude_paths", path, "remove", resolve_path=True)
        if result != 0:
            return result
    return 0


def monitor_ignore_dir_default(args: argparse.Namespace) -> int:
    """Default action for ignore_dirnames: show list."""
    if not args.ignore_dirnames_op:
        return show_monitor_list(args.display_obj, "ignore_dirnames", "Ignore Directory Names")
    return 0


def monitor_ignore_dir_add(args: argparse.Namespace) -> int:
    """Add directory name(s) to ignore_dirnames."""
    for dirname in args.dirnames:
        result = modify_monitor_list(args.display_obj, "ignore_dirnames", dirname, "add", resolve_path=False)
        if result != 0:
            return result
    return 0


def monitor_ignore_dir_remove(args: argparse.Namespace) -> int:
    """Remove directory name(s) from ignore_dirnames."""
    for dirname in args.dirnames:
        result = modify_monitor_list(args.display_obj, "ignore_dirnames", dirname, "remove", resolve_path=False)
        if result != 0:
            return result
    return 0


def monitor_ignore_glob_default(args: argparse.Namespace) -> int:
    """Default action for ignore_globs: show list."""
    if not args.ignore_globs_op:
        return show_monitor_list(args.display_obj, "ignore_globs", "Ignore Glob Patterns")
    return 0


def monitor_ignore_glob_add(args: argparse.Namespace) -> int:
    """Add glob pattern(s) to ignore_globs."""
    for pattern in args.patterns:
        result = modify_monitor_list(args.display_obj, "ignore_globs", pattern, "add", resolve_path=False)
        if result != 0:
            return result
    return 0


def monitor_ignore_glob_remove(args: argparse.Namespace) -> int:
    """Remove glob pattern(s) from ignore_globs."""
    for pattern in args.patterns:
        result = modify_monitor_list(args.display_obj, "ignore_globs", pattern, "remove", resolve_path=False)
        if result != 0:
            return result
    return 0


def monitor_managed_default(args: argparse.Namespace) -> int:
    """Default action for managed: show list."""
    if not args.managed_op:
        cfg = load_config()
        monitor_config = cfg.get("monitor", {})
        managed_dirs = monitor_config.get("managed_directories", {})

        if not managed_dirs:
            args.display_obj.info("No managed_directories configured")
            return 0

        table_data = []
        for path, priority in sorted(managed_dirs.items(), key=lambda x: -x[1]):
            table_data.append({"Path": path, "Priority": str(priority)})

        args.display_obj.table(table_data, title="Managed Directories")
        return 0
    return 0


def monitor_managed_add(args: argparse.Namespace) -> int:
    """Add managed directory with priority."""
    config_path = get_config_path()

    if not config_path.exists():
        args.display_obj.error(f"Config file not found: {config_path}")
        return 2

    path = str(Path(args.path).expanduser().resolve())

    with open(config_path) as f:
        cfg = json.load(f)

    if "monitor" not in cfg:
        cfg["monitor"] = {}
    if "managed_directories" not in cfg["monitor"]:
        cfg["monitor"]["managed_directories"] = {}

    cfg["monitor"]["managed_directories"][path] = args.priority

    with open(config_path, "w") as f:
        json.dump(cfg, f, indent=4)

    args.display_obj.success(f"Added managed directory: {path} (priority {args.priority})")
    args.display_obj.info("Restart the monitor service for changes to take effect")
    return 0


def monitor_managed_remove(args: argparse.Namespace) -> int:
    """Remove managed directory."""
    config_path = get_config_path()

    if not config_path.exists():
        args.display_obj.error(f"Config file not found: {config_path}")
        return 2

    path = str(Path(args.path).expanduser().resolve())

    with open(config_path) as f:
        cfg = json.load(f)

    if "monitor" not in cfg or "managed_directories" not in cfg["monitor"]:
        args.display_obj.warning("No managed_directories configured")
        return 0

    if path not in cfg["monitor"]["managed_directories"]:
        args.display_obj.warning(f"Not a managed directory: {path}")
        return 0

    del cfg["monitor"]["managed_directories"][path]

    with open(config_path, "w") as f:
        json.dump(cfg, f, indent=4)

    args.display_obj.success(f"Removed managed directory: {path}")
    args.display_obj.info("Restart the monitor service for changes to take effect")
    return 0


def monitor_managed_priority(args: argparse.Namespace) -> int:
    """Set priority for managed directory."""
    config_path = get_config_path()

    if not config_path.exists():
        args.display_obj.error(f"Config file not found: {config_path}")
        return 2

    path = str(Path(args.path).expanduser().resolve())

    with open(config_path) as f:
        cfg = json.load(f)

    if "monitor" not in cfg or "managed_directories" not in cfg["monitor"]:
        args.display_obj.error("No managed_directories configured")
        return 2

    if path not in cfg["monitor"]["managed_directories"]:
        args.display_obj.error(f"Not a managed directory: {path}")
        return 2

    old_priority = cfg["monitor"]["managed_directories"][path]
    cfg["monitor"]["managed_directories"][path] = args.priority

    with open(config_path, "w") as f:
        json.dump(cfg, f, indent=4)

    args.display_obj.success(f"Updated priority: {path} ({old_priority} ‚Üí {args.priority})")
    args.display_obj.info("Restart the monitor service for changes to take effect")
    return 0


def setup_monitor_parser(subparsers) -> None:
    """Setup monitor command parser."""
    mon = subparsers.add_parser("monitor", help="Filesystem monitoring status and configuration")
    monsub = mon.add_subparsers(dest="monitor_cmd", required=False)

    # monitor status
    monstatus = monsub.add_parser("status", help="Show monitoring statistics")
    monstatus.add_argument(
        "--live",
        action="store_true",
        help="Keep display updated automatically (refreshes every 2 seconds)"
    )
    monstatus.set_defaults(func=monitor_status_cmd)

    # monitor validate
    monvalidate = monsub.add_parser("validate", help="Check for configuration inconsistencies")
    monvalidate.set_defaults(func=monitor_validate_cmd)

    # monitor check
    moncheck = monsub.add_parser("check", help="Check if a path would be monitored")
    moncheck.add_argument("path", help="Path to check")
    moncheck.set_defaults(func=monitor_check_cmd)

    # monitor include_paths
    mon_include = monsub.add_parser("include_paths", help="Manage include_paths")
    mon_include_sub = mon_include.add_subparsers(dest="include_paths_op", required=False)
    mon_include.set_defaults(func=monitor_include_default)
    mon_include_add = mon_include_sub.add_parser("add", help="Add path(s) to include_paths")
    mon_include_add.add_argument("paths", nargs='+', help="Path(s) to monitor")
    mon_include_add.set_defaults(func=monitor_include_add)
    mon_include_remove = mon_include_sub.add_parser("remove", help="Remove path(s) from include_paths")
    mon_include_remove.add_argument("paths", nargs='+', help="Path(s) to remove")
    mon_include_remove.set_defaults(func=monitor_include_remove)

    # monitor exclude_paths
    mon_exclude = monsub.add_parser("exclude_paths", help="Manage exclude_paths")
    mon_exclude_sub = mon_exclude.add_subparsers(dest="exclude_paths_op", required=False)
    mon_exclude.set_defaults(func=monitor_exclude_default)
    mon_exclude_add = mon_exclude_sub.add_parser("add", help="Add path(s) to exclude_paths")
    mon_exclude_add.add_argument("paths", nargs='+', help="Path(s) to exclude")
    mon_exclude_add.set_defaults(func=monitor_exclude_add)
    mon_exclude_remove = mon_exclude_sub.add_parser("remove", help="Remove path(s) from exclude_paths")
    mon_exclude_remove.add_argument("paths", nargs='+', help="Path(s) to remove")
    mon_exclude_remove.set_defaults(func=monitor_exclude_remove)

    # monitor ignore_dirnames
    mon_ignore_dir = monsub.add_parser("ignore_dirnames", help="Manage ignore_dirnames")
    mon_ignore_dir_sub = mon_ignore_dir.add_subparsers(dest="ignore_dirnames_op", required=False)
    mon_ignore_dir.set_defaults(func=monitor_ignore_dir_default)
    mon_ignore_dir_add = mon_ignore_dir_sub.add_parser("add", help="Add directory name(s) to ignore_dirnames")
    mon_ignore_dir_add.add_argument("dirnames", nargs='+', help="Directory name(s) to ignore (e.g., node_modules)")
    mon_ignore_dir_add.set_defaults(func=monitor_ignore_dir_add)
    mon_ignore_dir_remove = mon_ignore_dir_sub.add_parser("remove", help="Remove directory name(s) from ignore_dirnames")
    mon_ignore_dir_remove.add_argument("dirnames", nargs='+', help="Directory name(s) to remove")
    mon_ignore_dir_remove.set_defaults(func=monitor_ignore_dir_remove)

    # monitor ignore_globs
    mon_ignore_glob = monsub.add_parser("ignore_globs", help="Manage ignore_globs")
    mon_ignore_glob_sub = mon_ignore_glob.add_subparsers(dest="ignore_globs_op", required=False)
    mon_ignore_glob.set_defaults(func=monitor_ignore_glob_default)
    mon_ignore_glob_add = mon_ignore_glob_sub.add_parser("add", help="Add glob pattern(s) to ignore_globs")
    mon_ignore_glob_add.add_argument("patterns", nargs='+', help="Glob pattern(s) to ignore (e.g., *.tmp)")
    mon_ignore_glob_add.set_defaults(func=monitor_ignore_glob_add)
    mon_ignore_glob_remove = mon_ignore_glob_sub.add_parser("remove", help="Remove glob pattern(s) from ignore_globs")
    mon_ignore_glob_remove.add_argument("patterns", nargs='+', help="Pattern(s) to remove")
    mon_ignore_glob_remove.set_defaults(func=monitor_ignore_glob_remove)

    # monitor managed
    mon_managed = monsub.add_parser("managed", help="Manage managed_directories with priorities")
    mon_managed_sub = mon_managed.add_subparsers(dest="managed_op", required=False)
    mon_managed.set_defaults(func=monitor_managed_default)
    mon_managed_add = mon_managed_sub.add_parser("add", help="Add managed directory with priority")
    mon_managed_add.add_argument("path", help="Directory path")
    mon_managed_add.add_argument("--priority", type=int, required=True, help="Priority score (e.g., 100)")
    mon_managed_add.set_defaults(func=monitor_managed_add)
    mon_managed_remove = mon_managed_sub.add_parser("remove", help="Remove managed directory")
    mon_managed_remove.add_argument("path", help="Directory path to remove")
    mon_managed_remove.set_defaults(func=monitor_managed_remove)
    mon_managed_priority = mon_managed_sub.add_parser("set-priority", help="Set priority for managed directory")
    mon_managed_priority.add_argument("path", help="Directory path")
    mon_managed_priority.add_argument("priority", type=int, help="New priority score")
    mon_managed_priority.set_defaults(func=monitor_managed_priority)
</file>

<file path="wks/cli/helpers.py">
"""Shared helper functions for CLI commands."""

import hashlib
import json
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from ..constants import WKS_EXTRACT_EXT
from ..extractor import Extractor
from ..utils import file_checksum as _file_checksum_util


def maybe_write_json(args: Any, payload: Dict[str, Any]) -> None:
    """Write JSON output if --json flag is set."""
    if getattr(args, "json", False):
        json.dump(payload, sys.stdout, indent=2)
        sys.stdout.write("\n")
        sys.exit(0)


def as_file_uri_local(path: Path) -> str:
    """Convert path to file:// URI."""
    try:
        return path.expanduser().resolve().as_uri()
    except ValueError:
        return "file://" + path.expanduser().resolve().as_posix()


def build_extractor(cfg: Dict[str, Any]) -> Extractor:
    """Build extractor from config."""
    ext = cfg.get("extract") or {}
    sim = cfg.get("related", {}).get("engines", {}).get("embedding", {}) or cfg.get("similarity") or {}
    return Extractor(
        engine=ext.get("engine", "docling"),
        ocr=bool(ext.get("ocr", False)),
        timeout_secs=int(ext.get("timeout_secs", 30)),
        options=dict(ext.get("options") or {}),
        max_chars=int(sim.get("max_chars", 200000)),
        write_extension=ext.get("write_extension"),
    )


def file_checksum(path: Path) -> str:
    """Calculate file checksum."""
    return _file_checksum_util(path)


def iter_files(paths: List[str], include_exts: Optional[List[str]], cfg: Dict[str, Any]) -> List[Path]:
    """Iterate over files matching criteria."""
    from ..monitor_controller import MonitorController

    def _should_skip(p: Path) -> bool:
        """Check if path should be skipped based on monitor rules."""
        try:
            result = MonitorController.check_path(str(p), cfg)
            return result.tracked is False
        except Exception:
            # If check fails, don't skip
            return False

    out: List[Path] = []
    for p in paths:
        pp = Path(p).expanduser()
        if not pp.exists():
            continue
        if _should_skip(pp):
            continue
        if pp.is_file():
            if not include_exts or pp.suffix.lower() in include_exts:
                out.append(pp)
        else:
            for x in pp.rglob('*'):
                if not x.is_file():
                    continue
                if _should_skip(x):
                    continue
                if include_exts and x.suffix.lower() not in include_exts:
                    continue
                out.append(x)
    return out


def make_progress(total: int, display: str = "cli"):
    """Create progress bar context manager."""
    from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn

    if display == "cli":
        class ProgressDriver:
            def __init__(self, total_steps: int):
                self._progress = Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    BarColumn(),
                    TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                    console=None,
                )
                self._task_id = None
                self._total = max(total_steps, 1)
                self._started = False
                self._completed = 0

            def __enter__(self):
                self._progress.__enter__()
                self._task_id = self._progress.add_task("", total=self._total)
                return self

            def __exit__(self, exc_type, exc, tb):
                if self._task_id is not None and self._started:
                    remaining = self._total - self._completed
                    if remaining > 0:
                        self._progress.update(self._task_id, advance=remaining)
                self._progress.__exit__(exc_type, exc, tb)

            def update(self, description: str):
                if self._task_id is None:
                    return
                if not self._started:
                    self._progress.update(self._task_id, description=description)
                    self._started = True
                    return
                self._completed = min(self._completed + 1, self._total)
                self._progress.update(self._task_id, advance=1, description=description)

        return ProgressDriver(total)

    # No-op progress for non-CLI modes
    class NoOpProgress:
        def __enter__(self):
            return self

        def __exit__(self, *args):
            pass

        def update(self, *args, **kwargs):
            pass

    return NoOpProgress()


def display_status_table(
    display: Any,
    status_rows: List[Tuple[str, str]],
    title: str = "Status",
    *,
    key_width: Optional[int] = None,
    value_width: Optional[int] = None,
) -> None:
    """Display a status table using reflowing two-column layout."""
    from rich.table import Table
    from rich.panel import Panel
    from rich.columns import Columns
    from ..constants import MAX_DISPLAY_WIDTH

    if key_width is None:
        key_width = max(16, min(30, MAX_DISPLAY_WIDTH // 5))
    if value_width is None:
        value_width = max(10, min(20, MAX_DISPLAY_WIDTH // 8))

    row_tables = []
    for key, value in status_rows:
        row_table = Table(show_header=False, box=None, padding=(0, 1))
        row_table.add_column("Key", justify="left", width=key_width, no_wrap=True)
        row_table.add_column("Value", justify="right", width=value_width)
        row_table.add_row(key, value)
        row_tables.append(row_table)

    columns = Columns(row_tables, equal=True, column_first=True, expand=True)
    panel = Panel.fit(columns, title=title, border_style="cyan", width=MAX_DISPLAY_WIDTH)
    display.console.print(panel)
</file>

<file path="wks/activity.py">
"""
File activity tracking with "angle" metric.

The angle represents the rate of change/attention on a file.
Higher angle = more active/important recently.
"""

import json
import math
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple

from .constants import WKS_HOME_EXT


class ActivityTracker:
    """Track file activity and calculate attention angles."""

    def __init__(self, state_file: Path):
        """
        Initialize activity tracker.

        Args:
            state_file: Path to JSON state file
        """
        self.state_file = Path(state_file)
        self.state = self._load_state()

    def _load_state(self) -> Dict:
        """Load state from JSON, backing up if corrupted."""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r') as f:
                    return json.load(f)
            except Exception:
                # Corrupted state; back up and start fresh
                try:
                    b = self.state_file.with_suffix('.json.backup')
                    self.state_file.rename(b)
                    print(f"Warning: Corrupted activity state backed up to {b}")
                except Exception:
                    pass
        return {}

    def _save_state(self):
        """Save state to JSON."""
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=2)

    def record_event(self, file_path: Path, event_type: str = "modified"):
        """
        Record a file event.

        Args:
            file_path: Path to file
            event_type: Type of event
        """
        path_str = str(file_path.resolve())
        now = datetime.now().isoformat()

        if path_str not in self.state:
            self.state[path_str] = {
                "events": [],
                "angles": [],
                "created": now
            }

        self.state[path_str]["events"].append({
            "type": event_type,
            "timestamp": now
        })

        # Keep last 50 events
        if len(self.state[path_str]["events"]) > 50:
            self.state[path_str]["events"] = self.state[path_str]["events"][-50:]

        self._calculate_angle(path_str)
        self._save_state()

    def _calculate_angle(self, path_str: str):
        """
        Calculate the 'angle' for a file based on recent activity.

        The angle represents attention/change velocity.
        Higher values = more recent/frequent changes.

        Algorithm:
        - Weight recent events higher than old events
        - Exponential decay over time
        """
        events = self.state[path_str]["events"]
        if not events:
            angle = 0.0
        else:
            now = datetime.now()
            angle = 0.0

            for event in events:
                event_time = datetime.fromisoformat(event["timestamp"])
                hours_ago = (now - event_time).total_seconds() / 3600

                # Exponential decay: events lose weight over time
                # Half-life of 24 hours
                weight = math.exp(-hours_ago / 24)
                angle += weight

        self.state[path_str]["angles"].append({
            "value": angle,
            "timestamp": datetime.now().isoformat()
        })

        # Keep last 100 angle measurements
        if len(self.state[path_str]["angles"]) > 100:
            self.state[path_str]["angles"] = self.state[path_str]["angles"][-100:]

    def _compute_angle_now(self, path_str: str) -> float:
        """Compute angle for a path at the current moment without recording an event."""
        data = self.state.get(path_str) or {}
        events = data.get("events") or []
        if not events:
            return 0.0
        now = datetime.now()
        import math as _math
        angle = 0.0
        for ev in events:
            try:
                et = datetime.fromisoformat(ev.get("timestamp"))
            except Exception:
                continue
            hours_ago = (now - et).total_seconds() / 3600.0
            weight = _math.exp(-hours_ago / 24.0)
            angle += weight
        return float(angle)

    def refresh_angles_all(self):
        """Append a current angle snapshot for all tracked files to enable positive/negative slopes.

        Keeps last 100 samples; saves state.
        """
        changed = False
        now_iso = datetime.now().isoformat()
        for path_str, data in list(self.state.items()):
            try:
                angle = self._compute_angle_now(path_str)
                arr = data.setdefault("angles", [])
                arr.append({"value": angle, "timestamp": now_iso})
                if len(arr) > 100:
                    data["angles"] = arr[-100:]
                changed = True
            except Exception:
                continue
        if changed:
            self._save_state()

    def get_angle(self, file_path: Path) -> float:
        """
        Get current angle for a file.

        Args:
            file_path: Path to file

        Returns:
            Current angle value
        """
        path_str = str(file_path.resolve())
        if path_str in self.state and self.state[path_str]["angles"]:
            return self.state[path_str]["angles"][-1]["value"]
        return 0.0

    def get_angle_delta(self, file_path: Path) -> float:
        """
        Get change in angle (velocity of attention change).

        Args:
            file_path: Path to file

        Returns:
            Delta angle (current - previous)
        """
        path_str = str(file_path.resolve())
        if path_str in self.state and len(self.state[path_str]["angles"]) >= 2:
            current = self.state[path_str]["angles"][-1]["value"]
            previous = self.state[path_str]["angles"][-2]["value"]
            return current - previous
        return 0.0

    def get_top_active_files(self, limit: int = 20) -> List[Tuple[str, float, float]]:
        """
        Get most active files by angle.

        Args:
            limit: Number of files to return

        Returns:
            List of (path, angle, delta_angle) tuples, sorted by angle descending
        """
        files = []

        for path_str, data in self.state.items():
            if data["angles"]:
                angle = data["angles"][-1]["value"]
                delta = 0.0
                if len(data["angles"]) >= 2:
                    delta = angle - data["angles"][-2]["value"]

                # Only include files with significant angle
                if angle > 0.1:
                    files.append((path_str, angle, delta))

        # Sort by angle descending
        files.sort(key=lambda x: x[1], reverse=True)

        return files[:limit]

    def get_last_modified(self, file_path: Path) -> str:
        """
        Get last modification time for a file.

        Args:
            file_path: Path to file

        Returns:
            ISO timestamp of last event
        """
        path_str = str(file_path.resolve())
        if path_str in self.state and self.state[path_str]["events"]:
            return self.state[path_str]["events"][-1]["timestamp"]
        return "Never"

    def get_angle_rate_per_minute(self, file_path: Path, window_minutes: int = 60) -> float:
        """Return angle change per minute over the last window (approx).

        Uses stored angle samples; if not enough data in the window, falls back
        to using the earliest available sample vs latest. Returns 0.0 on error.
        """
        try:
            path_str = str(file_path.resolve())
            series = (self.state.get(path_str) or {}).get("angles") or []
            if len(series) < 2:
                return 0.0
            from datetime import datetime, timedelta
            now = datetime.now()
            cutoff = now - timedelta(minutes=max(1, int(window_minutes)))
            # Find first sample within window; if none, use first overall
            first_idx = None
            for i, s in enumerate(series):
                try:
                    ts = datetime.fromisoformat(s.get("timestamp"))
                except Exception:
                    continue
                if ts >= cutoff:
                    first_idx = i
                    break
            if first_idx is None:
                first_idx = 0
            first = series[first_idx]
            last = series[-1]
            try:
                t0 = datetime.fromisoformat(first.get("timestamp"))
                t1 = datetime.fromisoformat(last.get("timestamp"))
            except Exception:
                return 0.0
            minutes = max(1.0, (t1 - t0).total_seconds() / 60.0)
            return float((last.get("value", 0.0) - first.get("value", 0.0)) / minutes)
        except Exception:
            return 0.0


if __name__ == "__main__":
    from rich.console import Console
    from rich.table import Table

    console = Console()

    # Example usage
    tracker = ActivityTracker(Path.home() / WKS_HOME_EXT / "activity_state.json")

    # Simulate some activity
    test_file = Path.home() / "2025-WKS" / "SPEC.md"
    for i in range(5):
        tracker.record_event(test_file, "modified")

    # Get top active files
    top_files = tracker.get_top_active_files(limit=10)

    # Display in table
    table = Table(title="Most Active Files")
    table.add_column("File", style="cyan")
    table.add_column("Angle", justify="right", style="yellow")
    table.add_column("Delta", justify="right", style="green")

    for path, angle, delta in top_files:
        table.add_row(Path(path).name, f"{angle:.2f}", f"{delta:+.2f}")

    console.print(table)
</file>

<file path="wks/extractor.py">
from __future__ import annotations

import hashlib
import io
import logging
import os
import re
import shutil
import subprocess
import tempfile
import zipfile
from contextlib import redirect_stdout, redirect_stderr
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

from .constants import WKS_EXTRACT_EXT


@dataclass
class ExtractResult:
    text: str
    content_path: Optional[Path]
    content_checksum: Optional[str]
    content_bytes: Optional[int]
    engine: str


class Extractor:
    """Shared document extraction helper used by wks0 CLI and SimilarityDB."""

    def __init__(
        self,
        engine: str = "docling",
        *,
        ocr: bool = False,
        timeout_secs: int = 30,
        options: Optional[Dict[str, Any]] = None,
        max_chars: int = 200000,
        write_extension: Optional[str] = None,
    ) -> None:
        self.engine = (engine or "docling").lower()
        self.ocr = bool(ocr)
        self.timeout_secs = int(timeout_secs)
        self.options = dict(options or {})
        self.max_chars = int(max_chars)
        self.write_extension = write_extension or "md"
        if self.engine == "docling":
            self._suppress_docling_noise()

    # --- public API ----------------------------------------------------- #
    def extract(
        self,
        source: Path,
        *,
        persist: bool = True,
        output_dir: Optional[Path] = None,
    ) -> ExtractResult:
        """Extract text from ``source`` and optionally persist to disk."""

        source = source.expanduser().resolve()
        text, ext_hint = self._read_file_text(source)
        if not text or not text.strip():
            raise RuntimeError(f"No extractable text produced for {source}")
        text = text[: self.max_chars]

        if not persist:
            return ExtractResult(
                text=text,
                content_path=None,
                content_checksum=None,
                content_bytes=None,
                engine=self.engine,
            )

        checksum = hashlib.sha256(text.encode("utf-8")).hexdigest()
        ext = (ext_hint or self.write_extension or source.suffix.lstrip(".") or "txt").lstrip(".")
        if output_dir is None:
            repo_root = self._find_repo_root(source)
            if repo_root is not None:
                output_dir = repo_root.parent / WKS_EXTRACT_EXT
            else:
                output_dir = source.parent / WKS_EXTRACT_EXT
        output_dir.mkdir(parents=True, exist_ok=True)
        content_path = output_dir / f"{checksum}.{ext}"
        try:
            with open(content_path, "w", encoding="utf-8") as fh:
                fh.write(text)
        except Exception as exc:  # pragma: no cover
            raise RuntimeError(f"Failed writing extracted text for {source}: {exc}") from exc

        try:
            content_bytes = content_path.stat().st_size
        except Exception:
            content_bytes = len(text.encode("utf-8"))

        return ExtractResult(
            text=text,
            content_path=content_path,
            content_checksum=checksum,
            content_bytes=content_bytes,
            engine=self.engine,
        )

    def read_text(self, source: Path) -> str:
        """Return extracted text without persisting to disk."""
        return self.extract(source, persist=False).text

    # --- engine-specific helpers --------------------------------------- #
    def _docling_convert(self, source: Path) -> str:
        try:
            from docling.document_converter import DocumentConverter
        except Exception as exc:  # pragma: no cover - docling missing
            raise RuntimeError("Docling extractor requested but not installed") from exc

        kwargs = dict(self.options)
        if self.ocr:
            kwargs.setdefault("ocr", True)
        converter = DocumentConverter(**kwargs)
        buffer = io.StringIO()
        with redirect_stdout(buffer), redirect_stderr(buffer):
            result = converter.convert(str(source))
        text = getattr(result, "text", None)
        if not text:
            doc = getattr(result, "document", None)
            if doc and hasattr(doc, "export_to_markdown"):
                try:
                    text = doc.export_to_markdown()
                except Exception:
                    text = None
        if not text:
            text = str(result)
        return text

    def _read_file_text(self, source: Path) -> Tuple[Optional[str], Optional[str]]:
        eff_max = self.max_chars
        if self.engine == "docling":
            try:
                text = self._docling_convert(source)
            except Exception:
                text = None
            if text and text.strip():
                return text, (self.write_extension or "md")
            fallback = self._read_builtin_text(source, eff_max)
            if fallback and fallback.strip():
                ext = source.suffix.lstrip(".") or "txt"
                return fallback, ext
            return None, None

        if self.engine == "builtin":
            text = self._read_builtin_text(source, eff_max)
            return text, (source.suffix.lstrip(".") or "txt")

        raise RuntimeError(f"Unsupported extractor engine: {self.engine}")

    # --- private helpers --------------------------------------------------- #
    _docling_quiet_applied = False

    @classmethod
    def _suppress_docling_noise(cls) -> None:
        if cls._docling_quiet_applied:
            return
        cls._docling_quiet_applied = True
        # Reduce logging verbosity from docling and related libraries
        for name in [
            "docling",
            "docling.document_converter",
            "docling_core",
            "docling.pipeline",
            "docling.pipeline.simple_pipeline",
        ]:
            logging.getLogger(name).setLevel(logging.WARNING)
        # Disable tqdm progress bars emitted by docling pipelines
        os.environ.setdefault("TQDM_DISABLE", "1")
        os.environ.setdefault("DISABLE_TQDM", "1")
        os.environ.setdefault("DOC_PARSER_DISABLE_TQDM", "1")
        os.environ.setdefault("DOC_PARSER_DISABLE_PROGRESS", "1")

    # --- builtin fallback ------------------------------------------------ #
    def _read_builtin_text(self, source: Path, max_chars: int) -> Optional[str]:
        suffix = source.suffix.lower()
        # simple text formats
        if suffix in {".txt", ".md", ".py", ".json", ".yaml", ".yml", ".toml", ".tex", ".rst"}:
            try:
                with open(source, "r", encoding="utf-8", errors="ignore") as fh:
                    return fh.read(max_chars)
            except Exception:
                return None

        if suffix == ".docx":
            return self._extract_docx_text(source, max_chars)
        if suffix == ".pptx":
            return self._extract_pptx_text(source, max_chars)
        if suffix == ".pdf":
            return self._extract_pdf_text(source, max_chars)

        try:
            with open(source, "r", encoding="utf-8", errors="ignore") as fh:
                return fh.read(max_chars)
        except Exception:
            return None

    @staticmethod
    def _find_repo_root(path: Path) -> Optional[Path]:
        try:
            for parent in [path.parent] + list(path.parents):
                git_dir = parent / ".git"
                if git_dir.exists():
                    return parent
        except Exception:
            return None
        return None

    def _extract_docx_text(self, path: Path, max_chars: int) -> Optional[str]:
        try:
            with zipfile.ZipFile(path) as zf:
                with zf.open("word/document.xml") as fh:
                    xml_bytes = fh.read()
            import xml.etree.ElementTree as ET

            root = ET.fromstring(xml_bytes)
            ns = {"w": "http://schemas.openxmlformats.org/wordprocessingml/2006/main"}
            texts = [node.text for node in root.findall(".//w:t", ns) if node.text]
            return "\n".join(texts)[:max_chars]
        except Exception:
            return None

    def _extract_pptx_text(self, path: Path, max_chars: int) -> Optional[str]:
        try:
            with zipfile.ZipFile(path) as zf:
                slide_names = [n for n in zf.namelist() if n.startswith("ppt/slides/slide") and n.endswith(".xml")]
                texts = []
                import xml.etree.ElementTree as ET

                ns = {"a": "http://schemas.openxmlformats.org/drawingml/2006/main"}
                for name in sorted(slide_names):
                    try:
                        with zf.open(name) as fh:
                            xml_bytes = fh.read()
                        root = ET.fromstring(xml_bytes)
                        for node in root.findall(".//a:t", ns):
                            if node.text:
                                texts.append(node.text)
                    except Exception:
                        continue
            return "\n".join(texts)[:max_chars]
        except Exception:
            return None

    def _extract_pdf_text(self, path: Path, max_chars: int) -> Optional[str]:
        try:
            if shutil.which("pdftotext"):
                with tempfile.NamedTemporaryFile(suffix=".txt", delete=True) as tmp:
                    subprocess.run(
                        ["pdftotext", "-layout", str(path), tmp.name],
                        check=False,
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL,
                        timeout=self.timeout_secs,
                    )
                    try:
                        txt = Path(tmp.name).read_text(encoding="utf-8", errors="ignore")
                        if txt and txt.strip():
                            return txt[:max_chars]
                    except Exception:
                        pass
        except Exception:
            pass

        try:
            if shutil.which("strings"):
                out = subprocess.check_output(
                    ["strings", "-n", "4", str(path)],
                    stderr=subprocess.DEVNULL,
                    timeout=self.timeout_secs,
                )
                txt = out.decode("utf-8", errors="ignore")
                txt = re.sub(r"\s+", " ", txt)
                return txt[:max_chars]
        except Exception:
            pass
        return None
</file>

<file path="wks/mcp_server.py">
"""
MCP Server for WKS - Model Context Protocol integration.

Exposes WKS functionality as MCP tools via stdio transport.
Uses existing controllers for zero code duplication per SPEC.md.
"""

import json
import sys
from typing import Any, Dict, List, Optional

from .config import load_config
from .monitor_controller import MonitorController


class MCPServer:
    """MCP server exposing WKS tools via stdio."""

    def __init__(self):
        """Initialize MCP server."""
        self.tools = {
            "wks_monitor_status": {
                "description": "Get filesystem monitoring status and configuration",
                "inputSchema": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            },
            "wks_monitor_check": {
                "description": "Check if a path would be monitored and calculate its priority",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "path": {
                            "type": "string",
                            "description": "File or directory path to check"
                        }
                    },
                    "required": ["path"]
                }
            },
            "wks_monitor_validate": {
                "description": "Validate monitor configuration for conflicts and issues",
                "inputSchema": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            },
            "wks_monitor_list": {
                "description": "Get contents of a monitor configuration list (include_paths, exclude_paths, ignore_dirnames, ignore_globs)",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "list_name": {
                            "type": "string",
                            "description": "Name of the list to retrieve",
                            "enum": ["include_paths", "exclude_paths", "ignore_dirnames", "ignore_globs"]
                        }
                    },
                    "required": ["list_name"]
                }
            },
            "wks_monitor_add": {
                "description": "Add a value to a monitor configuration list",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "list_name": {
                            "type": "string",
                            "description": "Name of the list to modify",
                            "enum": ["include_paths", "exclude_paths", "ignore_dirnames", "ignore_globs"]
                        },
                        "value": {
                            "type": "string",
                            "description": "Value to add (path for include/exclude_paths, dirname for ignore_dirnames, pattern for ignore_globs)"
                        }
                    },
                    "required": ["list_name", "value"]
                }
            },
            "wks_monitor_remove": {
                "description": "Remove a value from a monitor configuration list",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "list_name": {
                            "type": "string",
                            "description": "Name of the list to modify",
                            "enum": ["include_paths", "exclude_paths", "ignore_dirnames", "ignore_globs"]
                        },
                        "value": {
                            "type": "string",
                            "description": "Value to remove"
                        }
                    },
                    "required": ["list_name", "value"]
                }
            },
            "wks_monitor_managed_list": {
                "description": "Get all managed directories with their priorities",
                "inputSchema": {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            },
            "wks_monitor_managed_add": {
                "description": "Add a managed directory with priority",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "path": {
                            "type": "string",
                            "description": "Directory path to add"
                        },
                        "priority": {
                            "type": "integer",
                            "description": "Priority score (higher = more important)"
                        }
                    },
                    "required": ["path", "priority"]
                }
            },
            "wks_monitor_managed_remove": {
                "description": "Remove a managed directory",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "path": {
                            "type": "string",
                            "description": "Directory path to remove"
                        }
                    },
                    "required": ["path"]
                }
            },
            "wks_monitor_managed_set_priority": {
                "description": "Update priority for a managed directory",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "path": {
                            "type": "string",
                            "description": "Directory path"
                        },
                        "priority": {
                            "type": "integer",
                            "description": "New priority score"
                        }
                    },
                    "required": ["path", "priority"]
                }
            }
        }

    def _read_message(self) -> Optional[Dict[str, Any]]:
        """Read JSON-RPC message from stdin."""
        try:
            line = sys.stdin.readline()
            if not line:
                return None
            return json.loads(line)
        except Exception as e:
            # Log parse errors to stderr but don't send error response
            # (can't send valid JSON-RPC error without request ID)
            sys.stderr.write(f"Parse error: {e}\n")
            return None

    def _write_message(self, message: Dict[str, Any]) -> None:
        """Write JSON-RPC message to stdout."""
        sys.stdout.write(json.dumps(message) + "\n")
        sys.stdout.flush()

    def _write_response(self, request_id: Any, result: Any) -> None:
        """Write JSON-RPC response."""
        self._write_message({
            "jsonrpc": "2.0",
            "id": request_id,
            "result": result
        })

    def _write_error(self, request_id: Any, code: int, message: str, data: Any = None) -> None:
        """Write JSON-RPC error response."""
        error = {"code": code, "message": message}
        if data is not None:
            error["data"] = data
        self._write_message({
            "jsonrpc": "2.0",
            "id": request_id,
            "error": error
        })

    def _handle_initialize(self, request_id: Any, params: Dict[str, Any]) -> None:
        """Handle initialize request."""
        self._write_response(request_id, {
            "protocolVersion": "2024-11-05",
            "capabilities": {
                "tools": {}
            },
            "serverInfo": {
                "name": "wks-mcp-server",
                "version": "1.0.0"
            }
        })

    def _handle_list_tools(self, request_id: Any) -> None:
        """Handle tools/list request."""
        tools_list = [
            {
                "name": name,
                "description": info["description"],
                "inputSchema": info["inputSchema"]
            }
            for name, info in self.tools.items()
        ]
        self._write_response(request_id, {"tools": tools_list})

    def _build_tool_registry(self) -> Dict[str, callable]:
        """Build registry of tool handlers with parameter validation."""
        def _require_params(*param_names: str):
            """Decorator to validate required parameters."""
            def decorator(handler: callable) -> callable:
                def wrapper(config: Dict[str, Any], arguments: Dict[str, Any]) -> Dict[str, Any]:
                    missing = [p for p in param_names if arguments.get(p) is None]
                    if missing:
                        raise ValueError(f"Missing required parameters: {', '.join(missing)}")
                    return handler(config, arguments)
                return wrapper
            return decorator

        return {
            "wks_monitor_status": lambda config, args: self._tool_monitor_status(config),
            "wks_monitor_check": _require_params("path")(
                lambda config, args: self._tool_monitor_check(config, args["path"])
            ),
            "wks_monitor_validate": lambda config, args: self._tool_monitor_validate(config),
            "wks_monitor_list": _require_params("list_name")(
                lambda config, args: self._tool_monitor_list(config, args["list_name"])
            ),
            "wks_monitor_add": _require_params("list_name", "value")(
                lambda config, args: self._tool_monitor_add(config, args["list_name"], args["value"])
            ),
            "wks_monitor_remove": _require_params("list_name", "value")(
                lambda config, args: self._tool_monitor_remove(config, args["list_name"], args["value"])
            ),
            "wks_monitor_managed_list": lambda config, args: self._tool_monitor_managed_list(config),
            "wks_monitor_managed_add": _require_params("path", "priority")(
                lambda config, args: self._tool_monitor_managed_add(config, args["path"], args["priority"])
            ),
            "wks_monitor_managed_remove": _require_params("path")(
                lambda config, args: self._tool_monitor_managed_remove(config, args["path"])
            ),
            "wks_monitor_managed_set_priority": _require_params("path", "priority")(
                lambda config, args: self._tool_monitor_managed_set_priority(config, args["path"], args["priority"])
            ),
        }

    def _handle_call_tool(self, request_id: Any, params: Dict[str, Any]) -> None:
        """Handle tools/call request using registry pattern."""
        tool_name = params.get("name")
        arguments = params.get("arguments", {})

        if tool_name not in self.tools:
            self._write_error(request_id, -32601, f"Tool not found: {tool_name}")
            return

        try:
            config = load_config()
            registry = self._build_tool_registry()

            if tool_name not in registry:
                self._write_error(request_id, -32601, f"Tool not implemented: {tool_name}")
                return

            handler = registry[tool_name]
            result = handler(config, arguments)

            self._write_response(request_id, {
                "content": [
                    {
                        "type": "text",
                        "text": json.dumps(result, indent=2)
                    }
                ]
            })

        except ValueError as e:
            self._write_error(request_id, -32602, str(e))
        except Exception as e:
            self._write_error(request_id, -32000, f"Tool execution failed: {e}", {"traceback": str(e)})

    def _tool_monitor_status(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Execute wks_monitor_status tool."""
        status = MonitorController.get_status(config)
        return status.to_dict()

    def _tool_monitor_check(self, config: Dict[str, Any], path: str) -> Dict[str, Any]:
        """Execute wks_monitor_check tool."""
        return MonitorController.check_path(config, path)

    def _tool_monitor_validate(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Execute wks_monitor_validate tool."""
        return MonitorController.validate_config(config)

    def _tool_monitor_list(self, config: Dict[str, Any], list_name: str) -> Dict[str, Any]:
        """Execute wks_monitor_list tool."""
        return MonitorController.get_list(config, list_name)

    def _tool_monitor_add(self, config: Dict[str, Any], list_name: str, value: str) -> Dict[str, Any]:
        """Execute wks_monitor_add tool."""
        from .config import get_config_path

        config_path = get_config_path()
        if not config_path.exists():
            return {"success": False, "message": f"Config file not found: {config_path}"}

        # Load config from file
        with open(config_path) as f:
            config_dict = json.load(f)

        # Determine if we need to resolve paths
        resolve_path = list_name in ["include_paths", "exclude_paths"]

        # Add to list
        result = MonitorController.add_to_list(config_dict, list_name, value, resolve_path)

        # Save if successful
        if result.get("success"):
            with open(config_path, "w") as f:
                json.dump(config_dict, f, indent=4)
            result["note"] = "Restart the monitor service for changes to take effect"

        return result

    def _tool_monitor_remove(self, config: Dict[str, Any], list_name: str, value: str) -> Dict[str, Any]:
        """Execute wks_monitor_remove tool."""
        from .config import get_config_path

        config_path = get_config_path()
        if not config_path.exists():
            return {"success": False, "message": f"Config file not found: {config_path}"}

        # Load config from file
        with open(config_path) as f:
            config_dict = json.load(f)

        # Determine if we need to resolve paths
        resolve_path = list_name in ["include_paths", "exclude_paths"]

        # Remove from list
        result = MonitorController.remove_from_list(config_dict, list_name, value, resolve_path)

        # Save if successful
        if result.get("success"):
            with open(config_path, "w") as f:
                json.dump(config_dict, f, indent=4)
            result["note"] = "Restart the monitor service for changes to take effect"

        return result

    def _tool_monitor_managed_list(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Execute wks_monitor_managed_list tool."""
        return MonitorController.get_managed_directories(config)

    def _tool_monitor_managed_add(self, config: Dict[str, Any], path: str, priority: int) -> Dict[str, Any]:
        """Execute wks_monitor_managed_add tool."""
        from .config import get_config_path

        config_path = get_config_path()
        if not config_path.exists():
            return {"success": False, "message": f"Config file not found: {config_path}"}

        # Load config from file
        with open(config_path) as f:
            config_dict = json.load(f)

        # Add managed directory
        result = MonitorController.add_managed_directory(config_dict, path, priority)

        # Save if successful
        if result.get("success"):
            with open(config_path, "w") as f:
                json.dump(config_dict, f, indent=4)
            result["note"] = "Restart the monitor service for changes to take effect"

        return result

    def _tool_monitor_managed_remove(self, config: Dict[str, Any], path: str) -> Dict[str, Any]:
        """Execute wks_monitor_managed_remove tool."""
        from .config import get_config_path

        config_path = get_config_path()
        if not config_path.exists():
            return {"success": False, "message": f"Config file not found: {config_path}"}

        # Load config from file
        with open(config_path) as f:
            config_dict = json.load(f)

        # Remove managed directory
        result = MonitorController.remove_managed_directory(config_dict, path)

        # Save if successful
        if result.get("success"):
            with open(config_path, "w") as f:
                json.dump(config_dict, f, indent=4)
            result["note"] = "Restart the monitor service for changes to take effect"

        return result

    def _tool_monitor_managed_set_priority(self, config: Dict[str, Any], path: str, priority: int) -> Dict[str, Any]:
        """Execute wks_monitor_managed_set_priority tool."""
        from .config import get_config_path

        config_path = get_config_path()
        if not config_path.exists():
            return {"success": False, "message": f"Config file not found: {config_path}"}

        # Load config from file
        with open(config_path) as f:
            config_dict = json.load(f)

        # Set priority
        result = MonitorController.set_managed_priority(config_dict, path, priority)

        # Save if successful
        if result.get("success"):
            with open(config_path, "w") as f:
                json.dump(config_dict, f, indent=4)
            result["note"] = "Restart the monitor service for changes to take effect"

        return result

    def run(self) -> None:
        """Run MCP server main loop."""
        while True:
            message = self._read_message()
            if message is None:
                break

            request_id = message.get("id")
            method = message.get("method")
            params = message.get("params", {})

            # Handle different methods
            if method == "initialize":
                self._handle_initialize(request_id, params)
            elif method == "initialized" or method == "notifications/initialized":
                # Notification, no response needed
                pass
            elif method == "tools/list":
                self._handle_list_tools(request_id)
            elif method == "tools/call":
                self._handle_call_tool(request_id, params)
            elif method == "ping":
                self._write_response(request_id, {})
            else:
                # Only send error if this is a request (has ID), not a notification
                if request_id is not None:
                    self._write_error(request_id, -32601, f"Method not found: {method}")


def main():
    """Main entry point for MCP server."""
    server = MCPServer()
    try:
        server.run()
    except KeyboardInterrupt:
        sys.stderr.write("\nMCP Server stopped.\n")
        sys.exit(0)
    except Exception as e:
        sys.stderr.write(f"MCP Server error: {e}\n")
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="wks/utils.py">
"""Common utility functions for WKS."""

from pathlib import Path
import hashlib
import os

try:
    import importlib.metadata as importlib_metadata
except ImportError:  # pragma: no cover
    import importlib_metadata  # type: ignore

from .constants import WKS_HOME_EXT


def file_checksum(path: Path) -> str:
    """Calculate SHA256 checksum of a file."""
    hasher = hashlib.sha256()
    with open(path, "rb") as fh:
        for chunk in iter(lambda: fh.read(1024 * 1024), b""):
            if not chunk:
                break
            hasher.update(chunk)
    return hasher.hexdigest()


# Package version - cached for performance
_VERSION_CACHE = None


def get_package_version() -> str:
    """Get WKS package version (cached)."""
    global _VERSION_CACHE
    if _VERSION_CACHE is None:
        try:
            _VERSION_CACHE = importlib_metadata.version("wks")
        except Exception:
            _VERSION_CACHE = "unknown"
    return _VERSION_CACHE


def expand_path(path: str) -> Path:
    """Expand user path (~/...) to absolute path."""
    return Path(path).expanduser()


def get_wks_home() -> Path:
    """Get WKS home directory path.

    Checks WKS_HOME environment variable first, defaults to ~/.wks if not set.

    Returns:
        Path to WKS home directory

    Examples:
        >>> # WKS_HOME not set
        >>> get_wks_home()
        Path("/Users/user/.wks")
        >>> # WKS_HOME="/custom/path"
        >>> get_wks_home()
        Path("/custom/path")
    """
    wks_home_env = os.environ.get("WKS_HOME")
    if wks_home_env:
        return Path(wks_home_env).expanduser().resolve()
    return Path.home() / WKS_HOME_EXT


def wks_home_path(*parts: str) -> Path:
    """Get path under WKS home directory.

    Args:
        *parts: Path components to join (e.g., "config.json", "mongodb", etc.)

    Returns:
        Absolute path under WKS home directory

    Examples:
        >>> wks_home_path("config.json")
        Path("/Users/user/.wks/config.json")
        >>> wks_home_path("mongodb", "data")
        Path("/Users/user/.wks/mongodb/data")
    """
    wks_home = get_wks_home()
    return wks_home / Path(*parts) if parts else wks_home
</file>

<file path="ROADMAP.md">
# WKS Roadmap

A forward-looking view of outcomes and constraints. Use this as a compass; the checklists live in `TASKS.md`.

## Phase 1 ‚Äî Foundation (DONE)

Outcome: A clean baseline with structure, a minimal vault, and a settled spec.
Exit criteria:
- Home has year-scoped work roots; vault skeleton exists; spec is versioned.

## Phase 2 ‚Äî Migration (DONE)

Outcome: Legacy material is re-homed; staging areas are quiet; embeds reflect truth.
Exit criteria:
- Legacy trees are archived or adopted; loose files are routed; embeds resolve.

## Phase 3 ‚Äî Agent (IN PROGRESS)

Outcome: A background agent keeps filesystem and vault in sync with minimal noise.
Exit criteria:
- Link drift is corrected automatically; actionable suggestions are surfaced sparingly.

### Recent Cleanup Work (2025-11-13)

**CLI Refactoring & Cleanup**:
- Split monolithic `wks/cli.py` (3122 lines) into modular structure:
  - `wks/cli/main.py` - Main entry point and argument parsing
  - `wks/cli/commands/` - Command-specific modules (service, monitor, config, index, related, db)
  - `wks/cli/display_strategies.py` - Display mode strategies (CLI/MCP)
  - `wks/cli/helpers.py` - Shared helper functions
  - `wks/cli/dataclasses.py` - CLI-specific dataclasses
  - `wks/cli/constants.py` - CLI constants
- Renamed CLI from `wks0` to `wks0` (dev version)
- Unified table display format for `service status` and `monitor status`:
  - Reflowing two-column layout with panels
  - Health section on left, File System on right for service status
  - Bold cyan headings for section headers
  - Consistent styling with `MAX_DISPLAY_WIDTH = 80`
- Added `--live` option to both `service status` and `monitor status` for auto-updating displays
- Standardized CLI output: 4-step process (STDERR for status/progress, STDOUT for output)
- Removed all backward compatibility code and hedging
- Applied fail-hard principle: explicit validation, no defaults/fallbacks
- Reduced complexity: refactored functions with CCN>10 or NLOC>100
- Used dataclasses instead of dicts for internal data structures
- Applied design patterns: Registry, Builder, Strategy

**Status**: Service, monitor, config, and db commands are in good shape. **Next: Vault layer.**

## Phase 4 ‚Äî Iteration

Outcome: The system improves through measured changes tied to real usage.
Exit criteria:
- A light review cadence exists; pain points are logged and addressed.

## Constraints

- Prefer offline operation and local models.
- Keep the vault readable without plugins; treat helpers as optional.
- Favor small, reversible changes over large refactors.

## Risk Notes

- Naming drift can obscure meaning ‚Üí normalize proactively.
- Embeds can stale when files move ‚Üí maintain a consistent `_links/` hub.
- Automation can overreach ‚Üí require explicit opt‚Äëin for destructive changes.
</file>

<file path="wks/cli/display_strategies.py">
"""Display strategies for service status output."""

import argparse
import sys
import time
from typing import Any, List, Optional, Tuple

from ..constants import MAX_DISPLAY_WIDTH
from ..service_controller import ServiceController, ServiceStatusData, ServiceStatusLaunch


class DisplayStrategy:
    """Base class for display strategies."""

    def render(self, status: ServiceStatusData, args: argparse.Namespace) -> int:
        """Render status using this strategy. Returns exit code."""
        raise NotImplementedError


class MCPDisplayStrategy(DisplayStrategy):
    """Strategy for MCP display mode."""

    def render(self, status: ServiceStatusData, args: argparse.Namespace) -> int:
        live = getattr(args, "live", False)
        if live:
            sys.stderr.write("--live mode is not supported with MCP display\n")
            raise ValueError("Live mode not supported with MCP")

        display = args.display_obj
        payload = status.to_dict()
        display.success("WKS service status", data=payload)
        return 0


class CLIDisplayStrategy(DisplayStrategy):
    """Strategy for CLI display mode."""

    def _render_live(self, status: ServiceStatusData, args: argparse.Namespace) -> int:
        """Render live-updating display mode."""
        from rich.console import Console, Group
        from rich.live import Live
        from rich.table import Table
        from rich import box

        from ..constants import MAX_DISPLAY_WIDTH
        console = Console(width=MAX_DISPLAY_WIDTH)

        def _render_status():
            """Render current status as a Rich panel with Health left, File System right."""
            from rich.panel import Panel

            current_status = ServiceController.get_status()

            status_panel = self._status_panel(current_status, title="Status (Live)")
            renderables = [status_panel]
            launch_title, launch_content, launch_style = self._launch_panel_data(current_status.launch)
            renderables.append(
                Panel(launch_content, title=launch_title, border_style=launch_style, width=MAX_DISPLAY_WIDTH)
            )
            last_title, last_content, last_style = self._last_error_panel_data(current_status.last_error)
            renderables.append(
                Panel(last_content, title=last_title, border_style=last_style, width=MAX_DISPLAY_WIDTH)
            )
            notes_title, notes_content, notes_style = self._notes_panel_data(current_status.notes or [])
            renderables.append(
                Panel(notes_content, title=notes_title, border_style=notes_style, width=MAX_DISPLAY_WIDTH)
            )

            return Group(*renderables)

        try:
            with Live(_render_status(), refresh_per_second=0.5, screen=False, console=console) as live:
                while True:
                    time.sleep(2.0)
                    try:
                        live.update(_render_status())
                    except Exception as update_exc:
                        console.print(f"[yellow]Warning: {update_exc}[/yellow]", end="")
        except KeyboardInterrupt:
            console.print("\n[dim]Stopped monitoring.[/dim]")
            return 0
        except Exception as exc:
            console.print(f"[red]Error in live mode: {exc}[/red]")
            return 2

    def render(self, status: ServiceStatusData, args: argparse.Namespace) -> int:
        """Render status - either live or static based on args."""
        live = getattr(args, "live", False)
        if live:
            return self._render_live(status, args)

        display = args.display_obj
        status_panel = self._status_panel(status, title="Status")
        display.console.print(status_panel)
        self._render_launch_panel(display, status.launch)
        self._render_last_error_panel(display, status.last_error)
        self._render_notes_panel(display, status.notes or [])
        return 0

    @staticmethod
    def _last_error_panel_data(error_text: Optional[str]) -> Tuple[str, str, str]:
        content = error_text.strip() if error_text else "[dim]No recent errors[/dim]"
        return "Last Error", content, "red"

    @staticmethod
    def _notes_panel_data(notes: List[str]) -> Tuple[str, str, str]:
        if notes:
            content = "\n".join(f"‚Ä¢ {note}" for note in notes)
        else:
            content = "[dim]No notes[/dim]"
        return "Notes", content, "cyan"

    @staticmethod
    def _launch_panel_data(launch: ServiceStatusLaunch) -> Tuple[str, Any, str]:
        from rich.table import Table

        table = Table(show_header=False, box=None, padding=(0, 1))
        table.add_column("Key", justify="left", style="magenta")
        table.add_column("Value", justify="right")

        if launch and launch.present():
            fields = [
                ("Type", launch.type),
                ("Path", launch.path),
                ("Program", launch.arguments or launch.program),
                ("Stdout", launch.stdout),
                ("Stderr", launch.stderr),
            ]
            for label, value in fields:
                table.add_row(label, value or "-")
        else:
            table.add_row("", "[dim]No launch agent data[/dim]")

        return "Launch Agent", table, "magenta"

    def _render_last_error_panel(self, display: Any, error_text: Optional[str]) -> None:
        """Render the last error in its own panel for readability."""
        title, content, style = self._last_error_panel_data(error_text)
        display.panel(content, title=title, border_style=style, width=MAX_DISPLAY_WIDTH)

    def _render_notes_panel(self, display: Any, notes: List[str]) -> None:
        """Render notes in a dedicated panel."""
        title, content, style = self._notes_panel_data(notes)
        display.panel(content, title=title, border_style=style, width=MAX_DISPLAY_WIDTH)

    def _render_launch_panel(self, display: Any, launch: ServiceStatusLaunch) -> None:
        """Render launch agent details in its own panel."""
        title, content, style = self._launch_panel_data(launch)
        display.panel(content, title=title, border_style=style, width=MAX_DISPLAY_WIDTH)

    @staticmethod
    def _status_panel(status: ServiceStatusData, title: str):
        from rich.table import Table
        from rich.panel import Panel
        from rich.columns import Columns

        health_rows = status._build_health_rows()
        filesystem_rows = status._build_filesystem_rows()

        left_table = Table(show_header=False, box=None, padding=(0, 1), expand=True)
        left_table.add_column("Key", justify="left", min_width=20, no_wrap=True)
        left_table.add_column("Value", justify="right", min_width=12)
        for key, value in health_rows:
            left_table.add_row(key, value)

        right_table = Table(show_header=False, box=None, padding=(0, 1), expand=True)
        right_table.add_column("Key", justify="left", min_width=20, no_wrap=True)
        right_table.add_column("Value", justify="right", min_width=12)
        for key, value in filesystem_rows:
            right_table.add_row(key, value)

        columns = Columns([left_table, right_table], equal=True, expand=True)
        return Panel.fit(columns, title=title, border_style="cyan", width=MAX_DISPLAY_WIDTH)


def get_display_strategy(args: argparse.Namespace) -> DisplayStrategy:
    """Get appropriate display strategy based on args."""
    from .constants import DISPLAY_CHOICES

    display_mode = getattr(args, "display", None)
    if display_mode not in DISPLAY_CHOICES:
        raise ValueError(f"Invalid display mode: {display_mode!r}. Must be one of {DISPLAY_CHOICES}")

    if display_mode == "mcp":
        return MCPDisplayStrategy()

    return CLIDisplayStrategy()
</file>

<file path="wks/display/cli.py">
"""CLI display implementation using Rich library."""

import sys
from typing import Any, Dict, List, Optional
from pathlib import Path

from ..constants import MAX_DISPLAY_WIDTH

try:
    from rich.console import Console
    from rich.table import Table
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
    from rich.tree import Tree
    from rich.panel import Panel
    from rich.syntax import Syntax
    from rich import print as rprint
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

from .base import Display


class CLIDisplay(Display):
    """Beautiful CLI display using Rich library."""

    def __init__(self):
        if not RICH_AVAILABLE:
            raise ImportError("Rich library required for CLI display. Install with: pip install rich")

        # Limit console width to MAX_DISPLAY_WIDTH for consistent display
        detected_width = None
        try:
            import shutil
            detected_width = shutil.get_terminal_size().columns
        except Exception:
            pass

        console_width = min(detected_width or MAX_DISPLAY_WIDTH, MAX_DISPLAY_WIDTH)
        self.console = Console(force_terminal=True, width=console_width)
        self.stderr_console = Console(file=sys.stderr, width=console_width)
        self._progress_contexts = {}  # Store Progress contexts by handle

    def status(self, message: str, **kwargs) -> None:
        """Display a status message in blue."""
        self.console.print(f"[blue]‚Ñπ[/blue] {message}")

    def success(self, message: str, **kwargs) -> None:
        """Display a success message in green."""
        self.console.print(f"[green]‚úì[/green] {message}")

    def error(self, message: str, **kwargs) -> None:
        """Display an error message in red."""
        details = kwargs.get("details", "")
        if details:
            self.console.print(f"[red]‚úó[/red] {message}")
            self.console.print(f"  [dim]{details}[/dim]")
        else:
            self.console.print(f"[red]‚úó[/red] {message}")

    def warning(self, message: str, **kwargs) -> None:
        """Display a warning message in yellow."""
        self.console.print(f"[yellow]‚ö†[/yellow] {message}")

    def info(self, message: str, **kwargs) -> None:
        """Display an informational message."""
        self.console.print(message)

    def table(self, data: List[Dict[str, Any]], headers: Optional[List[str]] = None, **kwargs) -> None:
        """Display data in a rich table."""
        if not data:
            self.info("No data to display")
            return

        title = kwargs.get("title", "")
        column_justify = kwargs.get("column_justify", {})  # Dict[str, str] mapping header to justify
        show_header = kwargs.get("show_header", True)
        width = kwargs.get("width", MAX_DISPLAY_WIDTH)

        # Infer headers from first row if not provided
        if headers is None:
            headers = list(data[0].keys())

        table = Table(title=title, show_header=show_header, header_style="bold cyan", width=min(width, MAX_DISPLAY_WIDTH))

        for header in headers:
            justify = column_justify.get(header, "left")
            table.add_column(header, justify=justify)

        for row in data:
            table.add_row(*[str(row.get(h, "")) for h in headers])

        self.console.print(table)

    def progress_start(self, total: int, description: str = "", **kwargs) -> Any:
        """Start a progress bar (outputs to STDERR)."""
        progress = Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeRemainingColumn(),
            console=self.stderr_console  # Progress goes to STDERR
        )
        progress.start()
        task_id = progress.add_task(description, total=total)

        # Store progress context with task_id
        handle = id(progress)
        self._progress_contexts[handle] = (progress, task_id)

        return handle

    def progress_update(self, handle: Any, advance: int = 1, **kwargs) -> None:
        """Update progress bar."""
        if handle not in self._progress_contexts:
            return

        progress, task_id = self._progress_contexts[handle]

        # Update description if provided
        description = kwargs.get("description")
        if description:
            progress.update(task_id, description=description, advance=advance)
        else:
            progress.update(task_id, advance=advance)

    def progress_finish(self, handle: Any, **kwargs) -> None:
        """Finish progress bar."""
        if handle not in self._progress_contexts:
            return

        progress, task_id = self._progress_contexts[handle]
        progress.stop()
        del self._progress_contexts[handle]

    def spinner_start(self, description: str = "", **kwargs) -> Any:
        """Start a spinner."""
        status = self.console.status(description, spinner="dots")
        status.start()
        return status

    def spinner_update(self, handle: Any, description: str, **kwargs) -> None:
        """Update spinner description."""
        if handle:
            handle.update(description)

    def spinner_finish(self, handle: Any, message: str = "", **kwargs) -> None:
        """Stop spinner."""
        if handle:
            handle.stop()
        if message:
            self.info(message)

    def tree(self, data: Dict[str, Any], title: str = "", **kwargs) -> None:
        """Display hierarchical data as a tree."""
        tree = Tree(title if title else "Tree")
        self._build_tree(tree, data)
        self.console.print(tree)

    def _build_tree(self, tree: Tree, data: Any, key: str = "") -> None:
        """Recursively build tree structure."""
        if isinstance(data, dict):
            for k, v in data.items():
                if isinstance(v, (dict, list)):
                    branch = tree.add(f"[bold]{k}[/bold]")
                    self._build_tree(branch, v, k)
                else:
                    tree.add(f"{k}: {v}")
        elif isinstance(data, list):
            for i, item in enumerate(data):
                if isinstance(item, (dict, list)):
                    branch = tree.add(f"[dim][{i}][/dim]")
                    self._build_tree(branch, item)
                else:
                    tree.add(str(item))
        else:
            tree.add(str(data))

    def json_output(self, data: Any, **kwargs) -> None:
        """Output JSON with syntax highlighting."""
        import json
        indent = kwargs.get("indent", 2)
        json_str = json.dumps(data, indent=indent)
        syntax = Syntax(json_str, "json", theme="monokai", line_numbers=False)
        self.console.print(syntax)

    def panel(self, content: Any, title: str = "", **kwargs) -> None:
        """Display content in a panel."""
        panel_kwargs = dict(kwargs)
        border_style = panel_kwargs.pop("border_style", "blue")
        panel = Panel(
            content,
            title=title,
            border_style=border_style,
            **panel_kwargs,
        )
        self.console.print(panel)
</file>

<file path="wks/cli_db.py">
"""WKS CLI - Database commands

Per SPEC.md, database commands are organized by layer:
- wks0 db monitor    -- Query filesystem monitoring database
- wks0 db vault      -- Query knowledge graph links (future)
- wks0 db related    -- Query similarity embeddings (future)
- wks0 db index      -- Query search indices (future)
"""

import argparse
import json
from typing import Optional

from .config import load_config
from .db_helpers import get_monitor_db_config, connect_to_mongo


def _parse_json_arg(value: Optional[str], arg_name: str, display) -> tuple[Optional[dict], Optional[int]]:
    """Parse a JSON argument string.

    Returns:
        Tuple of (parsed_dict, error_code). If successful, error_code is None.
    """
    if not value:
        return {}, None

    try:
        return json.loads(value), None
    except json.JSONDecodeError as e:
        display.error(f"Invalid JSON {arg_name}: {e}")
        return None, 2


def _parse_query_args(args: argparse.Namespace, display) -> tuple[Optional[dict], Optional[dict], Optional[int]]:
    """Parse filter and projection arguments."""
    filter_dict, error = _parse_json_arg(args.filter if hasattr(args, 'filter') else None, "filter", display)
    if error:
        return None, None, error

    projection, error = _parse_json_arg(args.projection if hasattr(args, 'projection') else None, "projection", display)
    if error:
        return None, None, error

    return filter_dict or {}, projection or {}, None


def _build_query_cursor(coll, filter_dict: dict, projection: dict, limit: int, sort_arg: Optional[str]):
    """Build MongoDB query cursor with filter, projection, limit, and sort."""
    cursor = coll.find(filter_dict, projection).limit(limit)

    if sort_arg:
        field, direction = sort_arg.split(':') if ':' in sort_arg else (sort_arg, 'desc')
        sort_dir = 1 if direction.lower().startswith('asc') else -1
        cursor = cursor.sort(field, sort_dir)
    else:
        cursor = cursor.sort("timestamp", -1)

    return cursor


def _clean_document(doc: dict) -> dict:
    """Clean document for display by removing internal fields and formatting."""
    exclude_fields = {"_id", "touches", "avg_time_between_modifications", "touches_per_second"}
    doc_clean = {k: v for k, v in doc.items() if k not in exclude_fields}

    tpd = doc_clean.get("touches_per_day")
    if isinstance(tpd, (int, float)):
        doc_clean["touches_per_day"] = f"{tpd:.2e}"

    return doc_clean


def _display_query_results(display, db_name: str, coll_name: str, total: int, docs: list):
    """Display query results."""
    display.info(f"Database: {db_name}.{coll_name}")
    display.info(f"Total documents: {total}")
    display.info(f"Showing: {len(docs)} documents\n")

    if docs:
        for idx, doc in enumerate(docs, 1):
            doc_clean = _clean_document(doc)
            display.info(f"[{idx}]")
            display.json_output(doc_clean)
            display.info("")
    else:
        display.warning("No documents found")


def _db_monitor(args: argparse.Namespace) -> int:
    """Query the filesystem monitoring database."""
    display = args.display_obj
    cfg = load_config()
    uri, db_name, coll_name = get_monitor_db_config(cfg)

    filter_dict, projection, error = _parse_query_args(args, display)
    if error:
        return error

    try:
        client = connect_to_mongo(uri)
    except Exception as e:
        display.error(f"Database connection failed: {e}")
        return 2

    try:
        coll = client[db_name][coll_name]
        total = coll.count_documents(filter_dict)
        limit = getattr(args, 'limit', 10)
        sort_arg = getattr(args, 'sort', None) if hasattr(args, 'sort') else None

        cursor = _build_query_cursor(coll, filter_dict, projection, limit, sort_arg)
        docs = list(cursor)

        _display_query_results(display, db_name, coll_name, total, docs)
        return 0

    except Exception as e:
        display.error(f"Query failed: {e}")
        return 1
    finally:
        client.close()


def setup_db_parser(subparsers) -> argparse.ArgumentParser:
    """Setup argument parser for database commands."""
    dbp = subparsers.add_parser("db", help="Database helpers: query and stats")
    dbsub = dbp.add_subparsers(dest="db_cmd", required=False)

    # wks0 db monitor
    mon = dbsub.add_parser("monitor", help="Query filesystem monitoring database")
    mon.add_argument("--filter", help='JSON filter, e.g. {"priority": {"$gte": 100}}')
    mon.add_argument("--projection", help='JSON projection, e.g. {"path":1,"priority":1}')
    mon.add_argument("--limit", type=int, default=10, help="Limit results (default: 10)")
    mon.add_argument("--sort", help='Sort field:direction, e.g. "priority:desc" or "timestamp:asc"')
    mon.set_defaults(func=_db_monitor)

    return dbp
</file>

<file path="wks/config_schema.py">
"""Config schema and migration for WKS layered architecture."""

import copy
from pathlib import Path
from typing import Dict, Any, Optional


def is_old_config(config: Dict[str, Any]) -> bool:
    """Detect if config is old format.

    Old format has:
    - 'vault_path' at top level
    - 'obsidian' section (not 'vault')
    - 'similarity' section (not 'related')
    - No 'monitor.managed_directories'

    Args:
        config: Config dict

    Returns:
        True if old format detected
    """
    # Check for old format indicators
    has_vault_path = "vault_path" in config
    has_obsidian = "obsidian" in config
    has_similarity = "similarity" in config
    has_vault = "vault" in config

    # Old format has vault_path and obsidian, not vault
    if has_vault_path and has_obsidian and not has_vault:
        return True

    # Check for new format indicators
    if "monitor" in config:
        monitor = config["monitor"]
        if "managed_directories" in monitor:
            return False  # Definitely new format

    # Check for old similarity vs new related
    if has_similarity and "related" not in config:
        return True

    return False


def migrate_config(old_config: Dict[str, Any]) -> Dict[str, Any]:
    """Migrate old config format to new layered architecture format.

    Args:
        old_config: Config in old format

    Returns:
        Config in new format
    """
    new_config = {}

    # === Monitor section ===
    old_monitor = old_config.get("monitor", {})

    new_config["monitor"] = {
        "include_paths": old_monitor.get("include_paths", ["~"]),
        "exclude_paths": old_monitor.get("exclude_paths", ["~/Library", "~/obsidian", "~/.wks"]),
        "ignore_dirnames": old_monitor.get("ignore_dirnames", [
            ".cache", ".venv", "__pycache__", "_build",
            "build", "dist", "node_modules", "venv"
        ]),
        "ignore_globs": old_monitor.get("ignore_globs", [
            "**/.DS_Store", "*.swp", "*.tmp", "*~", "._*", "~$*", ".~lock.*#"
        ]),
        # New: managed directories with priorities
        "managed_directories": {
            "~/Desktop": 150,
            "~/deadlines": 120,
            "~": 100,
            "~/Documents": 100,
            "~/Pictures": 80,
            "~/Downloads": 50,
        },
        # New: priority calculation config
        "priority": {
            "depth_multiplier": 0.9,
            "underscore_divisor": 2,
            "single_underscore_divisor": 64,
            "extension_weights": {
                ".docx": 1.3,
                ".pptx": 1.3,
                ".pdf": 1.1,
                "default": 1.0
            },
            "auto_index_min": 2
        },
        "database": "wks.monitor",
        "max_documents": 1000000,
        "log_file": "~/.wks/monitor.log"
    }

    touch_weight = old_monitor.get("touch_weight")
    if isinstance(touch_weight, (int, float)):
        new_config["monitor"]["touch_weight"] = float(touch_weight)
    else:
        new_config["monitor"]["touch_weight"] = 0.1

    # === Vault section (from old obsidian section) ===
    old_obsidian = old_config.get("obsidian", {})
    vault_path = old_config.get("vault_path", "~/obsidian")

    new_config["vault"] = {
        "type": "obsidian",
        "base_dir": vault_path,
        "wks_dir": old_obsidian.get("base_dir", "WKS"),
        "update_frequency_seconds": 10,
        "database": "wks.vault"
    }

    # === DB section ===
    old_mongo = old_config.get("mongo", {})

    new_config["db"] = {
        "type": "mongodb",  # Database type: mongodb (only supported type currently)
        "uri": old_mongo.get("uri", "mongodb://localhost:27017/")
    }

    # === Extract section ===
    old_extract = old_config.get("extract", {})
    old_engine = old_extract.get("engine", "docling")

    new_config["extract"] = {
        "output_dir_rules": {
            "resolve_symlinks": True,
            "git_parent": True,
            "underscore_sibling": True
        },
        "engines": {
            "docling": {
                "enabled": old_engine == "docling",
                "is_default": old_engine == "docling",
                "ocr": old_extract.get("ocr", False),
                "timeout_secs": old_extract.get("timeout_secs", 30),
                "write_extension": old_extract.get("write_extension", "md")
            },
            "builtin": {
                "enabled": True,
                "max_chars": 200000
            }
        },
        "_router": {
            "rules": [
                {"extensions": [".pdf", ".docx", ".pptx"], "engine": "docling"},
                {"extensions": [".txt", ".md", ".py"], "engine": "builtin"}
            ],
            "fallback": "builtin"
        }
    }

    # === Diff section (new) ===
    new_config["diff"] = {
        "engines": {
            "bdiff": {
                "enabled": True,
                "is_default": True,
                "algorithm": "bsdiff"
            },
            "text": {
                "enabled": True,
                "algorithm": "unified",
                "context_lines": 3
            }
        },
        "_router": {
            "rules": [
                {"extensions": [".txt", ".md", ".py", ".json"], "engine": "text"},
                {"mime_prefix": "text/", "engine": "text"}
            ],
            "fallback": "bdiff"
        }
    }

    # === Related section (from old similarity) ===
    old_similarity = old_config.get("similarity", {})

    new_config["related"] = {
        "engines": {
            "embedding": {
                "enabled": old_similarity.get("enabled", True),
                "is_default": True,
                "model": old_similarity.get("model", "all-MiniLM-L6-v2"),
                "min_chars": old_similarity.get("min_chars", 10),
                "max_chars": old_similarity.get("max_chars", 200000),
                "chunk_chars": old_similarity.get("chunk_chars", 1500),
                "chunk_overlap": old_similarity.get("chunk_overlap", 200),
                "offline": old_similarity.get("offline", True),
                # Use old mongo settings if they exist
                "database": old_similarity.get("database",
                                               old_mongo.get("space_database", "wks_similarity")),
                "collection": old_similarity.get("collection",
                                                 old_mongo.get("space_collection", "file_embeddings"))
            },
            "diff_based": {
                "enabled": False,
                "threshold": 0.7,
                "database": "wks_similarity",
                "collection": "diff_similarity"
            }
        },
        "_router": {
            "default": "embedding",
            "rules": [
                {"priority_min": 50, "engine": "embedding"}
            ]
        }
    }

    # === Index section ===
    new_config["index"] = {
        "indices": {
            "main": {
                "enabled": True,
                "type": "embedding",
                "include_extensions": old_similarity.get("include_extensions", [
                    ".md", ".txt", ".py", ".ipynb", ".tex",
                    ".docx", ".pptx", ".pdf", ".html",
                    ".csv", ".xlsx"
                ]),
                "respect_monitor_ignores": old_similarity.get("respect_monitor_ignores", False),
                "respect_priority": True,
                "database": old_mongo.get("space_database", "wks_index_main"),
                "collection": old_mongo.get("space_collection", "documents")
            },
            "code": {
                "enabled": False,
                "type": "ast",
                "include_extensions": [".py", ".js", ".ts", ".cpp"],
                "database": "wks_index_code",
                "collection": "code_blocks"
            }
        }
    }

    # === Search section (new) ===
    new_config["search"] = {
        "default_index": "main",
        "combine": {
            "enabled": False,
            "indices": ["main", "code"],
            "weights": {
                "main": 0.7,
                "code": 0.3
            }
        }
    }

    # === Display section ===
    old_display = old_config.get("display", {})

    new_config["display"] = {
        "timestamp_format": old_display.get("timestamp_format", "%Y-%m-%d %H:%M:%S")
    }

    # === Activity section (keep for now) ===
    if "activity" in old_config:
        new_config["activity"] = copy.deepcopy(old_config["activity"])

    # === Metrics section (keep for now) ===
    if "metrics" in old_config:
        new_config["metrics"] = copy.deepcopy(old_config["metrics"])

    return new_config


def validate_config(config: Dict[str, Any]) -> tuple[bool, list[str]]:
    """Validate config structure.

    Args:
        config: Config dict to validate

    Returns:
        Tuple of (is_valid, list_of_error_messages)
    """
    errors = []

    # Check for required top-level sections
    required_sections = ["monitor", "vault", "db"]
    for section in required_sections:
        if section not in config:
            errors.append(f"Missing required section: {section}")

    # Validate monitor section
    if "monitor" in config:
        monitor = config["monitor"]
        if "managed_directories" not in monitor:
            errors.append("monitor section missing 'managed_directories'")
        if "priority" not in monitor:
            errors.append("monitor section missing 'priority'")

    # Validate vault section
    if "vault" in config:
        vault = config["vault"]
        if "base_dir" not in vault:
            errors.append("vault section missing 'base_dir'")
        if "database" not in vault:
            errors.append("vault section missing 'database'")

    # Validate db section
    if "db" in config:
        db = config["db"]
        if "uri" not in db:
            errors.append("db section missing 'uri'")

    return (len(errors) == 0, errors)
</file>

<file path="wks/db_helpers.py">
"""Common database helper functions for WKS CLI.

Reduces duplication across monitor and db commands.
"""

from typing import Dict, Tuple
from pymongo import MongoClient


def parse_database_key(db_key: str) -> Tuple[str, str]:
    """Parse database key like 'wks.monitor' into (database, collection).

    Args:
        db_key: Database key in format "database.collection"

    Returns:
        Tuple of (database_name, collection_name)

    Raises:
        ValueError: If format is not "database.collection"
    """
    if "." not in db_key:
        raise ValueError(f"Database key must be in format 'database.collection' (found: {db_key!r}, expected: format like 'wks.monitor')")
    parts = db_key.split(".", 1)
    if len(parts) != 2 or not parts[0] or not parts[1]:
        raise ValueError(f"Database key must be in format 'database.collection' (found: {db_key!r}, expected: format like 'wks.monitor' with both parts non-empty)")
    return parts[0], parts[1]


def get_monitor_db_config(cfg: dict) -> Tuple[str, str, str]:
    """Extract monitor database configuration.

    Args:
        cfg: Configuration dictionary

    Returns:
        Tuple of (uri, db_name, coll_name)

    Raises:
        ValueError: If database key is missing or not in "database.collection" format
        KeyError: If required config sections are missing
    """
    monitor_config = cfg.get("monitor")
    if not monitor_config:
        raise KeyError("monitor section is required in config (found: missing, expected: monitor section with database, include_paths, etc.)")

    db_config = cfg.get("db")
    if not db_config:
        raise KeyError("db section is required in config (found: missing, expected: db section with type and uri)")

    uri = db_config.get("uri")
    if not uri:
        raise KeyError("db.uri is required in config (found: missing, expected: MongoDB connection URI string)")

    db_key = monitor_config.get("database")
    if not db_key:
        raise KeyError("monitor.database is required in config (found: missing, expected: 'database.collection' format, e.g., 'wks.monitor')")

    db_name, coll_name = parse_database_key(db_key)

    return uri, db_name, coll_name


def connect_to_mongo(uri: str, timeout_ms: int = 5000) -> MongoClient:
    """Connect to MongoDB with timeout.

    Args:
        uri: MongoDB connection URI
        timeout_ms: Server selection timeout in milliseconds

    Returns:
        Connected MongoClient

    Raises:
        Exception: If connection fails
    """
    client = MongoClient(uri, serverSelectionTimeoutMS=timeout_ms)
    client.server_info()  # Test connection
    return client
</file>

<file path=".gitignore">
# Virtual environment
_requests
**/.wkso
venv/
.venv/
*.pyc
**/__pycache__/
.coverage

# IDE
.vscode/
.idea/

# OS
.DS_Store

# Distribution
*.egg-info/
dist/
build/

# Logs
*.log
</file>

<file path="tests/test_space_db.py">
import os
from pathlib import Path

import mongomock
import pytest


def dummy_encode(text: str):
    # Deterministic small vector based on text content
    import hashlib
    h = hashlib.sha256(text.encode('utf-8')).digest()
    # 8 floats in [0,1)
    return [b/255.0 for b in h[:8]]


@pytest.fixture(autouse=True)
def patch_mongo_and_model(monkeypatch):
    # Patch MongoClient to in-memory mongomock
    monkeypatch.setattr('wks.similarity.MongoClient', mongomock.MongoClient)

    # Patch SentenceTransformer to a dummy with encode()
    class DummyModel:
        def __init__(self, *a, **k):
            pass
        def encode(self, text):
            return dummy_encode(text)

    monkeypatch.setattr('wks.similarity.SentenceTransformer', DummyModel)


@pytest.fixture()
def tmpdir(tmp_path):
    return tmp_path


def make_file(p: Path, content: str):
    p.parent.mkdir(parents=True, exist_ok=True)
    # Ensure enough content length for embedding path (>=10 chars)
    if len(content) < 12:
        content = content * ((12 // max(1, len(content))) + 1)
    p.write_text(content, encoding='utf-8')
    return p


def test_index_and_stats_and_move(tmpdir, monkeypatch):
    from wks.similarity import SimilarityDB

    # Build DB with builtin extractor to avoid heavy dependencies during tests
    db = SimilarityDB(
        database_name='testdb',
        collection_name='file_embeddings',
        mongo_uri='mongodb://localhost:27017/',
        model_name='dummy',
        extract_engine='builtin',
        extract_ocr=False,
        extract_timeout_secs=5,
    )

    src = make_file(tmpdir / 'src' / 'file.txt', 'alpha')

    # Index
    updated = db.add_file(src)
    assert updated is True
    stats = db.get_stats()
    assert stats['total_files'] == 1

    # Move (daemon-like path update)
    dst = tmpdir / 'dst' / 'file.txt'
    dst.parent.mkdir(parents=True, exist_ok=True)
    os.rename(src, dst)
    # Simulate daemon move handling
    db.rename_file(src, dst)
    stats2 = db.get_stats()
    assert stats2['total_files'] == 1  # no duplicate
    doc = db.collection.find_one({'path': dst.resolve().as_uri()})
    assert doc is not None
    assert doc['filename'] == dst.name


def test_index_time_rename_detection(tmpdir):
    from wks.similarity import SimilarityDB

    db = SimilarityDB(
        database_name='testdb2',
        collection_name='file_embeddings',
        mongo_uri='mongodb://localhost:27017/',
        model_name='dummy',
        extract_engine='builtin',
        extract_ocr=False,
        extract_timeout_secs=5,
    )

    old = make_file(tmpdir / 'old' / 'a.txt', 'beta')
    assert db.add_file(old) is True
    # Move the file physically without calling rename_file (daemon absent in this scenario)
    new = tmpdir / 'new' / 'a.txt'
    new.parent.mkdir(parents=True, exist_ok=True)
    os.rename(old, new)
    # Now indexing the new path should detect same checksum and treat as rename (old path gone)
    assert db.add_file(new) is True
    stats = db.get_stats()
    assert stats['total_files'] == 1
    assert db.collection.find_one({'path': new.resolve().as_uri()}) is not None
    assert db.collection.find_one({'path': old.resolve().as_uri()}) is None


def test_rename_folder_updates_descendants(tmpdir):
    from wks.similarity import SimilarityDB

    db = SimilarityDB(
        database_name='testdb3',
        collection_name='file_embeddings',
        mongo_uri='mongodb://localhost:27017/',
        model_name='dummy',
        extract_engine='builtin',
        extract_ocr=False,
        extract_timeout_secs=5,
    )

    base = tmpdir / 'dir'
    f1 = make_file(base / 'a.txt', 'one')
    f2 = make_file(base / 'sub' / 'b.txt', 'two')
    assert db.add_file(f1)
    assert db.add_file(f2)
    # Move the directory
    new_base = tmpdir / 'dir-moved'
    os.rename(base, new_base)
    # Update DB paths in-place
    updated = db.rename_folder(base, new_base)
    assert updated >= 2
    # Verify records now reference moved paths
    assert db.collection.find_one({'path': (new_base / 'a.txt').resolve().as_uri()})
    assert db.collection.find_one({'path': (new_base / 'sub' / 'b.txt').resolve().as_uri()})


def test_similarity_audit_removes_missing_file(tmpdir):
    from wks.similarity import SimilarityDB

    db = SimilarityDB(
        database_name='audit_remove',
        collection_name='audit_remove_coll',
        mongo_uri='mongodb://localhost:27017/',
        model_name='dummy',
        extract_engine='builtin',
    )

    missing = (tmpdir / 'missing.txt').resolve()
    db.collection.insert_one({
        'path': f"file://{missing.as_posix()}",
        'path_local': str(missing),
        'bytes': None,
    })

    summary = db.audit_documents()
    assert summary['removed'] == 1
    assert db.collection.count_documents({}) == 0


def test_similarity_audit_fills_missing_bytes(tmpdir):
    from wks.similarity import SimilarityDB

    db = SimilarityDB(
        database_name='audit_fix',
        collection_name='audit_fix_coll',
        mongo_uri='mongodb://localhost:27017/',
        model_name='dummy',
        extract_engine='builtin',
    )

    file_path = (tmpdir / 'existing.txt').resolve()
    file_path.parent.mkdir(parents=True, exist_ok=True)
    file_path.write_text('hello index audit', encoding='utf-8')

    db.collection.insert_one({
        'path': f"file://{file_path.as_posix()}",
        'path_local': str(file_path),
        'bytes': None,
    })

    summary = db.audit_documents()
    doc = db.collection.find_one({'path_local': str(file_path)})
    assert summary['updated'] >= 1
    assert doc['bytes'] == file_path.stat().st_size


def test_similarity_audit_handles_plain_paths(tmpdir):
    from wks.similarity import SimilarityDB

    db = SimilarityDB(
        database_name='audit_fix_plain',
        collection_name='audit_fix_plain_coll',
        mongo_uri='mongodb://localhost:27017/',
        model_name='dummy',
        extract_engine='builtin',
    )

    file_path = (tmpdir / 'plain_path.txt').resolve()
    file_path.parent.mkdir(parents=True, exist_ok=True)
    file_path.write_text('plain path bytes', encoding='utf-8')

    db.collection.insert_one({
        'path': str(file_path),  # stored without file:// prefix
        'bytes': None,
    })

    summary = db.audit_documents()
    doc = db.collection.find_one({'path': str(file_path)})
    assert summary['updated'] >= 1
    assert doc['bytes'] == file_path.stat().st_size
</file>

<file path="wks/config_validator.py">
"""Configuration validation for WKS."""

from pathlib import Path
from typing import Any, Dict, List, Tuple


class ConfigValidationError(Exception):
    """Raised when configuration is invalid."""
    pass


def _validate_database_key(db_key: Any, section: str) -> List[str]:
    """Validate database key format (database.collection)."""
    errors = []
    if not db_key:
        errors.append(f"{section}.database is required (found: {db_key!r}, expected: 'database.collection' format, e.g., 'wks.monitor')")
    elif not isinstance(db_key, str):
        errors.append(f"{section}.database must be a string (found: {type(db_key).__name__} = {db_key!r}, expected: string in 'database.collection' format)")
    elif "." not in db_key:
        errors.append(f"{section}.database must be in format 'database.collection' (found: {db_key!r}, expected: format like 'wks.monitor')")
    else:
        parts = db_key.split(".", 1)
        if len(parts) != 2 or not parts[0] or not parts[1]:
            errors.append(f"{section}.database must be in format 'database.collection' (found: {db_key!r}, expected: format like 'wks.monitor' with both parts non-empty)")
    return errors


def _validate_vault_config(vault: Dict[str, Any]) -> List[str]:
    """Validate vault configuration section."""
    errors = []
    if not isinstance(vault, dict):
        errors.append("'vault' must be an object")
        return errors

    if "base_dir" not in vault:
        errors.append("vault.base_dir is required (found: missing, expected: path to Obsidian vault directory)")
    elif vault.get("base_dir"):
        vault_path = Path(str(vault["base_dir"])).expanduser()
        if not vault_path.exists():
            errors.append(f"vault.base_dir does not exist (found: {vault_path!r}, expected: existing directory path)")
        elif not vault_path.is_dir():
            errors.append(f"vault.base_dir is not a directory (found: {vault_path!r} is a file, expected: directory path)")

    if "wks_dir" not in vault:
        errors.append("vault.wks_dir is required (found: missing, expected: subdirectory name within vault, e.g., 'WKS')")

    if "update_frequency_seconds" not in vault:
        errors.append("vault.update_frequency_seconds is required (found: missing, expected: positive integer)")
    else:
        try:
            val = int(vault["update_frequency_seconds"])
            if val < 1:
                errors.append(f"vault.update_frequency_seconds must be positive (found: {val}, expected: integer >= 1)")
        except (ValueError, TypeError):
            errors.append(f"vault.update_frequency_seconds must be an integer (found: {type(vault['update_frequency_seconds']).__name__} = {vault['update_frequency_seconds']!r}, expected: integer)")

    if "database" in vault:
        errors.extend(_validate_database_key(vault["database"], "vault"))

    return errors


def _validate_monitor_config(mon: Dict[str, Any]) -> List[str]:
    """Validate monitor configuration section."""
    errors = []
    if not isinstance(mon, dict):
        errors.append("'monitor' must be an object")
        return errors

    if "database" in mon:
        errors.extend(_validate_database_key(mon["database"], "monitor"))

    required_mon = ["include_paths", "exclude_paths", "ignore_dirnames", "ignore_globs", "touch_weight"]
    for key in required_mon:
        if key not in mon:
            errors.append(f"monitor.{key} is required")

    if "include_paths" in mon:
        if not isinstance(mon["include_paths"], list):
            errors.append(f"monitor.include_paths must be an array (found: {type(mon['include_paths']).__name__} = {mon['include_paths']!r}, expected: list of path strings)")
        else:
            for idx, path_str in enumerate(mon["include_paths"]):
                path = Path(str(path_str)).expanduser()
                if not path.exists():
                    errors.append(f"monitor.include_paths[{idx}] does not exist (found: {path!r}, expected: existing directory path)")

    if "touch_weight" in mon:
        try:
            weight_val = float(mon.get("touch_weight"))
        except (TypeError, ValueError):
            errors.append(
                f"monitor.touch_weight must be a number between 0.001 and 1 (found: {
                    type(
                        mon.get('touch_weight')).__name__} = {
                    mon.get('touch_weight')!r}, expected: float between 0.001 and 1.0)")
        else:
            if weight_val < 0.001 or weight_val > 1.0:
                errors.append(f"monitor.touch_weight must be between 0.001 and 1 (found: {weight_val}, expected: 0.001 <= value <= 1.0)")

    return errors


def _validate_extract_config(ext: Dict[str, Any]) -> List[str]:
    """Validate extract configuration section."""
    errors = []
    if not isinstance(ext, dict):
        return errors

    if "engine" in ext:
        engine = str(ext["engine"]).lower()
        if engine not in ["docling", "builtin"]:
            errors.append(f"extract.engine must be 'docling' or 'builtin' (found: {ext['engine']!r}, expected: 'docling' or 'builtin')")

    if "timeout_secs" in ext:
        try:
            val = int(ext["timeout_secs"])
            if val < 1:
                errors.append(f"extract.timeout_secs must be positive (found: {val}, expected: integer >= 1)")
        except (ValueError, TypeError):
            errors.append(f"extract.timeout_secs must be an integer (found: {type(ext['timeout_secs']).__name__} = {ext['timeout_secs']!r}, expected: integer)")

    return errors


def _validate_db_config(db: Dict[str, Any]) -> List[str]:
    """Validate db configuration section."""
    errors = []
    if not db:
        errors.append("db section is required")
        return errors

    if not isinstance(db, dict):
        errors.append("'db' must be an object")
        return errors

    db_type = db.get("type")
    if not db_type:
        errors.append("db.type is required (found: missing, expected: 'mongodb')")
    elif db_type != "mongodb":
        errors.append(f"db.type must be 'mongodb' (found: {db_type!r}, expected: 'mongodb' - only supported type)")

    if "uri" not in db:
        errors.append("db.uri is required (found: missing, expected: MongoDB connection URI)")
    elif db_type == "mongodb":
        uri = str(db["uri"])
        if not uri.startswith("mongodb://"):
            errors.append(f"db.uri must start with 'mongodb://' when db.type is 'mongodb' (found: {uri[:20]}..., expected: URI starting with 'mongodb://')")

    return errors


def validate_config(cfg: Dict[str, Any]) -> List[str]:
    """
    Validate WKS configuration.

    Returns:
        List of error messages (empty if valid)
    """
    errors = []
    errors.extend(_validate_vault_config(cfg.get("vault", {})))
    errors.extend(_validate_monitor_config(cfg.get("monitor", {})))
    errors.extend(_validate_extract_config(cfg.get("extract", {})))
    errors.extend(_validate_db_config(cfg.get("db")))
    return errors


def validate_and_raise(cfg: Dict[str, Any]) -> None:
    """
    Validate configuration and raise ConfigValidationError if invalid.

    Args:
        cfg: Configuration dictionary

    Raises:
        ConfigValidationError: If configuration is invalid
    """
    errors = validate_config(cfg)
    if errors:
        msg = "Configuration validation failed:\n" + "\n".join(f"  - {e}" for e in errors)
        raise ConfigValidationError(msg)
</file>

<file path="wks/mongoctl.py">
"""
Helpers for managing the local MongoDB process that backs wks0.

Shared across the CLI and daemon so launchd/autostart paths behave
the same as manual CLI invocations.
"""

from __future__ import annotations

import os
import shutil
import signal
import subprocess
import threading
import time
from pathlib import Path
from typing import Optional

from pymongo.uri_parser import parse_uri as _parse_mongo_uri

import pymongo

from .utils import wks_home_path

MONGO_ROOT = wks_home_path("mongodb")
MONGO_PID_FILE = MONGO_ROOT / "mongod.pid"
MONGO_MANAGED_FLAG = MONGO_ROOT / "managed"
_LOCAL_URI_HOSTS = {"localhost", "127.0.0.1", "::1"}
MANAGED_MONGO_PORT = 27027


def pid_running(pid: int) -> bool:
    try:
        os.kill(pid, 0)
        return True
    except Exception:
        return False


def mongo_ping(uri: str, timeout_ms: int = 500) -> bool:
    try:
        client = pymongo.MongoClient(uri, serverSelectionTimeoutMS=timeout_ms, connectTimeoutMS=timeout_ms)
        client.admin.command("ping")
        return True
    except Exception:
        return False


def _local_node(uri: str) -> Optional[tuple[str, int]]:
    """Return the single loopback node defined by the URI, if any."""
    if not uri or uri.startswith("mongodb+srv://"):
        return None

    try:
        parsed = _parse_mongo_uri(uri, validate=False)
    except Exception:
        return None

    nodes = parsed.get("nodelist") or []
    if len(nodes) != 1:
        return None

    host, port = nodes[0]
    if host not in _LOCAL_URI_HOSTS:
        return None
    return host, port


def _is_local_uri(uri: str) -> bool:
    return _local_node(uri) is not None


def _managed_local_node(uri: str) -> Optional[tuple[str, int]]:
    node = _local_node(uri)
    if not node:
        return None
    host, port = node
    if port != MANAGED_MONGO_PORT:
        return None
    return host, port


def ensure_mongo_running(uri: str, *, record_start: bool = False) -> None:
    uri = (uri or "").strip()
    if not uri:
        print("Fatal: MongoDB URI is empty; configure db.uri in config.json")
        raise SystemExit(2)

    if mongo_ping(uri):
        return
    managed_node = _managed_local_node(uri)
    if managed_node and shutil.which("mongod"):
        host, port = managed_node
        bind_ip = "127.0.0.1" if host in ("localhost", "127.0.0.1") else host
        dbroot = MONGO_ROOT
        dbpath = dbroot / "db"
        logfile = dbroot / "mongod.log"
        dbroot.mkdir(parents=True, exist_ok=True)
        dbpath.mkdir(parents=True, exist_ok=True)
        pidfile = MONGO_PID_FILE if record_start else (dbroot / "mongod.pid.tmp")
        proc_pid: Optional[int] = None
        try:
            if pidfile.exists():
                pidfile.unlink()
        except Exception:
            pass
        try:
            with open(logfile, "ab") as log_handle:
                proc = subprocess.Popen(
                    [
                        "mongod",
                        "--dbpath",
                        str(dbpath),
                        "--logpath",
                        str(logfile),
                        "--logappend",
                        "--bind_ip",
                        bind_ip,
                        "--port",
                        str(port),
                    ],
                    stdout=log_handle,
                    stderr=subprocess.STDOUT,
                    start_new_session=True,
                )
            proc_pid = proc.pid
            try:
                pidfile.write_text(str(proc_pid))
            except Exception:
                pass
        except Exception as exc:
            print(f"Fatal: failed to auto-start local mongod: {exc}")
            raise SystemExit(2)
        deadline = time.time() + 5.0
        while time.time() < deadline:
            if mongo_ping(uri, timeout_ms=1000):
                if record_start:
                    if proc_pid:
                        try:
                            MONGO_MANAGED_FLAG.write_text(str(proc_pid))
                        except Exception:
                            pass
                else:
                    try:
                        pidfile.unlink()
                    except Exception:
                        pass
                return
            time.sleep(0.3)
        print(f"Fatal: mongod started but MongoDB still unreachable; check logs in {dbroot}/mongod.log")
        if proc_pid:
            try:
                os.kill(proc_pid, signal.SIGTERM)
            except Exception:
                pass
        raise SystemExit(2)
    print(f"Fatal: MongoDB not reachable at {uri}; start mongod and retry.")
    raise SystemExit(2)


def stop_managed_mongo() -> None:
    if not MONGO_MANAGED_FLAG.exists() or not MONGO_PID_FILE.exists():
        return
    try:
        pid = int(MONGO_MANAGED_FLAG.read_text().strip())
    except Exception:
        pid = None
    try:
        if pid:
            os.kill(pid, signal.SIGTERM)
            for _ in range(20):
                if not pid_running(pid):
                    break
                time.sleep(0.1)
        MONGO_MANAGED_FLAG.unlink(missing_ok=True)
        MONGO_PID_FILE.unlink(missing_ok=True)
    except Exception:
        pass


def create_client(
    uri: str,
    *,
    server_timeout: int = 500,
    connect_timeout: int = 500,
    ensure_running: bool = True,
):
    """Return a pymongo client, optionally ensuring the server is running."""
    if ensure_running:
        try:
            ensure_mongo_running(uri)
        except SystemExit:
            raise
        except Exception:
            pass
    return pymongo.MongoClient(
        uri,
        serverSelectionTimeoutMS=server_timeout,
        connectTimeoutMS=connect_timeout,
    )


class MongoGuard:
    """Background watcher that keeps the managed local MongoDB online."""

    def __init__(self, uri: str, *, ping_interval: float = 10.0):
        self.uri = (uri or "").strip()
        self.ping_interval = max(float(ping_interval), 0.01)
        self._thread: Optional[threading.Thread] = None
        self._stop_event = threading.Event()
        self._manage_local = bool(_managed_local_node(self.uri))

    def start(self, *, record_start: bool = True) -> None:
        if not self.uri:
            return
        ensure_mongo_running(self.uri, record_start=record_start)
        if not self._manage_local:
            return
        if self._thread and self._thread.is_alive():
            return
        self._stop_event.clear()
        thread = threading.Thread(target=self._loop, name="mongo-guard", daemon=True)
        self._thread = thread
        thread.start()

    def stop(self, timeout: float = 5.0) -> None:
        thread = self._thread
        if not thread:
            return
        self._stop_event.set()
        try:
            thread.join(timeout)
        except Exception:
            pass
        self._thread = None
        self._stop_event = threading.Event()

    def is_running(self) -> bool:
        thread = self._thread
        return bool(thread and thread.is_alive())

    def _loop(self) -> None:
        while not self._stop_event.wait(self.ping_interval):
            if mongo_ping(self.uri, timeout_ms=1000):
                continue
            try:
                ensure_mongo_running(self.uri, record_start=True)
                print("Mongo guard: restarted local mongod after ping failure")
            except SystemExit:
                # Service startup already handles fatal exit; guard keeps trying
                continue
            except Exception:
                continue
</file>

<file path="wks/monitor_controller.py">
"""
Monitor Controller - Business logic for filesystem monitoring operations.

This module contains all monitor-related business logic, completely separated
from the CLI display layer. This enables:
- Easy unit testing
- Reuse by MCP server
- Zero code duplication per SPEC.md
"""

from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import fnmatch
import math


class ValidationError(Exception):
    """Exception that collects multiple validation errors."""

    def __init__(self, errors: List[str]):
        self.errors = errors
        message = "Validation failed with multiple errors:\n" + "\n".join(f"  - {e}" for e in errors)
        super().__init__(message)


class MonitorValidator:
    """Validation logic for monitor configuration."""

    @staticmethod
    def status_symbol(error_msg: Optional[str], is_valid: bool = True) -> str:
        """Convert validation result to colored status symbol."""
        return "[green]‚úì[/]" if not error_msg else "[yellow]‚ö†[/]" if is_valid else "[red]‚úó[/]"

    @staticmethod
    def validate_ignore_dirname(dirname: str, ignore_globs: List[str]) -> Tuple[bool, Optional[str]]:
        """Validate an ignore_dirname entry."""
        if '*' in dirname or '?' in dirname or '[' in dirname:
            return False, "ignore_dirnames cannot contain wildcard characters (*, ?, [). Use ignore_globs for patterns."

        for glob_pattern in ignore_globs:
            if fnmatch.fnmatch(dirname, glob_pattern):
                return True, f"Redundant: dirname '{dirname}' already matched by ignore_globs pattern '{glob_pattern}'"

        return True, None

    @staticmethod
    def validate_ignore_glob(pattern: str) -> Tuple[bool, Optional[str]]:
        """Validate an ignore_glob pattern for syntax errors."""
        try:
            fnmatch.fnmatch("test", pattern)
            return True, None
        except Exception as e:
            return False, f"Invalid glob syntax: {str(e)}"

    @staticmethod
    def validate_managed_directory(
        managed_path: str,
        include_paths: List[str],
        exclude_paths: List[str],
        ignore_dirnames: List[str],
        ignore_globs: List[str]
    ) -> Tuple[bool, Optional[str]]:
        """Validate that a managed_directory would actually be monitored."""
        managed_resolved = Path(managed_path).expanduser().resolve()

        # Check for system paths that are always ignored
        wks_home = Path("~/.wks").expanduser().resolve()
        if managed_resolved == wks_home or str(managed_resolved).startswith(str(wks_home) + "/"):
            return False, "In WKS home directory (automatically ignored)"

        if ".wkso" in managed_resolved.parts:
            return False, "Contains .wkso directory (automatically ignored)"

        # Check if under any include_paths
        if not any(managed_resolved.is_relative_to(Path(p).expanduser().resolve())
                   for p in include_paths):
            return False, "Not under any include_paths"

        # Check if in exclude_paths
        for exclude_path in exclude_paths:
            try:
                if managed_resolved.is_relative_to(Path(exclude_path).expanduser().resolve()):
                    return False, f"Matched by exclude_paths: {exclude_path}"
            except BaseException:
                pass

        # Check if any path component matches ignore_dirnames
        for part in managed_resolved.parts:
            if part in ignore_dirnames:
                return False, f"Contains ignored dirname: {part}"

        # Check if matches ignore_globs
        for glob_pattern in ignore_globs:
            if fnmatch.fnmatch(str(managed_resolved), glob_pattern) or \
               fnmatch.fnmatch(managed_resolved.name, glob_pattern):
                return False, f"Matched by ignore_globs: {glob_pattern}"

        return True, None


@dataclass
class ListOperationResult:
    """Result of adding/removing items from a monitor list."""
    success: bool
    message: str
    value_stored: Optional[str] = None
    value_removed: Optional[str] = None
    not_found: bool = False
    already_exists: bool = False
    validation_failed: bool = False

    def __post_init__(self):
        """Validate after initialization."""
        if not self.message:
            raise ValueError(f"ListOperationResult.message cannot be empty (found: {self.message!r}, expected: non-empty string)")
        if self.success and self.not_found:
            raise ValueError(
                f"ListOperationResult: success cannot be True when not_found is True (found: success={
                    self.success}, not_found={
                    self.not_found}, expected: success=False when not_found=True)")
        if self.success and self.already_exists:
            raise ValueError(
                f"ListOperationResult: success cannot be True when already_exists is True (found: success={
                    self.success}, already_exists={
                    self.already_exists}, expected: success=False when already_exists=True)")
        if self.success and self.validation_failed:
            raise ValueError(
                f"ListOperationResult: success cannot be True when validation_failed is True (found: success={
                    self.success}, validation_failed={
                    self.validation_failed}, expected: success=False when validation_failed=True)")


@dataclass
class ManagedDirectoryInfo:
    """Information about a managed directory."""
    priority: int
    valid: bool
    error: Optional[str] = None

    def __post_init__(self):
        """Validate after initialization."""
        if self.priority < 0:
            raise ValueError(f"ManagedDirectoryInfo.priority must be non-negative (found: {self.priority}, expected: integer >= 0)")


@dataclass
class ManagedDirectoriesResult:
    """Result of get_managed_directories()."""
    managed_directories: Dict[str, int]
    count: int
    validation: Dict[str, ManagedDirectoryInfo]


@dataclass
class MonitorConfig:
    """Monitor configuration loaded from config dict with validation."""
    include_paths: List[str]
    exclude_paths: List[str]
    ignore_dirnames: List[str]
    ignore_globs: List[str]
    managed_directories: Dict[str, int]
    database: str
    touch_weight: float = 0.1
    priority: Dict[str, Any] = field(default_factory=dict)
    max_documents: int = 1000000
    prune_interval_secs: float = 300.0

    def __post_init__(self):
        """Validate monitor configuration after initialization.

        Collects all validation errors and raises a single ValidationError
        with all errors, so the user can see everything that needs fixing.
        """
        errors = []

        # Validate required fields are present and correct types
        if not isinstance(self.include_paths, list):
            errors.append(f"monitor.include_paths must be a list (found: {type(self.include_paths).__name__} = {self.include_paths!r}, expected: list)")

        if not isinstance(self.exclude_paths, list):
            errors.append(f"monitor.exclude_paths must be a list (found: {type(self.exclude_paths).__name__} = {self.exclude_paths!r}, expected: list)")

        if not isinstance(self.ignore_dirnames, list):
            errors.append(f"monitor.ignore_dirnames must be a list (found: {type(self.ignore_dirnames).__name__} = {self.ignore_dirnames!r}, expected: list)")

        if not isinstance(self.ignore_globs, list):
            errors.append(f"monitor.ignore_globs must be a list (found: {type(self.ignore_globs).__name__} = {self.ignore_globs!r}, expected: list)")

        if not isinstance(self.managed_directories, dict):
            errors.append(f"monitor.managed_directories must be a dict (found: {type(self.managed_directories).__name__} = {self.managed_directories!r}, expected: dict)")

        if not isinstance(self.database, str) or "." not in self.database:
            errors.append(f"monitor.database must be in format 'database.collection' (found: {self.database!r}, expected: format like 'wks.monitor')")
        elif isinstance(self.database, str):
            parts = self.database.split(".", 1)
            if len(parts) != 2 or not parts[0] or not parts[1]:
                errors.append(f"monitor.database must be in format 'database.collection' (found: {self.database!r}, expected: format like 'wks.monitor' with both parts non-empty)")

        if not isinstance(self.touch_weight, (int, float)) or self.touch_weight < 0.001 or self.touch_weight > 1.0:
            errors.append(f"monitor.touch_weight must be a number between 0.001 and 1 (found: {type(self.touch_weight).__name__} = {self.touch_weight!r}, expected: float between 0.001 and 1.0)")

        if not isinstance(self.max_documents, int) or self.max_documents < 0:
            errors.append(f"monitor.max_documents must be a non-negative integer (found: {type(self.max_documents).__name__} = {self.max_documents!r}, expected: integer >= 0)")

        if not isinstance(self.prune_interval_secs, (int, float)) or self.prune_interval_secs <= 0:
            errors.append(f"monitor.prune_interval_secs must be a positive number (found: {type(self.prune_interval_secs).__name__} = {self.prune_interval_secs!r}, expected: float > 0)")

        if errors:
            raise ValidationError(errors)

    @classmethod
    def from_config_dict(cls, config: dict) -> "MonitorConfig":
        """Load monitor config from config dict.

        Raises:
            KeyError: If monitor section is missing
            ValidationError: If field values are invalid (contains all validation errors)
        """
        monitor_config = config.get("monitor")
        if not monitor_config:
            raise KeyError("monitor section is required in config (found: missing, expected: monitor section with include_paths, exclude_paths, etc.)")

        # Use natural dataclass instantiation from dict
        return cls(**monitor_config)


@dataclass
class ConfigValidationResult:
    """Result of validate_config()."""
    issues: List[str] = field(default_factory=list)
    redundancies: List[str] = field(default_factory=list)
    managed_directories: Dict[str, ManagedDirectoryInfo] = field(default_factory=dict)
    include_paths: List[str] = field(default_factory=list)
    exclude_paths: List[str] = field(default_factory=list)
    ignore_dirnames: List[str] = field(default_factory=list)
    ignore_globs: List[str] = field(default_factory=list)
    ignore_dirname_validation: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    ignore_glob_validation: Dict[str, Dict[str, Any]] = field(default_factory=dict)


@dataclass
class MonitorStatus:
    """Monitor status data structure."""
    tracked_files: int
    issues: List[str] = field(default_factory=list)
    redundancies: List[str] = field(default_factory=list)
    managed_directories: Dict[str, ManagedDirectoryInfo] = field(default_factory=dict)
    include_paths: List[str] = field(default_factory=list)
    exclude_paths: List[str] = field(default_factory=list)
    ignore_dirnames: List[str] = field(default_factory=list)
    ignore_globs: List[str] = field(default_factory=list)
    ignore_dirname_validation: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    ignore_glob_validation: Dict[str, Dict[str, Any]] = field(default_factory=dict)

    def __post_init__(self):
        """Validate after initialization."""
        if self.tracked_files < 0:
            raise ValueError(f"MonitorStatus.tracked_files must be non-negative (found: {self.tracked_files}, expected: integer >= 0)")

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dict for JSON serialization using dataclasses.asdict."""
        return asdict(self)


class MonitorController:
    """Controller for monitor operations - returns data structures for any view."""

    @staticmethod
    def get_list(config: dict, list_name: str) -> dict:
        """Get contents of a monitor config list.

        Args:
            config: Configuration dictionary
            list_name: Name of list (include_paths, exclude_paths, ignore_dirnames, ignore_globs)

        Returns:
            dict with 'list_name', 'items' (list), and optional validation info

        Raises:
            KeyError: If monitor section or list_name is missing
        """
        monitor_cfg = MonitorConfig.from_config_dict(config)

        if list_name == "include_paths":
            items = monitor_cfg.include_paths
        elif list_name == "exclude_paths":
            items = monitor_cfg.exclude_paths
        elif list_name == "ignore_dirnames":
            items = monitor_cfg.ignore_dirnames
        elif list_name == "ignore_globs":
            items = monitor_cfg.ignore_globs
        else:
            raise ValueError(f"Unknown list_name: {list_name} (found: {list_name!r}, expected: include_paths, exclude_paths, ignore_dirnames, or ignore_globs)")

        result = {
            "list_name": list_name,
            "items": items,
            "count": len(items)
        }

        # Add validation for ignore_dirnames
        if list_name == "ignore_dirnames":
            validation = {}
            for dirname in items:
                is_valid, error_msg = MonitorValidator.validate_ignore_dirname(dirname, monitor_cfg.ignore_globs)
                validation[dirname] = {"valid": is_valid, "error": error_msg}
            result["validation"] = validation

        # Add validation for ignore_globs
        elif list_name == "ignore_globs":
            validation = {}
            for pattern in items:
                is_valid, error_msg = MonitorValidator.validate_ignore_glob(pattern)
                validation[pattern] = {"valid": is_valid, "error": error_msg}
            result["validation"] = validation

        return result

    @staticmethod
    def add_to_list(config_dict: dict, list_name: str, value: str, resolve_path: bool = True) -> ListOperationResult:
        """Add value to a monitor config list.

        Args:
            config_dict: Configuration dictionary (will be modified)
            list_name: Name of list to modify
            value: Value to add
            resolve_path: Whether to resolve paths (for include/exclude_paths)

        Returns:
            dict with 'success' (bool), 'message' (str), 'value_stored' (str if success)
        """
        # Ensure monitor section exists
        if "monitor" not in config_dict:
            config_dict["monitor"] = {}
        if list_name not in config_dict["monitor"]:
            config_dict["monitor"][list_name] = []

        # Normalize value if needed
        if resolve_path:
            value_resolved = str(Path(value).expanduser().resolve())
            # Preserve tilde notation if in home directory
            home_dir = str(Path.home())
            if value_resolved.startswith(home_dir):
                value_to_store = "~" + value_resolved[len(home_dir):]
            else:
                value_to_store = value_resolved
        else:
            value_resolved = value
            value_to_store = value

        # Check if already exists
        existing = None
        for item in config_dict["monitor"][list_name]:
            if resolve_path:
                item_resolved = str(Path(item).expanduser().resolve())
                if item_resolved == value_resolved:
                    existing = item
                    break
            else:
                if item == value:
                    existing = item
                    break

        if existing:
            return ListOperationResult(
                success=False,
                message=f"Already in {list_name}: {existing}",
                already_exists=True
            )

        # Validate ignore_dirnames before adding
        if list_name == "ignore_dirnames":
            monitor_cfg = MonitorConfig.from_config_dict(config_dict)
            is_valid, error_msg = MonitorValidator.validate_ignore_dirname(value, monitor_cfg.ignore_globs)
            if not is_valid:
                return ListOperationResult(
                    success=False,
                    message=error_msg,
                    validation_failed=True
                )

        # Add to list
        config_dict["monitor"][list_name].append(value_to_store)
        return ListOperationResult(
            success=True,
            message=f"Added to {list_name}: {value_to_store}",
            value_stored=value_to_store
        )

    @staticmethod
    def remove_from_list(config_dict: dict, list_name: str, value: str, resolve_path: bool = True) -> ListOperationResult:
        """Remove value from a monitor config list.

        Args:
            config_dict: Configuration dictionary (will be modified)
            list_name: Name of list to modify
            value: Value to remove
            resolve_path: Whether to resolve paths (for include/exclude_paths)

        Returns:
            dict with 'success' (bool), 'message' (str), 'value_removed' (str if success)
        """
        if "monitor" not in config_dict or list_name not in config_dict["monitor"]:
            return ListOperationResult(
                success=False,
                message=f"No {list_name} configured",
                not_found=True
            )

        # Find matching entry
        existing = None
        if resolve_path:
            value_resolved = str(Path(value).expanduser().resolve())
            for item in config_dict["monitor"][list_name]:
                item_resolved = str(Path(item).expanduser().resolve())
                if item_resolved == value_resolved:
                    existing = item
                    break
        else:
            if value in config_dict["monitor"][list_name]:
                existing = value

        if not existing:
            return ListOperationResult(
                success=False,
                message=f"Not in {list_name}: {value}",
                not_found=True
            )

        # Remove from list
        config_dict["monitor"][list_name].remove(existing)
        return ListOperationResult(
            success=True,
            message=f"Removed from {list_name}: {existing}",
            value_removed=existing
        )

    @staticmethod
    def get_managed_directories(config: dict) -> ManagedDirectoriesResult:
        """Get managed directories with their priorities.

        Args:
            config: Configuration dictionary

        Returns:
            dict with 'managed_directories' (dict of path->priority), 'count', and validation info

        Raises:
            KeyError: If monitor section or required fields are missing
        """
        monitor_cfg = MonitorConfig.from_config_dict(config)

        # Validate each managed directory
        validation = {}
        for path, priority in monitor_cfg.managed_directories.items():
            is_valid, error_msg = MonitorValidator.validate_managed_directory(
                path, monitor_cfg.include_paths, monitor_cfg.exclude_paths,
                monitor_cfg.ignore_dirnames, monitor_cfg.ignore_globs
            )
            validation[path] = ManagedDirectoryInfo(
                priority=priority,
                valid=is_valid,
                error=error_msg
            )

        return ManagedDirectoriesResult(
            managed_directories=monitor_cfg.managed_directories,
            count=len(monitor_cfg.managed_directories),
            validation=validation
        )

    @staticmethod
    def add_managed_directory(config_dict: dict, path: str, priority: int) -> dict:
        """Add a managed directory with priority.

        Args:
            config_dict: Configuration dictionary (will be modified)
            path: Directory path to add
            priority: Priority score

        Returns:
            dict with 'success' (bool), 'message' (str), 'path_stored' (str if success)
        """
        # Ensure sections exist
        if "monitor" not in config_dict:
            config_dict["monitor"] = {}
        if "managed_directories" not in config_dict["monitor"]:
            config_dict["monitor"]["managed_directories"] = {}

        # Resolve path
        path_resolved = str(Path(path).expanduser().resolve())

        # Check if already exists
        if path_resolved in config_dict["monitor"]["managed_directories"]:
            return {
                "success": False,
                "message": f"Already a managed directory: {path_resolved}",
                "already_exists": True
            }

        # Add to managed directories
        config_dict["monitor"]["managed_directories"][path_resolved] = priority

        return {
            "success": True,
            "message": f"Added managed directory: {path_resolved} (priority {priority})",
            "path_stored": path_resolved,
            "priority": priority
        }

    @staticmethod
    def remove_managed_directory(config_dict: dict, path: str) -> dict:
        """Remove a managed directory.

        Args:
            config_dict: Configuration dictionary (will be modified)
            path: Directory path to remove

        Returns:
            dict with 'success' (bool), 'message' (str), 'path_removed' (str if success)
        """
        if "monitor" not in config_dict or "managed_directories" not in config_dict["monitor"]:
            return {
                "success": False,
                "message": "No managed_directories configured",
                "not_found": True
            }

        # Resolve path
        path_resolved = str(Path(path).expanduser().resolve())

        # Check if exists
        if path_resolved not in config_dict["monitor"]["managed_directories"]:
            return {
                "success": False,
                "message": f"Not a managed directory: {path_resolved}",
                "not_found": True
            }

        # Get priority before removing
        priority = config_dict["monitor"]["managed_directories"][path_resolved]

        # Remove from managed directories
        del config_dict["monitor"]["managed_directories"][path_resolved]

        return {
            "success": True,
            "message": f"Removed managed directory: {path_resolved}",
            "path_removed": path_resolved,
            "priority": priority
        }

    @staticmethod
    def set_managed_priority(config_dict: dict, path: str, priority: int) -> dict:
        """Update priority for a managed directory.

        Args:
            config_dict: Configuration dictionary (will be modified)
            path: Directory path
            priority: New priority score

        Returns:
            dict with 'success' (bool), 'message' (str), 'old_priority', 'new_priority'
        """
        if "monitor" not in config_dict or "managed_directories" not in config_dict["monitor"]:
            return {
                "success": False,
                "message": "No managed_directories configured",
                "not_found": True
            }

        # Resolve path
        path_resolved = str(Path(path).expanduser().resolve())

        # Check if exists
        if path_resolved not in config_dict["monitor"]["managed_directories"]:
            return {
                "success": False,
                "message": f"Not a managed directory: {path_resolved}",
                "not_found": True
            }

        # Get old priority
        old_priority = config_dict["monitor"]["managed_directories"][path_resolved]

        # Update priority
        config_dict["monitor"]["managed_directories"][path_resolved] = priority

        return {
            "success": True,
            "message": f"Updated priority for {path_resolved}: {old_priority} ‚Üí {priority}",
            "path": path_resolved,
            "old_priority": old_priority,
            "new_priority": priority
        }

    @staticmethod
    def get_status(config: Dict[str, Any]) -> MonitorStatus:
        """Get monitor status, including validation issues.

        Raises:
            KeyError: If monitor section is missing
        """
        from .config import mongo_settings
        from pymongo import MongoClient

        monitor_cfg = MonitorConfig.from_config_dict(config)
        mongo_config = mongo_settings(config)

        # Get total tracked files
        try:
            client = MongoClient(mongo_config["uri"], serverSelectionTimeoutMS=5000)
            client.server_info()
            db_name, coll_name = monitor_cfg.database.split(".", 1)
            db = client[db_name]
            collection = db[coll_name]
            total_files = collection.count_documents({})
            client.close()
        except Exception:
            total_files = 0

        # Get config validation
        validation = MonitorController.validate_config(config)

        return MonitorStatus(
            tracked_files=total_files,
            issues=validation.issues,
            redundancies=validation.redundancies,
            managed_directories=validation.managed_directories,
            include_paths=validation.include_paths,
            exclude_paths=validation.exclude_paths,
            ignore_dirnames=validation.ignore_dirnames,
            ignore_globs=validation.ignore_globs,
            ignore_dirname_validation=validation.ignore_dirname_validation,
            ignore_glob_validation=validation.ignore_glob_validation,
        )

    @staticmethod
    def validate_config(config: dict) -> ConfigValidationResult:
        """Validate monitor configuration for conflicts and issues.

        Raises:
            KeyError: If monitor section or required fields are missing
        """
        monitor_cfg = MonitorConfig.from_config_dict(config)

        include_paths = set(monitor_cfg.include_paths)
        exclude_paths = set(monitor_cfg.exclude_paths)
        managed_dirs = set(monitor_cfg.managed_directories.keys())

        issues = []
        redundancies = []
        managed_directories = {}
        ignore_dirname_validation = {}
        ignore_glob_validation = {}

        # Check 1: Paths in both include and exclude
        conflicts = include_paths & exclude_paths
        if conflicts:
            for path in conflicts:
                issues.append(f"Path in both include and exclude: {path}")

        # Check 2: Duplicate managed directories (same resolved path)
        managed_list = list(managed_dirs)
        for i, dir1 in enumerate(managed_list):
            p1 = Path(dir1).expanduser().resolve()
            for dir2 in managed_list[i + 1:]:
                p2 = Path(dir2).expanduser().resolve()
                try:
                    if p1 == p2:
                        redundancies.append(f"Duplicate managed directories: {dir1} and {dir2} resolve to same path")
                except BaseException:
                    pass

        # Check 3: Vault path redundancy (if vault_path exists in config)
        vault_path = config.get("vault_path")
        if vault_path:
            vault_resolved = str(Path(vault_path).expanduser().resolve())
            for exclude_path in exclude_paths:
                exclude_resolved = str(Path(exclude_path).expanduser().resolve())
                if vault_resolved == exclude_resolved:
                    redundancies.append(f"exclude_paths entry '{exclude_path}' is redundant - vault_path is automatically ignored")

        # Check 4: WKS home redundancy
        wks_home = str(Path("~/.wks").expanduser().resolve())
        for exclude_path in exclude_paths:
            exclude_resolved = str(Path(exclude_path).expanduser().resolve())
            if wks_home == exclude_resolved:
                redundancies.append(f"exclude_paths entry '{exclude_path}' is redundant - WKS home is automatically ignored")

        # Validate each managed directory
        for path, priority in monitor_cfg.managed_directories.items():
            is_valid, error_msg = MonitorValidator.validate_managed_directory(
                path, monitor_cfg.include_paths, monitor_cfg.exclude_paths,
                monitor_cfg.ignore_dirnames, monitor_cfg.ignore_globs
            )
            managed_directories[path] = ManagedDirectoryInfo(
                priority=priority,
                valid=is_valid,
                error=error_msg
            )
            if not is_valid:
                issues.append(f"managed_directories entry '{path}' would NOT be monitored: {error_msg}")

        # Validate ignore_dirnames
        for dirname in monitor_cfg.ignore_dirnames:
            is_valid, error_msg = MonitorValidator.validate_ignore_dirname(dirname, monitor_cfg.ignore_globs)
            ignore_dirname_validation[dirname] = {"valid": is_valid, "error": error_msg}
            if error_msg and "Redundant" in error_msg:
                redundancies.append(f"ignore_dirnames entry '{dirname}': {error_msg}")

        # Validate ignore_globs
        for glob_pattern in monitor_cfg.ignore_globs:
            is_valid, error_msg = MonitorValidator.validate_ignore_glob(glob_pattern)
            ignore_glob_validation[glob_pattern] = {"valid": is_valid, "error": error_msg}
            if not is_valid:
                issues.append(f"ignore_globs entry '{glob_pattern}': {error_msg}")

        return ConfigValidationResult(
            issues=issues,
            redundancies=redundancies,
            managed_directories=managed_directories,
            include_paths=monitor_cfg.include_paths,
            exclude_paths=monitor_cfg.exclude_paths,
            ignore_dirnames=monitor_cfg.ignore_dirnames,
            ignore_globs=monitor_cfg.ignore_globs,
            ignore_dirname_validation=ignore_dirname_validation,
            ignore_glob_validation=ignore_glob_validation,
        )

    @staticmethod
    def check_path(config: dict, path_str: str) -> dict:
        """Check if a path would be monitored and calculate its priority.

        Raises:
            KeyError: If monitor section or required fields are missing
        """
        from .priority import calculate_priority

        monitor_cfg = MonitorConfig.from_config_dict(config)

        # Resolve path
        test_path = Path(path_str).expanduser().resolve()

        # Build decision chain
        decisions = []
        is_monitored = True
        reason = None
        priority = None

        # Step 1: Check if path exists
        path_exists = test_path.exists()
        decisions.append({
            "symbol": "‚úì" if path_exists else "‚ö†",
            "message": f"Path exists: {test_path}" if path_exists else f"Path does not exist (checking as if it did): {test_path}"
        })

        # Step 2: Check include_paths
        included = False
        for include_path in monitor_cfg.include_paths:
            include_resolved = Path(include_path).expanduser().resolve()
            try:
                test_path.relative_to(include_resolved)
                included = True
                decisions.append({"symbol": "‚úì", "message": f"Matches include_paths: {include_path}"})
                break
            except ValueError:
                continue

        if not included:
            is_monitored = False
            reason = "Not under any include_paths"
            decisions.append({"symbol": "‚úó", "message": reason})
            return {
                "path": str(test_path),
                "is_monitored": is_monitored,
                "reason": reason,
                "priority": None,
                "decisions": decisions
            }

        # Step 3: Check exclude_paths
        for exclude_path in monitor_cfg.exclude_paths:
            exclude_resolved = Path(exclude_path).expanduser().resolve()
            try:
                test_path.relative_to(exclude_resolved)
                is_monitored = False
                reason = f"Matched by exclude_paths: {exclude_path}"
                decisions.append({"symbol": "‚úó", "message": reason})
                return {
                    "path": str(test_path),
                    "is_monitored": is_monitored,
                    "reason": reason,
                    "priority": None,
                    "decisions": decisions
                }
            except ValueError:
                continue

        decisions.append({"symbol": "‚úì", "message": "Not in any exclude_paths"})

        # Step 4: Check ignore_dirnames
        for part in test_path.parts:
            if part in monitor_cfg.ignore_dirnames:
                is_monitored = False
                reason = f"Contains ignored dirname: {part}"
                decisions.append({"symbol": "‚úó", "message": reason})
                return {
                    "path": str(test_path),
                    "is_monitored": is_monitored,
                    "reason": reason,
                    "priority": None,
                    "decisions": decisions
                }

        decisions.append({"symbol": "‚úì", "message": "No ignored dirnames in path"})

        # Step 5: Check ignore_globs
        for glob_pattern in monitor_cfg.ignore_globs:
            if fnmatch.fnmatch(str(test_path), glob_pattern) or \
               fnmatch.fnmatch(test_path.name, glob_pattern):
                is_monitored = False
                reason = f"Matched by ignore_globs: {glob_pattern}"
                decisions.append({"symbol": "‚úó", "message": reason})
                return {
                    "path": str(test_path),
                    "is_monitored": is_monitored,
                    "reason": reason,
                    "priority": None,
                    "decisions": decisions
                }

        decisions.append({"symbol": "‚úì", "message": "Not matched by any ignore_globs"})

        # Step 6: Calculate priority
        try:
            priority = calculate_priority(test_path, monitor_cfg.managed_directories, monitor_cfg.priority)
            decisions.append({"symbol": "‚úì", "message": f"Priority calculated: {priority}"})
        except Exception as e:
            priority = None
            decisions.append({"symbol": "‚ö†", "message": f"Could not calculate priority: {e}"})

        return {
            "path": str(test_path),
            "is_monitored": is_monitored,
            "reason": "Would be monitored",
            "priority": priority,
            "decisions": decisions
        }

    @staticmethod
    def prune_ignored_files(config: dict) -> dict:
        """Prune ignored files from the monitor database."""
        from .monitor import WKSFileMonitor
        from .config import mongo_settings
        from .uri_utils import uri_to_path
        from pymongo import MongoClient
        import os

        monitor_cfg = MonitorConfig.from_config_dict(config)
        mongo_config = mongo_settings(config)

        try:
            client = MongoClient(mongo_config["uri"], serverSelectionTimeoutMS=5000)
            client.server_info()  # Will raise an exception if connection fails
            db_name, coll_name = monitor_cfg.database.split(".", 1)
            db = client[db_name]
            collection = db[coll_name]
        except Exception as e:
            return {"success": False, "errors": [f"DB connection failed: {e}"], "pruned_files": []}

        # Instantiate the monitor to get access to the ignore logic
        monitor = WKSFileMonitor(
            state_file=Path(os.path.expanduser("~/.wks/monitor_state.json")),  # Optional field, use default
            include_paths=[os.path.expanduser(p) for p in monitor_cfg.include_paths],
            exclude_paths=[os.path.expanduser(p) for p in monitor_cfg.exclude_paths],
            ignore_dirs=set(monitor_cfg.ignore_dirnames),
            ignore_globs=monitor_cfg.ignore_globs
        )

        pruned_files = []
        errors = []
        processed_count = 0

        try:
            for doc in collection.find():
                processed_count += 1
                uri = doc.get("path")
                if not uri:
                    continue

                try:
                    # The URI in the DB is the canonical representation
                    path_to_check = uri_to_path(uri)
                except Exception as e:
                    errors.append(f"Error converting URI {uri}: {e}")
                    continue

                if monitor._should_ignore(str(path_to_check)):
                    pruned_files.append(str(path_to_check))
                    collection.delete_one({"_id": doc["_id"]})
        except Exception as e:
            errors.append(f"An error occurred during pruning: {e}")
        finally:
            client.close()

        return {
            "success": not errors,
            "pruned_count": len(pruned_files),
            "processed_count": processed_count,
            "pruned_files": pruned_files,
            "errors": errors,
        }

    @staticmethod
    def prune_deleted_files(config: dict) -> dict:
        """Prune deleted files from the monitor database."""
        from .config import mongo_settings
        from .uri_utils import uri_to_path
        from pymongo import MongoClient

        monitor_cfg = MonitorConfig.from_config_dict(config)
        mongo_config = mongo_settings(config)

        try:
            client = MongoClient(mongo_config["uri"], serverSelectionTimeoutMS=5000)
            client.server_info()
            db_name, coll_name = monitor_cfg.database.split(".", 1)
            db = client[db_name]
            collection = db[coll_name]
        except Exception as e:
            return {"success": False, "errors": [f"DB connection failed: {e}"], "pruned_files": []}

        pruned_files = []
        errors = []
        processed_count = 0

        try:
            for doc in collection.find():
                processed_count += 1
                uri = doc.get("path")
                if not uri:
                    continue

                try:
                    path = uri_to_path(uri)
                except Exception as e:
                    errors.append(f"Error converting URI {uri}: {e}")
                    continue

                if not path.exists():
                    pruned_files.append(str(path))
                    collection.delete_one({"_id": doc["_id"]})
        except Exception as e:
            errors.append(f"An error occurred during pruning: {e}")
        finally:
            client.close()

        return {
            "success": not errors,
            "pruned_count": len(pruned_files),
            "processed_count": processed_count,
            "pruned_files": pruned_files,
            "errors": errors,
        }
</file>

<file path="wks/service_controller.py">
"""
Service Controller - shared service/daemon status logic for CLI and MCP.

This module encapsulates all data gathering needed to report service status so
that both the CLI (`wks0 service status`) and the MCP server can reuse the same
implementation without duplicating code. Helper utilities around launchd
integration are also centralised here.
"""

from __future__ import annotations

import json
import os
import platform
import re
import subprocess
import sys
import time
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from . import mongoctl
from .config import load_config, mongo_settings
from .constants import WKS_HOME_EXT


LOCK_FILE = Path.home() / WKS_HOME_EXT / "daemon.lock"


def _fmt_bool(value: Optional[bool], color: bool = False) -> str:
    if value is None:
        return "-"
    if color:
        # Return Rich markup for coloring
        return "[green]true[/green]" if value else "[red]false[/red]"
    return "true" if value else "false"


def _format_timestamp_value(value: Optional[Any], fmt: str) -> str:
    if value is None:
        return ""
    text = str(value).strip()
    if not text:
        return ""
    from datetime import datetime

    try:
        s = text
        if s.endswith("Z"):
            s = s[:-1] + "+00:00"
        s = s.replace("Z", "+00:00")
        dt = datetime.fromisoformat(s)
    except Exception:
        try:
            fallback = text.replace("T", " ").replace("Z", "")
            dt = datetime.fromisoformat(fallback)
        except Exception:
            return text
    try:
        return dt.strftime(fmt)
    except Exception:
        return text


def _pid_running(pid: int) -> bool:
    try:
        os.kill(pid, 0)
        return True
    except Exception:
        return False


def stop_managed_mongo() -> None:
    mongoctl.stop_managed_mongo()


def agent_label() -> str:
    return "com.wieselquist.wks0"


def agent_plist_path() -> Path:
    return Path.home() / "Library" / "LaunchAgents" / f"{agent_label()}.plist"


def is_macos() -> bool:
    return platform.system() == "Darwin"


def _launchctl(*args: str) -> int:
    try:
        return subprocess.call(["launchctl", *args], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except FileNotFoundError:
        return 2


def agent_installed() -> bool:
    return agent_plist_path().exists()


def daemon_start_launchd() -> None:
    uid = os.getuid()
    pl = str(agent_plist_path())
    rc = _launchctl("kickstart", "-k", f"gui/{uid}/{agent_label()}")
    if rc == 0:
        return
    _launchctl("bootout", f"gui/{uid}", pl)
    _launchctl("bootstrap", f"gui/{uid}", pl)
    _launchctl("enable", f"gui/{uid}/{agent_label()}")
    _launchctl("kickstart", "-k", f"gui/{uid}/{agent_label()}")


def daemon_stop_launchd() -> None:
    uid = os.getuid()
    _launchctl("bootout", f"gui/{uid}", str(agent_plist_path()))


def daemon_status_launchd() -> int:
    uid = os.getuid()
    try:
        return subprocess.call(["launchctl", "print", f"gui/{uid}/{agent_label()}"])
    except Exception:
        return 3


def default_mongo_uri() -> str:
    return mongo_settings(load_config())["uri"]


@dataclass
class ServiceStatusLaunch:
    state: Optional[str] = None
    active_count: Optional[str] = None
    pid: Optional[str] = None
    program: Optional[str] = None
    arguments: Optional[str] = None
    working_dir: Optional[str] = None
    stdout: Optional[str] = None
    stderr: Optional[str] = None
    runs: Optional[str] = None
    last_exit: Optional[str] = None
    path: Optional[str] = None
    type: Optional[str] = None

    def present(self) -> bool:
        return any(
            [
                self.state,
                self.pid,
                self.program,
                self.arguments,
                self.working_dir,
                self.stdout,
                self.stderr,
                self.path,
            ]
        )

    def as_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class ServiceStatusData:
    running: Optional[bool] = None
    uptime: Optional[str] = None
    pid: Optional[int] = None
    pending_deletes: Optional[int] = None
    pending_mods: Optional[int] = None
    ok: Optional[bool] = None
    lock: Optional[bool] = None
    last_error: Optional[str] = None
    fs_rate_short: Optional[float] = None
    fs_rate_long: Optional[float] = None
    fs_rate_weighted: Optional[float] = None
    launch: ServiceStatusLaunch = field(default_factory=ServiceStatusLaunch)
    notes: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "service": {
                "running": self.running,
                "uptime": self.uptime,
                "pid": self.pid,
                "pending_deletes": self.pending_deletes,
                "pending_mods": self.pending_mods,
                "ok": self.ok,
                "lock": self.lock,
                "last_error": self.last_error,
                "fs_rate_short": self.fs_rate_short,
                "fs_rate_long": self.fs_rate_long,
                "fs_rate_weighted": self.fs_rate_weighted,
            },
            "launch_agent": self.launch.as_dict(),
            "notes": list(self.notes),
        }

    def _build_health_rows(self) -> List[Tuple[str, str]]:
        """Build Health section rows."""
        return [
            ("[bold cyan]Health[/bold cyan]", ""),
            ("  Running", _fmt_bool(self.running, color=True)),
            ("  Uptime", self.uptime or "-"),
            ("  PID", str(self.pid) if self.pid is not None else "-"),
            ("  OK", _fmt_bool(self.ok, color=True)),
            ("  Lock", _fmt_bool(self.lock, color=True)),
            (
                "  Type",
                self.launch.type
                if (self.launch and self.launch.present() and self.launch.type)
                else "LaunchAgent",
            ),
        ]

    def _build_filesystem_rows(self) -> List[Tuple[str, str]]:
        """Build File System section rows."""
        rows: List[Tuple[str, str]] = [("[bold cyan]File System[/bold cyan]", "")]
        rows.append(("  Pending deletes", str(self.pending_deletes) if self.pending_deletes is not None else "-"))
        rows.append(("  Pending mods", str(self.pending_mods) if self.pending_mods is not None else "-"))

        if self.fs_rate_weighted is not None:
            rows.append(("  Ops (last min)", str(int(self.fs_rate_weighted * 60))))
        if self.fs_rate_short is not None:
            rows.append(("  Ops/sec (10s)", f"{self.fs_rate_short:.2f}"))
        if self.fs_rate_long is not None:
            rows.append(("  Ops/sec (10m)", f"{self.fs_rate_long:.2f}"))
        if self.fs_rate_weighted is not None:
            rows.append(("  Ops/sec (weighted)", f"{self.fs_rate_weighted:.2f}"))

        return rows

    def _build_launch_rows(self) -> List[Tuple[str, str]]:
        """Build Launch section rows."""
        if not self.launch.present():
            return []

        return [
            ("[bold cyan]Launch[/bold cyan]", ""),
            ("  Program", self.launch.arguments or self.launch.program or "-"),
            ("  Stdout", self.launch.stdout or "-"),
            ("  Stderr", self.launch.stderr or "-"),
            ("  Path", self.launch.path or "-"),
            ("  Type", self.launch.type or "-"),
        ]

    def to_rows(self) -> List[Tuple[str, str]]:
        """Return rows grouped by section: Health, File System, Launch."""
        rows = []
        rows.extend(self._build_health_rows())
        rows.extend(self._build_filesystem_rows())
        return rows


class ServiceController:
    """Business logic for service status inspection."""

    @staticmethod
    def _read_launch_agent() -> Optional[ServiceStatusLaunch]:
        if not (is_macos() and agent_installed()):
            return None
        try:
            uid = os.getuid()
            out = subprocess.check_output(
                ["launchctl", "print", f"gui/{uid}/{agent_label()}"],
                stderr=subprocess.STDOUT,
            )
            launch_text = out.decode("utf-8", errors="ignore")

            def _find(pattern: str, default: str = "") -> str:
                match = re.search(pattern, launch_text)
                return match.group(1).strip() if match else default

            launch = ServiceStatusLaunch(
                active_count=_find(r"active count =\s*(\d+)"),
                path=_find(r"\n\s*path =\s*(.*)"),
                type=_find(r"\n\s*type =\s*(.*)"),
                state=_find(r"\n\s*state =\s*(.*)"),
                program=_find(r"\n\s*program =\s*(.*)"),
                working_dir=_find(r"\n\s*working directory =\s*(.*)"),
                stdout=_find(r"\n\s*stdout path =\s*(.*)"),
                stderr=_find(r"\n\s*stderr path =\s*(.*)"),
                runs=_find(r"\n\s*runs =\s*(\d+)"),
                pid=_find(r"\n\s*pid =\s*(\d+)"),
                last_exit=_find(r"\n\s*last exit code =\s*(\d+)"),
            )
            try:
                args_block = re.search(r"arguments = \{([^}]*)\}", launch_text, re.DOTALL)
                if args_block:
                    lines = [ln.strip() for ln in args_block.group(1).splitlines() if ln.strip()]
                    launch.arguments = " ".join(lines)
            except Exception:
                pass
            return launch
        except Exception:
            return None

    @staticmethod
    def _read_health(status: ServiceStatusData) -> None:
        health_path = Path.home() / WKS_HOME_EXT / "health.json"
        health: Dict[str, Any] = {}

        if health_path.exists():
            try:
                with open(health_path, "r") as f:
                    health = json.load(f)
            except Exception:
                status.notes.append("Failed to read health metrics")

        if health:
            status.running = bool(health.get("lock_present"))
            status.uptime = str(health.get("uptime_hms") or "")
            try:
                status.pid = int(health.get("pid"))
            except (ValueError, TypeError):
                status.pid = None
            status.pending_deletes = health.get("pending_deletes")
            status.pending_mods = health.get("pending_mods")
            status.ok = not bool(health.get("last_error"))
            status.lock = bool(health.get("lock_present"))
            if health.get("last_error"):
                status.last_error = str(health.get("last_error"))

            for attr, key in [
                ("fs_rate_short", "fs_rate_short"),
                ("fs_rate_long", "fs_rate_long"),
                ("fs_rate_weighted", "fs_rate_weighted"),
            ]:
                val = health.get(key)
                try:
                    setattr(status, attr, float(val) if val is not None else None)
                except (ValueError, TypeError):
                    setattr(status, attr, None)
        else:
            lock_exists = LOCK_FILE.exists()
            status.lock = lock_exists
            if lock_exists:
                try:
                    pid = int(LOCK_FILE.read_text().strip().splitlines()[0])
                    status.pid = pid
                    status.running = _pid_running(pid)
                except Exception:
                    status.notes.append("Lock present but PID unavailable")
                    status.running = None
            else:
                status.running = False
                if not status.notes:
                    status.notes.append("WKS daemon: not running")

    @staticmethod
    def get_status() -> ServiceStatusData:
        status = ServiceStatusData()

        launch_info = ServiceController._read_launch_agent()
        if launch_info:
            status.launch = launch_info
        else:
            if is_macos() and agent_installed():
                status.notes.append("Launch agent status unavailable")

        ServiceController._read_health(status)

        return status


__all__ = [
    "LOCK_FILE",
    "ServiceController",
    "ServiceStatusData",
    "ServiceStatusLaunch",
    "agent_installed",
    "agent_label",
    "agent_plist_path",
    "daemon_start_launchd",
    "daemon_status_launchd",
    "daemon_stop_launchd",
    "default_mongo_uri",
    "is_macos",
    "stop_managed_mongo",
]
</file>

<file path="CHANGELOG.md">
# Changelog

All notable changes to this project will be documented in this file.

The format is based on Keep a Changelog, and this project adheres to Semantic Versioning (SemVer).

## [0.2.8] - 2025-11-03
### Added
- Every Mongo database now stores a `_wks_meta` compatibility tag so upgrades can reuse existing embeddings when the schema is still compatible. Configure overrides via `mongo.compatibility.space|time` in `~/.wks/config.json`.
- `wks0 db info/query` and the daemon status panel validate compatibility tags and explain how to override them instead of silently rebuilding the database.
- Optional Obsidian views that read `SimilarityDB` (Health/ActiveFiles) honor the same compatibility tags.
- Markdown outputs (service status, `db query`) now render through Jinja2 templates fed by the same JSON payloads used for `--display json`.

### Changed
- `wks0 service`/daemon startup halts early when the stored compatibility tag does not match the current build, preventing unintended data wipes.
- Progress bars no longer rely on Rich-only options, restoring compatibility with the pinned Rich version in `.venv`.
- MongoDB connection setup is centralized in `mongoctl.create_client`, removing duplicated ensure/ping logic across the CLI.

## [0.2.7] - 2025-11-03
### Added
- `MongoGuard` keeps the managed local MongoDB process alive for the entire daemon lifetime, automatically restarting `mongod` whenever connectivity checks fail.

### Changed
- `python -m wks.daemon` (the launchd entry point) now enforces Mongo availability before monitoring starts and passes the configured URI into the daemon so service start/restart always brings the database online.

## [0.2.6] - 2025-11-01
### Added
- Daemon now launches a background maintenance thread that regularly runs `SimilarityDB.audit_documents()` and shuts down cleanly with the Mongo client.

### Changed
- `wks0 service status` and `wks0 db info` display timestamps using the configured `display.timestamp_format` and backfill missing byte totals by inspecting on-disk files.

### Fixed
- Space DB audits handle documents stored with plain filesystem paths (without `file://`), ensuring missing `bytes` metadata is repopulated.

## [0.2.5] - 2025-10-29
### Added
- `wks0 extract` runs the configured extractor and persists artefacts without touching the database.
- `wks0 index --untrack` removes tracked entries and cleans their extraction artefacts from the Space DB.

### Changed
- Space database documents now store absolute file URIs, checksum/size/angle metadata, and all CLI views respect `display.timestamp_format`; `wks0 db info` surfaces timestamp, checksum, size, angle, and URI in that order.
- `wks0 config print` emits the canonical config structure (including `display` and `mongo` blocks) and normalization defaults; `wks0 index` shares the extraction pipeline with `wks0 extract`.
- Similarity indexing caches extracted content under `.wkso/<checksum>.md`, cleans stale artefacts on updates/moves, and tracks removals via the CLI.

## [0.2.4] - 2025-10-29
### Added
- `display.timestamp_format` config option (default `%Y-%m-%d %H:%M:%S`) drives all CLI/Obsidian timestamp output.

### Changed
- `wks0 db info` now surfaces checksum, chunk count, and size columns and respects the configured timestamp format across both space/time views.

## [0.2.3] - 2025-10-29
### Added
- `wks0 --version` now includes the current git SHA (when available) alongside the package version.
- `wks0 config print` surfaces the shared `mongo` block and hides legacy `similarity.mongo_*` keys so the output matches the canonical config structure.

### Changed
- Service lifecycle now fully manages the bundled MongoDB: `install|start|reset` auto-start `mongod`, `stop|reset|uninstall` shut it down, and `wks0 db reset` restarts it after clearing data so the daemon immediately reconnects.
- CLI database commands (`wks0 db info/query/reset`) bootstrap the local Mongo instance before use, preventing connection-refused errors during manual runs.

## [0.2.2] - 2025-10-28
### Added
- `wks0 --version` prints the installed CLI version.

### Fixed
- `wks0 db query` and related commands now connect with minimal config and respect patched `pymongo.MongoClient`, avoiding hard `similarity.enabled` requirements.

### Changed
- Canonicalized `wks0 db info` (dropping the `stats` alias) with short timeouts and defaults for missing config values.
- `wks0 service install|start|reset` now verify MongoDB is reachable and auto-start a local `mongod` on the default port when needed.
- All CLI code paths (including `wks0 db`) now auto-start the default local MongoDB when it isn't already running.
- Config promotes MongoDB settings to a dedicated `mongo` block; similarity inherits those values automatically.

## [0.2.0] - 2025-10-28
### Added
- Rich dashboard for `wks0 service status` with parsed launchctl info and fast Space DB panel.
- `wks0 service reset` to stop agent, reset DB/state, and restart cleanly.
- `wks0 db reset` to drop Mongo databases and remove local data dir.
- `-n/--latest` to `wks0 db info` to list most-recent files/snapshots quickly.
- Auto-start local mongod (27027) during `wks0 index` when needed.
- Directory move handling via `rename_folder` to update descendant paths in-place.
- Pytest suite for Space DB (index, move, rename detection, folder moves).

### Changed
- `wks0 db info` uses a lightweight client with short timeouts for responsiveness.
- Service status defaults to rich panels; json mode emits the structured status document.
- Docling is now a required extractor (no optional branches).
- Primary CLI is `wks0`; package remains `wks`.

### Removed
- Legacy `analyze` CLI surface; kept CLI minimal: `config`, `service`, `index`, `db`.

## [0.1.1] - 2025-10-28
### Added
- Initial rich status panel, tests setup, KISS/DRY cleanup.

## [0.1.0] - 2025-10-27
- Initial release of `wks0` CLI with `config`, `service`, `index`, `db`.
</file>

<file path="tests/test_cli_basic.py">
import argparse
import io
import json
import os
import signal
import threading
from contextlib import redirect_stdout
from pathlib import Path

try:
    import importlib.metadata as importlib_metadata
except ImportError:  # pragma: no cover
    import importlib_metadata  # type: ignore

import mongomock
import pytest

from wks.constants import WKS_HOME_EXT


@pytest.fixture(autouse=True)
def _temp_home(monkeypatch, tmp_path):
    monkeypatch.setattr('pathlib.Path.home', lambda: tmp_path)


def run_cli(args, monkeypatch=None):
    from wks.cli import main
    buf = io.StringIO()
    with redirect_stdout(buf):
        try:
            rc = main(args)
        except SystemExit as exc:
            code = exc.code
            rc = code if isinstance(code, int) else 0
    return rc, buf.getvalue()


def test_cli_config_print_json(monkeypatch):
    cfg = {
        "vault": {"base_dir": "~/obsidian"},
        "similarity": {"enabled": True},
        "db": {"type": "mongodb", "uri": "mongodb://localhost:27017/"},
        "display": {"timestamp_format": "%Y-%m-%d %H:%M:%S"}
    }
    monkeypatch.setattr('wks.cli.load_config', lambda: cfg)
    rc, out = run_cli(['--display', 'mcp', 'config'], monkeypatch)
    assert rc == 0
    data = json.loads(out)
    # New structure: vault section
    assert "vault" in data
    assert data["vault"]["base_dir"] == "~/obsidian"
    # DB section (new) or mongo (legacy)
    assert "db" in data or "mongo" in data
    if "db" in data:
        assert data["db"]["uri"].startswith("mongodb://")
    elif "mongo" in data:
        assert data["mongo"]["uri"].startswith("mongodb://")
    assert "mongo_uri" not in data.get("similarity", {})
    assert data["display"]["timestamp_format"] == "%Y-%m-%d %H:%M:%S"


def test_cli_version_flag():
    rc, out = run_cli(['--version'])
    assert rc == 0
    expected = importlib_metadata.version('wks')
    assert f"wks0 {expected}" in out
    # git SHA is optional but when present must be within parentheses
    if '(' in out:
        assert out.strip().endswith(')')


def test_cli_db_query_space_json(monkeypatch):
    # Minimal config
    monkeypatch.setattr('wks.cli.load_config', lambda: {
        'similarity': {
            'mongo_uri': 'mongodb://localhost:27027/',
            'database': 'wks_similarity',
            'collection': 'file_embeddings',
        }
    })
    # Seed DB
    client = mongomock.MongoClient()
    coll = client['wks_similarity']['file_embeddings']
    coll.insert_one({'path': '/tmp/x.txt', 'timestamp': '2025-10-28T10:00:00.000000'})
    monkeypatch.setattr('pymongo.MongoClient', lambda *a, **k: client)
    rc, out = run_cli(['--display', 'json', 'db', 'query', '--space', '--filter', '{}', '--limit', '1'])
    assert rc == 0
    assert '/tmp/x.txt' in out


def test_cli_service_status_json_parsed(tmp_path, monkeypatch):
    # Prepare health.json under fake HOME
    home = tmp_path
    (home / WKS_HOME_EXT).mkdir(parents=True, exist_ok=True)
    health = {
        'uptime_hms': '00:01:00', 'pid': '12345',
        'avg_beats_per_min': 30.0, 'pending_deletes': 0, 'pending_mods': 0, 'last_error': None,
        'lock_present': True
    }
    (home / WKS_HOME_EXT / 'health.json').write_text(json.dumps(health))
    monkeypatch.setattr('pathlib.Path.home', lambda: home)
    # Fake launchctl print

    def _fake_check_output(args, stderr=None):
        return ("""
gui/XXXX/com.wieselquist.wks0 = {
    active count = 1
    path = /Users/you/Library/LaunchAgents/com.wieselquist.wks0.plist
    type = LaunchAgent
    state = running

    program = /path/to/python
    arguments = {
        /path/to/python
        -m
        wks.daemon
    }

    working directory = /path/to/cwd
    stdout path = /tmp/daemon.log
    stderr path = /tmp/daemon.error.log
    runs = 2
    pid = 12345
    last exit code = 0
}
""").encode('utf-8')
    monkeypatch.setattr('subprocess.check_output', _fake_check_output)
    monkeypatch.setattr('wks.cli._agent_installed', lambda: True)

    class FakeCursor(list):
        def sort(self, *args, **kwargs):
            return self

        def limit(self, *args, **kwargs):
            return self

    class FakeCollection:
        def count_documents(self, *_):
            return 0

        def find(self, *args, **kwargs):
            return FakeCursor()

    class FakeDB:
        def __getitem__(self, name):
            return FakeCollection()

    class FakeClient:
        def __init__(self, *a, **k):
            self.admin = self

        def command(self, *_):
            return {"ok": 1}

        def __getitem__(self, name):
            return FakeDB()

        def close(self):
            pass

    monkeypatch.setattr('pymongo.MongoClient', lambda *a, **k: FakeClient())

    rc, out = run_cli(['--display', 'json', 'service', 'status'])
    assert rc == 0
    data = json.loads(out)
    service = data['service']
    assert service['pid'] == 12345
    assert service['lock'] is True
    if service['fs_rate_short'] is not None:
        assert service['fs_rate_short'] == pytest.approx(0.0)
    if service['fs_rate_long'] is not None:
        assert service['fs_rate_long'] == pytest.approx(0.0)
    if service['fs_rate_weighted'] is not None:
        assert service['fs_rate_weighted'] == pytest.approx(0.0)
    launch = data['launch_agent']
    assert launch['state'] == 'running'
    assert launch['pid'] == '12345'


def test_cli_db_reset_drops_and_removes(tmp_path, monkeypatch):
    # Fake HOME for ~/.wks/mongodb
    home = tmp_path
    dbdir = home / WKS_HOME_EXT / 'mongodb' / 'db'
    dbdir.mkdir(parents=True, exist_ok=True)
    monkeypatch.setattr('pathlib.Path.home', lambda: home)
    # Config
    monkeypatch.setattr('wks.cli.load_config', lambda: {
        'similarity': {'mongo_uri': 'mongodb://localhost:27027/', 'database': 'wks_similarity', 'collection': 'file_embeddings'}
    })
    # Fake Mongo drop

    class FakeClient:
        def __init__(self, *a, **k): pass

        class admin:
            @staticmethod
            def command(x): return {'ok': 1}

        def drop_database(self, name): self.dropped = name
    monkeypatch.setattr('wks.cli.pymongo.MongoClient', FakeClient)
    rc, out = run_cli(['--display', 'json', 'db', 'reset'])
    assert rc == 0
    assert not (home / WKS_HOME_EXT / 'mongodb').exists()


def test_cli_index_json_progress(monkeypatch):
    # Fake similarity loader and iter_files
    class FakeDB:
        def __init__(self): self.count = 0
        def add_file(self, p, extraction=None, **kwargs): self.count += 1; return True
        def get_last_add_result(self): return {'content_checksum': 'abc', 'text': 'content', 'timings': {'embed': 0.01, 'db_update': 0.002, 'chunks': 0.001}}
        def get_stats(self): return {'database': 'wks_similarity', 'collection': 'file_embeddings', 'total_files': self.count, 'total_bytes': 1234}
    monkeypatch.setattr('wks.cli._load_similarity_required', lambda: (FakeDB(), {}))
    monkeypatch.setattr('wks.cli._iter_files', lambda paths, exts, cfg: [Path('a.txt'), Path('b.txt')])

    class FakeExtractor:
        def extract(self, path, persist=True):
            from types import SimpleNamespace
            return SimpleNamespace(
                text='dummy content',
                content_path=None,
                content_checksum='abc',
                content_bytes=12,
                engine='builtin',
            )
    monkeypatch.setattr('wks.cli._build_extractor', lambda cfg: FakeExtractor())
    # Vault writer

    class FakeVault:
        def write_doc_text(self, *a, **k): pass
    monkeypatch.setattr('wks.cli._load_vault', lambda: FakeVault())
    rc, out = run_cli(['--display', 'json', 'index', 'a.txt', 'b.txt'])
    assert rc == 0
    # Expect textual progress lines even in JSON mode (shows in captured output)
    assert '[1/2]' in out or '[2/2]' in out
    assert 'total_bytes=1234' in out
    # Plain timing summary should include headers and aligned units
    assert 'Timing Details' in out
    assert 'Totals' in out
    assert 'Hash' in out and 'Extract' in out and 'Embed' in out
    assert ' ms' in out or ' s' in out


def test_cli_extract_runs_extractor(monkeypatch):
    monkeypatch.setattr('wks.cli.load_config', lambda: {'similarity': {'include_extensions': ['.txt']}})
    files = [Path('/tmp/a.txt'), Path('/tmp/b.txt')]
    monkeypatch.setattr('wks.cli._iter_files', lambda paths, exts, cfg: files)

    class FakeExtractor:
        def __init__(self): self.calls = []

        def extract(self, path, persist=True):
            self.calls.append(path)
            from types import SimpleNamespace
            return SimpleNamespace(
                text='content',
                content_path=Path(f'/tmp/.wkso/{path.name}.md'),
                content_checksum='hash',
                content_bytes=42,
                engine='builtin',
            )

    fake = FakeExtractor()
    monkeypatch.setattr('wks.cli._build_extractor', lambda cfg: fake)
    rc, out = run_cli(['--display', 'json', 'extract', '/tmp'])
    assert rc == 0
    assert len(fake.calls) == len(files)
    for path in files:
        assert str(path) in out
        assert f".wkso/{path.name}.md" in out


def test_cli_index_untrack(monkeypatch):
    class FakeDB:
        def __init__(self): self.removed = []

        def remove_file(self, path, **kwargs):
            self.removed.append(path)
            return True
    fake_db = FakeDB()
    monkeypatch.setattr('wks.cli._load_similarity_required', lambda: (fake_db, {}))
    monkeypatch.setattr('wks.cli._iter_files', lambda paths, exts, cfg: [Path('one.txt'), Path('two.txt')])
    monkeypatch.setattr('wks.cli.load_config', lambda: {})
    rc, out = run_cli(['--display', 'json', 'index', '--untrack', 'one.txt', 'two.txt'])
    assert rc == 0
    assert 'Untracked 2 file(s)' in out
    assert fake_db.removed == [Path('one.txt'), Path('two.txt')]


def test_ensure_mongo_running_autostarts(monkeypatch, tmp_path):
    import wks.mongoctl as mongoctl
    calls = []

    def fake_ping(uri, timeout_ms=500):
        calls.append((uri, timeout_ms))
        return len(calls) > 1
    monkeypatch.setattr(mongoctl, "mongo_ping", fake_ping)
    starts = []
    monkeypatch.setattr(mongoctl.shutil, "which", lambda exe: "/usr/bin/mongod" if exe == "mongod" else None)

    def fake_popen(cmd, *args, **kwargs):
        starts.append(cmd)

        class Proc:
            pid = 4242
        return Proc()
    monkeypatch.setattr(mongoctl.subprocess, "Popen", fake_popen)
    monkeypatch.setattr(mongoctl.time, "sleep", lambda _: None)
    mongoctl.ensure_mongo_running("mongodb://localhost:27027/")
    assert starts, "mongod should be started when initial ping fails"
    assert len(calls) >= 2
    assert "--port" in starts[0]
    port_arg_index = starts[0].index("--port") + 1
    assert starts[0][port_arg_index] == "27027"


def test_ensure_mongo_running_fails_without_mongod(monkeypatch):
    import wks.mongoctl as mongoctl
    monkeypatch.setattr(mongoctl, "mongo_ping", lambda uri, timeout_ms=500: False)
    monkeypatch.setattr(mongoctl.shutil, "which", lambda exe: None)
    with pytest.raises(SystemExit):
        mongoctl.ensure_mongo_running("mongodb://localhost:27027/")


def test_mongo_client_params_calls_ensure(monkeypatch, tmp_path):
    import wks.cli as cli
    import wks.mongoctl as mongoctl
    called = {}

    def fake_ensure(uri, record_start=False):
        called['ensure_uri'] = uri
        called['record'] = record_start
    monkeypatch.setattr(mongoctl, "ensure_mongo_running", fake_ensure)

    def fake_create(uri, server_timeout=500, connect_timeout=500, ensure_running=True):
        if ensure_running:
            fake_ensure(uri)
        import mongomock
        return mongomock.MongoClient()
    monkeypatch.setattr(mongoctl, "create_client", fake_create)
    monkeypatch.setattr('wks.cli.load_config', lambda: {
        'db': {
            'uri': 'mongodb://localhost:27027/',
        },
        'mongo': {
            'space_database': 'wks_similarity',
            'space_collection': 'file_embeddings',
        }
    })
    client, mongo_cfg = cli._mongo_client_params()
    assert called['ensure_uri'] == 'mongodb://localhost:27027/'
    assert mongo_cfg['space_database'] == 'wks_similarity'


def test_mongo_client_params_skip_ensure(monkeypatch, tmp_path):
    import wks.cli as cli
    import wks.mongoctl as mongoctl
    called = {"ensure": 0}

    def fail_ensure(uri, record_start=False):
        called["ensure"] += 1
        raise AssertionError("ensure should not run")
    monkeypatch.setattr(mongoctl, "ensure_mongo_running", fail_ensure)

    def fake_create(uri, server_timeout=500, connect_timeout=500, ensure_running=True):
        called["ensure_running"] = ensure_running
        import mongomock
        return mongomock.MongoClient()
    monkeypatch.setattr(mongoctl, "create_client", fake_create)
    monkeypatch.setattr('wks.cli.load_config', lambda: {
        'db': {
            'uri': 'mongodb://localhost:27027/',
        },
        'mongo': {
            'space_database': 'wks_similarity',
            'space_collection': 'file_embeddings',
        }
    })
    client, mongo_cfg = cli._mongo_client_params(ensure_running=False)
    assert called.get("ensure_running") is False
    assert called["ensure"] == 0
    assert mongo_cfg['space_collection'] == 'file_embeddings'


def test_stop_managed_mongo(monkeypatch, tmp_path):
    import wks.mongoctl as mongoctl
    flag = tmp_path / 'managed'
    pidfile = tmp_path / 'mongod.pid'
    flag.write_text('123')
    pidfile.write_text('123')
    monkeypatch.setattr(mongoctl, "MONGO_MANAGED_FLAG", flag)
    monkeypatch.setattr(mongoctl, "MONGO_PID_FILE", pidfile)
    killed = []
    monkeypatch.setattr(mongoctl, "pid_running", lambda pid: False)
    monkeypatch.setattr(mongoctl.os, 'kill', lambda pid, sig: killed.append((pid, sig)))
    mongoctl.stop_managed_mongo()
    assert killed == [(123, signal.SIGTERM)]
    assert not flag.exists()
    assert not pidfile.exists()


def test_mongo_guard_restarts_local(monkeypatch):
    import wks.mongoctl as mongoctl
    ping_values = iter([False, True])

    def fake_ping(uri, timeout_ms=500):
        try:
            return next(ping_values)
        except StopIteration:
            return True
    monkeypatch.setattr(mongoctl, "mongo_ping", fake_ping)
    ensure_calls = []
    resumed = threading.Event()

    def fake_ensure(uri, record_start=False):
        ensure_calls.append(record_start)
        if len(ensure_calls) > 1:
            resumed.set()
    monkeypatch.setattr(mongoctl, "ensure_mongo_running", fake_ensure)
    guard = mongoctl.MongoGuard("mongodb://localhost:27027/", ping_interval=0.01)
    guard.start(record_start=True)
    assert resumed.wait(0.3), "Guard should trigger a restart when ping fails"
    guard.stop()
    assert len(ensure_calls) >= 2


def test_db_reset_restarts_mongo(monkeypatch, tmp_path):
    import wks.cli as cli
    import wks.mongoctl as mongoctl
    calls = {}

    def fake_ensure(uri, record_start=False):
        calls['uri'] = uri
        calls['record'] = record_start
    monkeypatch.setattr(mongoctl, "ensure_mongo_running", fake_ensure)
    monkeypatch.setattr(cli, "_stop_managed_mongo", lambda: None)
    monkeypatch.setattr(cli, "mongo_settings", lambda cfg: {
        "uri": "mongodb://localhost:27027/",
        "space_database": "wks_similarity",
        "space_collection": "file_embeddings",
        "time_database": "wks_similarity",
        "time_collection": "file_snapshots",
    })

    class FakeClient:
        def __init__(self, *a, **k):
            pass

        class admin:
            @staticmethod
            def command(_):
                return {"ok": 1}

        def drop_database(self, name):
            pass
    monkeypatch.setattr('pymongo.MongoClient', lambda *a, **k: FakeClient())
    monkeypatch.setattr('wks.cli.load_config', lambda: {})
    rc, _ = run_cli(['--display', 'json', 'db', 'reset'])
    assert rc == 0
    assert calls["uri"] == "mongodb://localhost:27027/"
    assert calls["record"] is True
</file>

<file path="wks/config.py">
from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Optional

from .constants import WKS_HOME_EXT
from .utils import wks_home_path, get_wks_home

DEFAULT_MONGO_URI = "mongodb://localhost:27017/"
DEFAULT_SPACE_DATABASE = "wks_similarity"
DEFAULT_SPACE_COLLECTION = "file_embeddings"
DEFAULT_TIME_COLLECTION = "file_snapshots"
DEFAULT_TIMESTAMP_FORMAT = "%Y-%m-%d %H:%M:%S"


def mongo_settings(cfg: Dict[str, Any]) -> Dict[str, str]:
    """Normalize Mongo connection settings from config.
    
    Uses 'db' section for URI, 'related' section for database/collection names.
    """
    db_cfg = cfg.get("db", {})
    related_cfg = cfg.get("related", {})
    embedding_cfg = related_cfg.get("engines", {}).get("embedding", {})

    def _norm(value, default):
        return str(value) if value and value != "" else default

    uri = _norm(db_cfg.get("uri"), DEFAULT_MONGO_URI)
    space_db = _norm(embedding_cfg.get("database"), DEFAULT_SPACE_DATABASE)
    space_coll = _norm(embedding_cfg.get("collection"), DEFAULT_SPACE_COLLECTION)
    time_db = _norm(embedding_cfg.get("database"), DEFAULT_SPACE_DATABASE)
    time_coll = _norm(embedding_cfg.get("snapshots_collection"), DEFAULT_TIME_COLLECTION)
    
    return {
        "uri": uri,
        "space_database": space_db,
        "space_collection": space_coll,
        "time_database": time_db,
        "time_collection": time_coll,
    }


def apply_similarity_mongo_defaults(sim_cfg: Dict[str, Any], mongo_cfg: Dict[str, str]) -> Dict[str, Any]:
    """Ensure similarity config contains the canonical Mongo keys."""
    sim = dict(sim_cfg)
    sim.setdefault("mongo_uri", mongo_cfg["uri"])
    sim.setdefault("database", mongo_cfg["space_database"])
    sim.setdefault("collection", mongo_cfg["space_collection"])
    sim.setdefault("snapshots_collection", mongo_cfg["time_collection"])
    return sim


def timestamp_format(cfg: Dict[str, Any]) -> str:
    disp = cfg.get("display", {})
    fmt = disp.get("timestamp_format")
    return fmt if isinstance(fmt, str) and fmt.strip() else DEFAULT_TIMESTAMP_FORMAT


def get_config_path() -> Path:
    """Get path to WKS config file.

    Calls get_wks_home() to determine WKS home directory (checking WKS_HOME env var),
    then returns path to config.json within that directory.

    Returns:
        Path to config.json file

    Examples:
        >>> # WKS_HOME not set
        >>> get_config_path()
        Path("/Users/user/.wks/config.json")
        >>> # WKS_HOME="/custom/path"
        >>> get_config_path()
        Path("/custom/path/config.json")
    """
    return get_wks_home() / "config.json"


def load_config(path: Optional[Path] = None) -> Dict[str, Any]:
    """Load WKS configuration from JSON file.

    If path is None, calls get_config_path() to determine the default config location
    by checking WKS_HOME environment variable and defaulting to ~/.wks/config.json.

    Args:
        path: Optional explicit path to config file. If None, uses get_config_path().

    Returns:
        Configuration dictionary, or empty dict if file doesn't exist or can't be loaded.

    Examples:
        >>> # Load from default location
        >>> config = load_config()
        >>> # Load from custom location
        >>> config = load_config(Path("/custom/config.json"))
    """
    if path is None:
        path = get_config_path()

    if path.exists():
        try:
            with open(path, "r") as fh:
                return json.load(fh)
        except Exception:
            return {}
    return {}


# Backwards compatibility alias
def load_user_config() -> Dict[str, Any]:
    """DEPRECATED: Use load_config() instead."""
    return load_config()
</file>

<file path="wks/monitor.py">
"""
File system monitoring for WKS using watchdog.
Monitors directories and tracks file changes without external dependencies.
"""

import time
import json
from pathlib import Path
import fnmatch
from datetime import datetime
from typing import Dict, Set, Callable, Optional, List

from .constants import WKS_HOME_EXT, WKS_DOT_DIRS
from watchdog.observers import Observer
try:
    from watchdog.observers.fsevents import FSEventsObserver  # macOS
except Exception:  # pragma: no cover
    FSEventsObserver = None  # type: ignore
try:
    from watchdog.observers.kqueue import KqueueObserver  # BSD/macOS
except Exception:  # pragma: no cover
    KqueueObserver = None  # type: ignore
try:
    from watchdog.observers.polling import PollingObserver  # cross-platform
except Exception:  # pragma: no cover
    PollingObserver = None  # type: ignore
from watchdog.events import FileSystemEventHandler, FileSystemEvent


class WKSFileMonitor(FileSystemEventHandler):
    """
    Monitor file system changes and track them for WKS.

    This replaces the A_GIS MongoDB-based monitoring with a simpler
    JSON-based tracking system suitable for personal use.
    """

    def __init__(
        self,
        state_file: Path,
        on_change: Optional[Callable[[str, str], None]] = None,
        ignore_patterns: Optional[Set[str]] = None,
        ignore_dirs: Optional[Set[str]] = None,
        include_paths: Optional[List[Path]] = None,
        exclude_paths: Optional[List[Path]] = None,
        ignore_globs: Optional[List[str]] = None,
    ):
        """
        Initialize the file monitor.

        Args:
            state_file: Path to JSON file for tracking state
            on_change: Callback function(event_type, file_path) when files change
            ignore_patterns: Set of patterns to ignore (e.g., {'.git', 'venv'})
            ignore_dirs: Set of directory names to completely ignore
        """
        super().__init__()
        self.state_file = Path(state_file)
        self.on_change = on_change
        # Deprecated: ignore_patterns. We'll fold these into glob rules for consistency.
        self.ignore_patterns = ignore_patterns or {'.git', '__pycache__', '.DS_Store', 'venv', '.venv', 'node_modules'}
        self.ignore_dirs = ignore_dirs or {'Library', 'Applications', '.Trash', '.cache', 'Cache', '_build'}
        # Paths to explicitly include/exclude (resolved)
        self.include_paths = [Path(p).expanduser().resolve() for p in include_paths] if include_paths else []
        self.exclude_paths = [Path(p).expanduser().resolve() for p in exclude_paths] if exclude_paths else []
        # Glob patterns (Unix shell-style) to ignore. Fold legacy ignore_patterns into globs.
        _globs = list(ignore_globs or [])
        if self.ignore_patterns:
            for tok in self.ignore_patterns:
                # Ignore a directory named tok anywhere, and files named tok
                _globs.append(f"**/{tok}/**")
                _globs.append(f"**/{tok}")
        self.ignore_globs = _globs
        # Dot-path whitelist that should never be ignored by default rules
        self._dot_whitelist = set()
        self.state = self._load_state()

    def _load_state(self) -> Dict:
        """Load state from JSON file."""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r') as f:
                    return json.load(f)
            except (json.JSONDecodeError, OSError) as e:
                # State file corrupted, back it up and start fresh
                backup_file = self.state_file.with_suffix('.json.backup')
                try:
                    self.state_file.rename(backup_file)
                    print(f"Warning: Corrupted state file backed up to {backup_file}")
                except OSError:
                    pass
        return {"files": {}, "last_update": None}

    def _save_state(self):
        """Save state to JSON file."""
        self.state["last_update"] = datetime.now().isoformat()
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=2)

    def _is_within(self, path: Path, base: Path) -> bool:
        """Return True if path is within base (or equal)."""
        try:
            path.resolve().relative_to(base.resolve())
            return True
        except ValueError:
            return False

    def _should_ignore(self, path: str) -> bool:
        """Check if path should be ignored based on patterns."""
        path_obj = Path(path).resolve()

        # Exclude by explicit exclude paths
        for ex in self.exclude_paths:
            if self._is_within(path_obj, ex):
                return True

        # If include paths are provided, ignore anything outside them
        if self.include_paths:
            if not any(self._is_within(path_obj, inc) for inc in self.include_paths):
                return True

        # Check if any parent directory should be ignored
        for part in path_obj.parts:
            if part in self.ignore_dirs:
                return True
            # Ignore dot-directories except whitelisted
            if part in WKS_DOT_DIRS:
                return True
            if part.startswith('.') and part not in self._dot_whitelist:
                return True
            # Ignore directories starting with underscore (e.g., _build, _site)
            if part.startswith('_'):
                return True

        # No separate ignore_patterns check: patterns are folded into glob rules.

        # Glob-based ignores against full path and basename
        path_str = path_obj.as_posix()
        basename = path_obj.name
        for g in self.ignore_globs:
            try:
                if fnmatch.fnmatchcase(path_str, g) or fnmatch.fnmatchcase(basename, g):
                    # Preserve whitelist
                    if basename in self._dot_whitelist:
                        continue
                    return True
            except Exception:
                # Ignore malformed globs
                continue

        return False

    def _track_change(self, event_type: str, path: str):
        """Track a file change in state."""
        if self._should_ignore(path):
            return

        path_str = str(Path(path).resolve())

        if path_str not in self.state["files"]:
            self.state["files"][path_str] = {
                "created": datetime.now().isoformat(),
                "modifications": []
            }

        self.state["files"][path_str]["modifications"].append({
            "type": event_type,
            "timestamp": datetime.now().isoformat()
        })

        # Keep only last 10 modifications per file
        if len(self.state["files"][path_str]["modifications"]) > 10:
            self.state["files"][path_str]["modifications"] = \
                self.state["files"][path_str]["modifications"][-10:]

        self._save_state()

        if self.on_change:
            self.on_change(event_type, path_str)

    def on_created(self, event: FileSystemEvent):
        """Handle file/directory creation."""
        if not event.is_directory:
            self._track_change("created", event.src_path)
        else:
            # Emit callback for directory creation so higher layers can react (e.g., project notes)
            if self.on_change and not self._should_ignore(event.src_path):
                try:
                    self.on_change("created", event.src_path)
                except Exception:
                    pass

    def on_modified(self, event: FileSystemEvent):
        """Handle file/directory modification."""
        if not event.is_directory:
            self._track_change("modified", event.src_path)

    def on_moved(self, event: FileSystemEvent):
        """Handle file/directory move."""
        # Track the move with both paths for files; emit callback for all
        if self._should_ignore(event.src_path) or self._should_ignore(event.dest_path):
            return

        src_str = str(Path(event.src_path).resolve())
        dest_str = str(Path(event.dest_path).resolve())

        if not event.is_directory:
            if dest_str not in self.state["files"]:
                self.state["files"][dest_str] = {
                    "created": datetime.now().isoformat(),
                    "modifications": []
                }

            self.state["files"][dest_str]["modifications"].append({
                "type": "moved",
                "from": src_str,
                "to": dest_str,
                "timestamp": datetime.now().isoformat()
            })

            # Keep only last 10 modifications
            if len(self.state["files"][dest_str]["modifications"]) > 10:
                self.state["files"][dest_str]["modifications"] = \
                    self.state["files"][dest_str]["modifications"][-10:]

            self._save_state()

        if self.on_change:
            # Pass both paths as a tuple for moves
            try:
                self.on_change("moved", (src_str, dest_str))
            except Exception:
                pass

    def on_deleted(self, event: FileSystemEvent):
        """Handle file/directory deletion."""
        if not event.is_directory:
            self._track_change("deleted", event.src_path)
        else:
            if self.on_change and not self._should_ignore(event.src_path):
                try:
                    self.on_change("deleted", event.src_path)
                except Exception:
                    pass

    def get_recent_changes(self, hours: int = 24) -> Dict[str, Dict]:
        """
        Get files changed in the last N hours.

        Args:
            hours: Number of hours to look back

        Returns:
            Dict of {file_path: file_info} for recently changed files
        """
        cutoff = datetime.now().timestamp() - (hours * 3600)
        recent = {}

        for path, info in self.state["files"].items():
            if info["modifications"]:
                last_mod = datetime.fromisoformat(info["modifications"][-1]["timestamp"])
                if last_mod.timestamp() > cutoff:
                    recent[path] = info

        return recent


def start_monitoring(
    directories: list[Path],
    state_file: Path,
    on_change: Optional[Callable] = None,
    ignore_dirs: Optional[Set[str]] = None,
    ignore_patterns: Optional[Set[str]] = None,
    include_paths: Optional[List[Path]] = None,
    exclude_paths: Optional[List[Path]] = None,
    ignore_globs: Optional[List[str]] = None,
) -> Observer:
    """
    Start monitoring directories for changes.

    Args:
        directories: List of directories to monitor
        state_file: Path to state tracking file
        on_change: Optional callback for changes
        ignore_dirs: Optional set of directory names to ignore

    Returns:
        Observer instance (call .stop() to stop monitoring)
    """
    event_handler = WKSFileMonitor(
        state_file=state_file,
        on_change=on_change,
        ignore_dirs=ignore_dirs,
        ignore_patterns=ignore_patterns,
        include_paths=include_paths or directories,
        exclude_paths=exclude_paths,
        ignore_globs=ignore_globs,
    )
    # Try observers in order of preference with fallback on start failures
    candidates: list[type] = []
    if FSEventsObserver is not None:
        candidates.append(FSEventsObserver)  # type: ignore
    if KqueueObserver is not None:
        candidates.append(KqueueObserver)  # type: ignore
    # Always include generic polling as last resort
    if PollingObserver is not None:
        candidates.append(PollingObserver)  # type: ignore
    # Default to base Observer if none of the above
    if not candidates:
        candidates = [Observer]  # type: ignore

    # Track last error encountered by the observer thread
    last_error: Optional[Exception] = None
    for Obs in candidates:
        try:
            observer = Obs()  # type: ignore
            for directory in directories:
                observer.schedule(event_handler, str(directory), recursive=True)
            observer.start()
            return observer
        except Exception as e:  # pragma: no cover
            last_error = e
            try:
                observer.stop()
            except Exception:
                pass
            continue
    # If we get here, all observers failed
    raise RuntimeError(f"Failed to start file observer: {last_error}")


if __name__ == "__main__":
    # Example usage
    from rich.console import Console

    console = Console()

    def on_file_change(event_type: str, path: str):
        console.print(f"[yellow]{event_type}[/yellow]: {path}")

    # Monitor home directory
    observer = start_monitoring(
        directories=[Path.home()],
        state_file=Path.home() / WKS_HOME_EXT / "monitor_state.json",
        on_change=on_file_change
    )

    try:
        console.print("[green]Monitoring started. Press Ctrl+C to stop.[/green]")
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
        console.print("[red]Monitoring stopped.[/red]")

    observer.join()
</file>

<file path="wks/__main__.py">
"""
Entry point for running wks.daemon as a module.

Loads ~/.wks/config.json to configure include/exclude paths.
"""

import json
import shutil
import subprocess
import time
from pathlib import Path

from .constants import WKS_HOME_EXT, WKS_HOME_DISPLAY
from .daemon import WKSDaemon
from .mongoctl import ensure_mongo_running
from .dbmeta import resolve_db_compatibility, IncompatibleDatabase
from .config import load_config
from .config_validator import validate_and_raise, ConfigValidationError
from .utils import get_package_version, expand_path
try:
    from .config import mongo_settings
    from .similarity import build_similarity_from_config
except Exception:
    build_similarity_from_config = None  # type: ignore

if __name__ == "__main__":
    # Load and validate config
    config = load_config()
    try:
        validate_and_raise(config)
    except ConfigValidationError as e:
        print(str(e))
        raise SystemExit(2)

    vault_path = expand_path(config.get("vault_path"))
    mongo_cfg = mongo_settings(config)
    space_compat_tag, time_compat_tag = resolve_db_compatibility(config)
    ensure_mongo_running(mongo_cfg['uri'], record_start=True)

    monitor_cfg = config.get("monitor", {})
    include_paths = [expand_path(p) for p in monitor_cfg.get("include_paths")]
    exclude_paths = [expand_path(p) for p in monitor_cfg.get("exclude_paths")]
    ignore_dirnames = set(monitor_cfg.get("ignore_dirnames"))
    ignore_patterns = set()  # deprecated
    ignore_globs = list(monitor_cfg.get("ignore_globs"))
    state_file = expand_path(monitor_cfg.get("state_file"))

    obsidian_cfg = config.get("obsidian", {})
    base_dir = obsidian_cfg.get("base_dir")
    # Explicit obsidian logging settings
    for k in ["log_max_entries", "active_files_max_rows", "source_max_chars", "destination_max_chars", "docs_keep"]:
        if k not in obsidian_cfg:
            print(f"Fatal: missing required config key: obsidian.{k}")
            raise SystemExit(2)
    log_max_entries = int(obsidian_cfg["log_max_entries"])
    active_rows = int(obsidian_cfg["active_files_max_rows"])
    src_max = int(obsidian_cfg["source_max_chars"])
    dst_max = int(obsidian_cfg["destination_max_chars"])
    docs_keep = int(obsidian_cfg["docs_keep"])

    metrics_cfg = config.get("metrics") or {}
    fs_short_window = float(metrics_cfg.get("fs_rate_short_window_secs", 10))
    fs_long_window = float(metrics_cfg.get("fs_rate_long_window_secs", 600))
    fs_short_weight = float(metrics_cfg.get("fs_rate_short_weight", 0.8))
    fs_long_weight = float(metrics_cfg.get("fs_rate_long_weight", 0.2))

    daemon = WKSDaemon(
        vault_path=vault_path,
        base_dir=base_dir,
        obsidian_log_max_entries=log_max_entries,
        obsidian_active_files_max_rows=active_rows,
        obsidian_source_max_chars=src_max,
        obsidian_destination_max_chars=dst_max,
        obsidian_docs_keep=docs_keep,
        monitor_paths=include_paths,
        state_file=state_file,
        ignore_dirnames=ignore_dirnames,
        exclude_paths=exclude_paths,
        ignore_patterns=ignore_patterns,
        ignore_globs=ignore_globs,
        mongo_uri=mongo_cfg.get("uri"),
        fs_rate_short_window_secs=fs_short_window,
        fs_rate_long_window_secs=fs_long_window,
        fs_rate_short_weight=fs_short_weight,
        fs_rate_long_weight=fs_long_weight,
    )

    # Similarity (explicit)
    sim_cfg_raw = config.get("similarity")
    if sim_cfg_raw is None or "enabled" not in sim_cfg_raw:
        print(f"Fatal: 'similarity.enabled' is required (true/false) in {WKS_HOME_DISPLAY}/config.json")
        raise SystemExit(2)
    if sim_cfg_raw.get("enabled"):
        if build_similarity_from_config is None:
            print("Fatal: similarity enabled but SimilarityDB not available")
            raise SystemExit(2)
        try:
            simdb, sim_cfg = build_similarity_from_config(
                config,
                require_enabled=True,
                compatibility_tag=space_compat_tag,
                product_version=get_package_version(),
            )
        except IncompatibleDatabase as exc:
            print(exc)
            raise SystemExit(2)
        except Exception as e:
            print(f"Fatal: failed to initialize similarity DB: {e}")
            raise SystemExit(2)
        if not simdb or not sim_cfg:
            print("Fatal: similarity initialization failed")
            raise SystemExit(2)
        daemon.similarity = simdb
        daemon.similarity_extensions = set([e.lower() for e in sim_cfg["include_extensions"]])
        daemon.similarity_min_chars = int(sim_cfg["min_chars"])
        print("Similarity indexing enabled.")

    # base_dir is set via constructor; no defaults applied

    print("Starting WKS daemon...")
    print("Press Ctrl+C to stop")

    try:
        daemon.run()
    except KeyboardInterrupt:
        print("\nStopping...")
</file>

<file path="wks/similarity.py">
from __future__ import annotations

import hashlib
import logging
import os
import math
import re
import time

logger = logging.getLogger(__name__)
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse, unquote

import numpy as np
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer

from .extractor import Extractor, ExtractResult
from .status import record_db_activity
from .dbmeta import ensure_db_compat, SPACE_COMPAT_DEFAULT
from .config import apply_similarity_mongo_defaults, mongo_settings


def _utc_now_iso() -> str:
    """Return current UTC timestamp in ISO 8601 with Z suffix."""
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")


def _parse_iso(ts: Optional[str]) -> Optional[datetime]:
    if not ts:
        return None
    try:
        s = ts.strip()
        if s.endswith("Z"):
            s = s[:-1] + "+00:00"
        return datetime.fromisoformat(s)
    except Exception:
        return None


def _as_file_uri(path: Path) -> str:
    try:
        return path.resolve().as_uri()
    except ValueError:
        return "file://" + path.resolve().as_posix()


class SimilarityDB:
    """Space database wrapper: manages embeddings and extraction artefacts."""

    def __init__(
        self,
        database_name: str = "wks_similarity",
        collection_name: str = "file_embeddings",
        mongo_uri: str = "mongodb://localhost:27017/",
        model_name: str = "all-MiniLM-L6-v2",
        model_path: Optional[str] = None,
        offline: bool = False,
        min_chars: int = 10,
        max_chars: int = 200_000,
        chunks_collection: str = "file_chunks",
        chunk_chars: int = 1_500,
        chunk_overlap: int = 200,
        extract_engine: str = "docling",
        extract_ocr: bool = False,
        extract_timeout_secs: int = 30,
        extract_options: Optional[Dict[str, Any]] = None,
        write_extension: Optional[str] = None,
        mongo_client: Optional[MongoClient] = None,
        compatibility_tag: str = SPACE_COMPAT_DEFAULT,
        product_version: Optional[str] = None,
    ) -> None:
        if mongo_client is not None:
            self.client = mongo_client
            self._own_client = False
        else:
            try:
                self.client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
                # Test connection immediately
                self.client.admin.command('ping')
            except Exception as e:
                from .error_messages import mongodb_connection_error
                mongodb_connection_error(mongo_uri, e)
            self._own_client = True
        self.db = self.client[database_name]
        ensure_db_compat(self.client, database_name, "space", compatibility_tag, product_version=product_version)
        self.collection = self.db[collection_name]
        self.chunks = self.db[chunks_collection]
        self.changes = self.db["embedding_changes"]

        if offline:
            os.environ.setdefault("HF_HUB_OFFLINE", "1")
            os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")
        os.environ.setdefault("SENTENCE_TRANSFORMERS_DISABLE_DEFAULT_PROGRESS_BAR", "1")
        logging.getLogger("sentence_transformers").setLevel(logging.WARNING)

        target = (model_path or model_name).strip()
        self.model_name = model_name
        try:
            self.model = SentenceTransformer(target)
        except Exception as e:
            from .error_messages import model_download_error
            model_download_error(target, e)

        self.min_chars = int(min_chars)
        self.max_chars = int(max_chars)
        self.chunk_chars = int(chunk_chars)
        self.chunk_overlap = int(chunk_overlap)

        self.extractor = Extractor(
            engine=extract_engine or "docling",
            ocr=bool(extract_ocr),
            timeout_secs=int(extract_timeout_secs),
            options=dict(extract_options or {}),
            max_chars=self.max_chars,
            write_extension=write_extension or "md",
        )

        self._ensure_indexes()
        self._empty_embedding: Optional[List[float]] = None
        self._last_add_result: Optional[Dict[str, Any]] = None

    # ------------------------------------------------------------------ #
    def _ensure_indexes(self) -> None:
        self.collection.create_index("path", unique=True)
        self.collection.create_index("checksum")
        self.collection.create_index("path_local")
        self.chunks.create_index([("file_path", 1), ("chunk_index", 1)], unique=True)
        self.chunks.create_index([("file_path", 1), ("chunk_id", 1)], unique=True)
        self.chunks.create_index("timestamp")
        self.changes.create_index([("file_path", 1), ("t_new_epoch", 1)])

    @staticmethod
    def _file_digest(path: Path) -> Tuple[str, int]:
        hasher = hashlib.sha256()
        size = 0
        with open(path, "rb") as fh:
            for chunk in iter(lambda: fh.read(1024 * 1024), b""):
                if not chunk:
                    break
                hasher.update(chunk)
                size += len(chunk)
        return hasher.hexdigest(), size

    @staticmethod
    def _chunk_text(text: str, chunk_chars: int, chunk_overlap: int) -> List[str]:
        if not text:
            return []
        n = max(256, int(chunk_chars))
        ov = max(0, min(int(chunk_overlap), n - 1))
        chunks: List[str] = []
        i = 0
        L = len(text)
        while i < L:
            j = min(L, i + n)
            chunk = text[i:j]
            if chunk:
                chunks.append(chunk)
            if j >= L:
                break
            i = j - ov
        if not chunks:
            chunks.append(text)
        return chunks

    @staticmethod
    def _cleanup_content_file(content_path: Optional[str]) -> None:
        if not content_path:
            return
        try:
            artefact = Path(content_path)
            if artefact.exists():
                artefact.unlink()
                parent = artefact.parent
                if parent.is_dir():
                    try:
                        next(parent.iterdir())
                    except StopIteration:
                        parent.rmdir()
        except Exception:
            pass

    def _update_related_paths(self, old_uri: Optional[str], new_uri: str, new_local: str) -> None:
        if not old_uri or old_uri == new_uri:
            return
        try:
            self.chunks.update_many(
                {"file_path": old_uri},
                {"$set": {"file_path": new_uri, "file_local": new_local}},
            )
        except Exception:
            pass

    def close(self) -> None:
        if getattr(self, "_own_client", False):
            try:
                self.client.close()
            except Exception:
                pass

    # ------------------------------------------------------------------ #
    def _embed_text(
        self,
        file_uri: str,
        path_local: str,
        text: str,
        content_checksum: Optional[str],
        timestamp: str,
    ) -> Tuple[List[float], List[Dict[str, Any]]]:
        chunks = self._chunk_text(text, self.chunk_chars, self.chunk_overlap)
        vectors: List[np.ndarray] = []
        docs: List[Dict[str, Any]] = []
        for idx, chunk in enumerate(chunks):
            try:
                vec = np.array(self.model.encode(chunk, show_progress_bar=False), dtype=float)
            except Exception:
                continue
            vectors.append(vec)
            docs.append(
                {
                    "file_path": file_uri,
                    "file_local": path_local,
                    "chunk_index": idx,
                    "chunk_id": idx,  # backwards compatibility
                    "timestamp": timestamp,
                    "text_preview": chunk[:400],
                    "embedding": vec.tolist(),
                    "content_checksum": content_checksum,
                }
            )
        if vectors:
            return np.mean(np.stack(vectors, axis=0), axis=0).tolist(), docs
        try:
            vec = self.model.encode(text, show_progress_bar=False)
            return np.array(vec, dtype=float).tolist(), docs
        except Exception:
            return [], docs

    def _empty_embed(self) -> List[float]:
        if self._empty_embedding is not None:
            return self._empty_embedding
        try:
            self._empty_embedding = self.model.encode("", show_progress_bar=False).tolist()
        except Exception:
            self._empty_embedding = []
        return self._empty_embedding

    @staticmethod
    def _angle_deg(a: List[float], b: List[float]) -> Optional[float]:
        if not a or not b:
            return None
        va = np.array(a, dtype=float)
        vb = np.array(b, dtype=float)
        denom = float(np.linalg.norm(va) * np.linalg.norm(vb))
        if denom <= 0:
            return None
        cosv = float(np.dot(va, vb) / denom)
        cosv = max(-1.0, min(1.0, cosv))
        try:
            return math.degrees(math.acos(cosv))
        except ValueError:
            return None

    def angle_from_empty(self, embedding: List[float]) -> Optional[float]:
        base = self._empty_embed()
        return self._angle_deg(base, embedding)

    # ------------------------------------------------------------------ #
    def get_file_embedding(self, path: Path) -> Optional[List[float]]:
        file_uri = _as_file_uri(path.expanduser().resolve())
        try:
            doc = self.collection.find_one({"path": file_uri})
        except Exception:
            doc = None
        if not doc:
            return None
        record_db_activity("similarity.get_file_embedding", str(path))
        return doc.get("embedding")

    def add_file(
        self,
        path: Path,
        *,
        extraction: Optional[ExtractResult] = None,
        force: bool = False,
        file_checksum: Optional[str] = None,
        file_bytes: Optional[int] = None,
    ) -> bool:
        path = path.expanduser().resolve()
        if not path.exists() or not path.is_file():
            self._last_add_result = {
                "updated": False,
                "path_local": str(path),
                "error": "missing",
            }
            return False

        file_uri = _as_file_uri(path)
        path_local = str(path)

        timings: Dict[str, float] = {}

        checksum_start = time.perf_counter()
        computed_checksum = False
        if file_checksum is None or file_bytes is None:
            file_checksum, file_bytes = self._file_digest(path)
            computed_checksum = True
        timings["checksum"] = time.perf_counter() - checksum_start if computed_checksum else 0.0

        existing = (
            self.collection.find_one({"path": file_uri})
            or self.collection.find_one({"path_local": path_local})
        )
        rename_from_uri: Optional[str] = None
        if not existing:
            other = self.collection.find_one({"checksum": file_checksum})
            if other and other.get("path") != file_uri:
                rename_from_uri = other.get("path")
                existing = other

        rename_detected = bool(rename_from_uri and rename_from_uri != file_uri)

        if existing and not force and not rename_detected:
            if existing.get("checksum") == file_checksum:
                self._last_add_result = {
                    "updated": False,
                    "path": existing.get("path") or file_uri,
                    "path_local": existing.get("path_local") or path_local,
                    "checksum": file_checksum,
                    "content_checksum": existing.get("content_checksum"),
                    "content_hash": existing.get("content_checksum"),
                }
                return False

        try:
            result = extraction or self.extractor.extract(path, persist=True)
        except Exception as exc:
            self._last_add_result = {
                "updated": False,
                "path": file_uri,
                "path_local": path_local,
                "error": str(exc),
            }
            return False

        text = (result.text or "").strip()
        if len(text) < self.min_chars and not force:
            self._last_add_result = {
                "updated": False,
                "path": file_uri,
                "path_local": path_local,
                "skipped": "min_chars",
            }
            return False

        content_path = str(result.content_path) if result.content_path else None
        content_checksum = result.content_checksum
        content_bytes = result.content_bytes

        prev_embedding = existing.get("embedding") if existing else None
        prev_timestamp = existing.get("timestamp") if existing else None
        prev_content_path = existing.get("content_path") if existing else None

        now_iso = _utc_now_iso()
        embed_start = time.perf_counter()
        embedding, chunk_docs = self._embed_text(
            file_uri=file_uri,
            path_local=path_local,
            text=result.text or "",
            content_checksum=content_checksum,
            timestamp=now_iso,
        )
        timings["embed"] = time.perf_counter() - embed_start
        angle = self.angle_from_empty(embedding)

        if file_bytes is None:
            try:
                file_bytes = path.stat().st_size
            except Exception:
                file_bytes = 0

        doc_payload = {
            "path": file_uri,
            "path_local": path_local,
            "filename": path.name,
            "parent": str(path.parent),
            "timestamp": now_iso,
            "model": self.model_name,
            "checksum": file_checksum,
            "bytes": int(file_bytes),
            "content_path": content_path,
            "content_checksum": content_checksum,
            "content_hash": content_checksum,  # backwards compatibility
            "content_bytes": int(content_bytes) if content_bytes is not None else None,
            "embedding": embedding,
            "angle": angle,
            "text_preview": (result.text or "")[:500],
            "num_chunks": len(chunk_docs),
            "chunk_chars": self.chunk_chars,
            "chunk_overlap": self.chunk_overlap,
        }

        target = {"_id": existing["_id"]} if existing and "_id" in existing else {"path": file_uri}
        db_start = time.perf_counter()
        self.collection.update_one(target, {"$set": doc_payload}, upsert=True)
        timings["db_update"] = time.perf_counter() - db_start
        record_db_activity("similarity.add_file", str(path))

        if rename_from_uri and rename_from_uri != file_uri:
            self._update_related_paths(rename_from_uri, file_uri, path_local)

        try:
            chunk_start = time.perf_counter()
            self.chunks.delete_many({"file_path": file_uri})
            if chunk_docs:
                self.chunks.insert_many(chunk_docs, ordered=False)
            timings["chunks"] = time.perf_counter() - chunk_start
        except Exception:
            timings.setdefault("chunks", 0.0)
            pass

        try:
            if prev_embedding:
                prev_dt = _parse_iso(prev_timestamp) or datetime.now(timezone.utc)
                now_dt = datetime.now(timezone.utc)
                prev_vec = np.array(prev_embedding, dtype=float)
                new_vec = np.array(embedding, dtype=float)
                denom = float(np.linalg.norm(prev_vec) * np.linalg.norm(new_vec))
                if denom > 0:
                    cosv = float(np.dot(prev_vec, new_vec) / denom)
                    cosv = max(-1.0, min(1.0, cosv))
                    degrees = math.degrees(math.acos(cosv))
                else:
                    degrees = 0.0
                seconds = max(1.0, (now_dt - prev_dt).total_seconds())
                self.changes.insert_one(
                    {
                        "file_path": file_uri,
                        "t_prev": prev_dt.replace(microsecond=0).isoformat().replace("+00:00", "Z"),
                        "t_new": now_iso,
                        "t_new_epoch": int(now_dt.timestamp()),
                        "seconds": float(seconds),
                        "degrees": float(degrees),
                    }
                )
        except Exception:
            pass

        if prev_content_path and prev_content_path != content_path:
            self._cleanup_content_file(prev_content_path)

        self._last_add_result = {
            "updated": True,
            "path": file_uri,
            "path_local": path_local,
            "checksum": file_checksum,
            "content_checksum": content_checksum,
            "content_hash": content_checksum,
            "content_path": content_path,
            "text": result.text,
            "timings": timings,
        }
        return True

    def get_last_add_result(self) -> Optional[Dict[str, Any]]:
        return self._last_add_result

    def remove_file(self, path: Path) -> bool:
        path = path.expanduser().resolve()
        file_uri = _as_file_uri(path)
        doc = self.collection.find_one({"path": file_uri}) or self.collection.find_one(
            {"path_local": str(path)}
        )
        if not doc:
            self._last_add_result = {
                "removed": False,
                "path": file_uri,
                "path_local": str(path),
                "error": "not_found",
            }
            return False
        self.collection.delete_one({"_id": doc["_id"]})
        try:
            self.chunks.delete_many({"file_path": doc.get("path")})
        except Exception:
            pass
        try:
            self.changes.delete_many({"file_path": doc.get("path")})
        except Exception:
            pass
        try:
            self.db["file_snapshots"].delete_many({"path": doc.get("path")})
        except Exception:
            pass
        self._cleanup_content_file(doc.get("content_path"))
        record_db_activity("similarity.remove_file", str(path))
        self._last_add_result = {
            "removed": True,
            "path": doc.get("path"),
            "path_local": doc.get("path_local"),
        }
        return True

    def rename_file(self, src: Path, dest: Path) -> bool:
        src_uri = _as_file_uri(src.expanduser().resolve())
        dest = dest.expanduser().resolve()
        dest_uri = _as_file_uri(dest)
        doc = self.collection.find_one({"path": src_uri})
        if not doc:
            return False
        now_iso = _utc_now_iso()
        update = {
            "path": dest_uri,
            "path_local": str(dest),
            "filename": dest.name,
            "parent": str(dest.parent),
            "timestamp": now_iso,
        }
        self.collection.update_one({"_id": doc["_id"]}, {"$set": update})
        self._update_related_paths(src_uri, dest_uri, str(dest))
        record_db_activity("similarity.rename_file", f"{src} -> {dest}")
        return True

    def rename_folder(self, old_dir: Path, new_dir: Path) -> int:
        old_dir = old_dir.expanduser().resolve()
        new_dir = new_dir.expanduser().resolve()
        old_local = str(old_dir)
        new_local = str(new_dir)
        pattern = f"^{re.escape(old_local)}/"
        cursor = self.collection.find({"path_local": {"$regex": pattern}})
        updated = 0
        for doc in cursor:
            local = doc.get("path_local") or ""
            if not local.startswith(old_local):
                continue
            new_path_local = local.replace(old_local, new_local, 1)
            dest_path = Path(new_path_local)
            dest_uri = _as_file_uri(dest_path)
            now_iso = _utc_now_iso()
            self.collection.update_one(
                {"_id": doc["_id"]},
                {
                    "$set": {
                        "path": dest_uri,
                        "path_local": new_path_local,
                        "filename": dest_path.name,
                        "parent": str(dest_path.parent),
                        "timestamp": now_iso,
                    }
                },
            )
            self._update_related_paths(doc.get("path"), dest_uri, new_path_local)
            updated += 1
        if updated:
            record_db_activity("similarity.rename_folder", f"{old_dir} -> {new_dir} ({updated} files)")
        return updated

    # ------------------------------------------------------------------ #
    def find_similar(
        self,
        query_path: Optional[Path] = None,
        query_text: Optional[str] = None,
        limit: int = 10,
        min_similarity: float = 0.0,
        mode: str = "file",
    ) -> List[Tuple[str, float]]:
        if query_path:
            try:
                text = self.extractor.read_text(query_path.expanduser().resolve())
            except Exception:
                return []
            query_embedding = self.model.encode(text, show_progress_bar=False)
        elif query_text:
            query_embedding = self.model.encode(query_text, show_progress_bar=False)
        else:
            raise ValueError("Must provide either query_path or query_text")

        results: List[Tuple[str, float]] = []
        if mode == "chunk":
            best: Dict[str, float] = {}
            for doc in self.chunks.find():
                emb = doc.get("embedding")
                if not emb:
                    continue
                sim = self._cosine_similarity(query_embedding, emb)
                if sim < min_similarity:
                    continue
                uri = doc.get("file_path")
                if sim > best.get(uri, -1.0):
                    best[uri] = sim
            results = list(best.items())
        else:
            for doc in self.collection.find():
                emb = doc.get("embedding")
                if not emb:
                    continue
                sim = self._cosine_similarity(query_embedding, emb)
                if sim >= min_similarity:
                    results.append((doc.get("path"), sim))
        results.sort(key=lambda item: item[1], reverse=True)
        return results[:limit]

    @staticmethod
    def _cosine_similarity(a: Any, b: Any) -> float:
        va = np.array(a, dtype=float)
        vb = np.array(b, dtype=float)
        denom = float(np.linalg.norm(va) * np.linalg.norm(vb))
        if denom <= 0:
            return 0.0
        return float(np.dot(va, vb) / denom)

    def get_stats(self) -> Dict[str, Any]:
        try:
            total_docs = self.collection.count_documents({})
            total_bytes = (
                self.collection.aggregate(
                    [{"$group": {"_id": None, "bytes": {"$sum": {"$ifNull": ["$bytes", 0]}}}}]
                )
            )
            agg = next(total_bytes, None)
            record_db_activity("similarity.get_stats", None)
            return {
                "database": self.db.name,
                "collection": self.collection.name,
                "total_files": int(total_docs),
                "total_bytes": int(agg["bytes"]) if agg and agg.get("bytes") is not None else None,
            }
        except Exception:
            record_db_activity("similarity.get_stats.error", None)
            return {
                "database": self.db.name,
                "collection": self.collection.name,
                "total_files": 0,
                "total_bytes": None,
            }

    # ------------------------------------------------------------------ #
    def _resolve_local_path(self, doc: Dict[str, Any]) -> Optional[Path]:
        local = doc.get("path_local")
        if isinstance(local, str) and local:
            try:
                return Path(local).expanduser()
            except Exception:
                pass
        uri = doc.get("path")
        if isinstance(uri, str) and uri:
            if uri.startswith("file://"):
                try:
                    parsed = urlparse(uri)
                    return Path(unquote(parsed.path or "")).expanduser()
                except Exception:
                    return None
            try:
                return Path(uri).expanduser()
            except Exception:
                return None
        return None

    def _purge_document(self, doc: Dict[str, Any]) -> None:
        try:
            self.collection.delete_one({"_id": doc["_id"]})
        except Exception:
            pass
        try:
            self.chunks.delete_many({"file_path": doc.get("path")})
        except Exception:
            pass
        try:
            self.changes.delete_many({"file_path": doc.get("path")})
        except Exception:
            pass
        try:
            self.db["file_snapshots"].delete_many({"path": doc.get("path")})
        except Exception:
            pass
        self._cleanup_content_file(doc.get("content_path"))

    def audit_documents(
        self,
        remove_missing: bool = True,
        fix_missing_metadata: bool = True,
    ) -> Dict[str, int]:
        results = {"removed": 0, "updated": 0}
        cursor = self.collection.find(
            {},
            {
                "_id": 1,
                "path": 1,
                "path_local": 1,
                "bytes": 1,
                "content_path": 1,
                "content_checksum": 1,
                "content_bytes": 1,
            },
        )
        for doc in cursor:
            local_path = self._resolve_local_path(doc)
            exists = local_path is not None and local_path.exists()
            doc_label = doc.get("path_local") or doc.get("path") or ""

            if remove_missing and local_path is not None and not exists:
                self._purge_document(doc)
                results["removed"] += 1
                record_db_activity("similarity.audit.remove", doc_label)
                continue

            updates: Dict[str, Any] = {}
            if fix_missing_metadata and exists and local_path is not None:
                try:
                    size = local_path.stat().st_size
                except Exception:
                    size = None
                if size is not None and doc.get("bytes") != size:
                    updates["bytes"] = int(size)

            content_path = doc.get("content_path")
            if content_path and isinstance(content_path, str):
                try:
                    artefact = Path(content_path)
                    if not artefact.exists():
                        updates["content_path"] = None
                        updates["content_checksum"] = None
                        updates["content_bytes"] = None
                except Exception:
                    updates["content_path"] = None
                    updates["content_checksum"] = None
                    updates["content_bytes"] = None

            if updates:
                try:
                    self.collection.update_one({"_id": doc["_id"]}, {"$set": updates})
                    results["updated"] += 1
                    record_db_activity("similarity.audit.fix", doc_label)
                except Exception:
                    pass

        return results


def build_similarity_from_config(
    cfg: Dict[str, Any],
    *,
    require_enabled: bool = True,
    compatibility_tag: str = SPACE_COMPAT_DEFAULT,
    product_version: Optional[str] = None,
) -> Tuple[Optional[SimilarityDB], Optional[Dict[str, Any]]]:
    """
    Construct SimilarityDB from a configuration dictionary.

    Returns (db, normalized_similarity_config). When require_enabled=False and
    similarity is disabled, (None, None) is returned.
    """
    sim_raw = cfg.get("similarity") or {}
    enabled = bool(sim_raw.get("enabled"))
    if not enabled:
        if require_enabled:
            raise RuntimeError("similarity.enabled must be true")
        return None, None
    mongo_cfg = mongo_settings(cfg)
    sim_cfg = apply_similarity_mongo_defaults(sim_raw, mongo_cfg)
    required = [
        "mongo_uri",
        "database",
        "collection",
        "model",
        "include_extensions",
        "min_chars",
        "max_chars",
        "chunk_chars",
        "chunk_overlap",
    ]
    missing = [k for k in required if k not in sim_cfg]
    if missing:
        raise RuntimeError("missing similarity keys: " + ", ".join([f"similarity.{k}" for k in missing]))
    extract_cfg = cfg.get("extract")
    if extract_cfg is None or "engine" not in extract_cfg or "ocr" not in extract_cfg or "timeout_secs" not in extract_cfg:
        raise RuntimeError("'extract.engine', 'extract.ocr', and 'extract.timeout_secs' are required in config")
    if str(extract_cfg.get("engine")).lower() != "docling":
        raise RuntimeError("extract.engine must be 'docling'")
    db = SimilarityDB(
        database_name=sim_cfg["database"],
        collection_name=sim_cfg["collection"],
        mongo_uri=sim_cfg["mongo_uri"],
        model_name=sim_cfg["model"],
        model_path=sim_cfg.get("model_path"),
        offline=bool(sim_cfg.get("offline", False)),
        min_chars=int(sim_cfg["min_chars"]),
        max_chars=int(sim_cfg["max_chars"]),
        chunk_chars=int(sim_cfg["chunk_chars"]),
        chunk_overlap=int(sim_cfg["chunk_overlap"]),
        extract_engine=extract_cfg["engine"],
        extract_ocr=bool(extract_cfg["ocr"]),
        extract_timeout_secs=int(extract_cfg["timeout_secs"]),
        extract_options=dict(extract_cfg.get("options") or {}),
        write_extension=extract_cfg.get("write_extension"),
        compatibility_tag=compatibility_tag,
        product_version=product_version,
    )
    return db, sim_cfg
</file>

<file path="setup.py">
from setuptools import setup, find_packages

setup(
    name="wks",
    version="0.2.8",
    description="Wieselquist Knowledge System - AI-assisted file organization",
    author="William Wieselquist",
    packages=find_packages(),
    python_requires=">=3.8",
    install_requires=[
        "watchdog",           # File system monitoring
        "rich",               # Terminal formatting
        "pymongo",            # MongoDB
        "sentence-transformers",  # Embeddings
        "docling",            # Required content extraction engine
        "bsdiff4",            # Binary diff size for change snapshots
        "mongomock",          # In-memory MongoDB for tests
        "jinja2",             # Template rendering for CLI outputs
        "pytest>=7.0",        # Testing framework
        "pytest-timeout>=2.1",  # Test timeouts
    ],
    entry_points={
        "console_scripts": [
            # Primary CLI name (dev version with zero)
            "wks0=wks.cli:main",
        ],
    },
)
</file>

<file path="README.md">
# WKS (Wieselquist Knowledge System)

## Install


```
source venv/bin/activate
pip install -e .
pipx install --force .
```


## Documentation

- Instructions for the CODEX developer: `AGENTS.md`
- How to develop `CONTRIBUTING.md`
- Development plan `ROADMAP.md`
- Specification `SPEC.md`
- Changes `CHANGELOG.md`
</file>

<file path="SPEC.md">
# WKS Command-Line Utility (wks0)

This spec documents the wks0 CLI: a layered architecture for filesystem monitoring, knowledge graph management, and semantic indexing.

## Architecture Overview

WKS is built as a stack of independent, composable layers:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Patterns (CLAUDE.md)                                ‚îÇ
‚îÇ  Organizational guidance, not configuration          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Search Layer                                        ‚îÇ
‚îÇ  Combines indices with weights                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Index Layer                                         ‚îÇ
‚îÇ  Multiple independent indices (RAG, AST, etc.)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Semantic Engines (pluggable)                        ‚îÇ
‚îÇ  Related | Diff | Extract                            ‚îÇ
‚îÇ  Each with _router for engine selection              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Vault Layer (Obsidian)                              ‚îÇ
‚îÇ  Knowledge graph: links only                         ‚îÇ
‚îÇ  DB: wks_vault.links                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Monitor Layer                                       ‚îÇ
‚îÇ  Filesystem state: paths, checksums, priorities      ‚îÇ
‚îÇ  DB: wks.monitor                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Installation
- Recommended: `pipx install .`
- Optional: `pipx runpip wks0 install docling` (for PDF/Office extraction)
- Python: 3.10+

## Configuration

Stored at `~/.wks/config.json`

```json
{
  "monitor": {
    "include_paths": ["~"],
    "exclude_paths": ["~/Library", "~/obsidian", "~/.wks", "~/miniforge3"],
    "ignore_dirnames": [
      ".cache", ".venv", "__pycache__", "_build",
      "build", "dist", "node_modules", "venv"
    ],
    "ignore_globs": [
      "**/.DS_Store", "*.swp", "*.tmp", "*~", "._*", "~$*", ".~lock.*#"
    ],
    "managed_directories": {
      "~/Desktop": 150,
      "~/deadlines": 120,
      "~": 100,
      "~/Documents": 100,
      "~/Pictures": 80,
      "~/Downloads": 50
    },
    "priority": {
      "depth_multiplier": 0.9,
      "underscore_divisor": 2,
      "single_underscore_divisor": 64,
      "extension_weights": {
        ".docx": 1.3,
        ".pptx": 1.3,
        ".pdf": 1.1,
        "default": 1.0
      },
      "auto_index_min": 2
    },
    "database": "wks.monitor",
    "log_file": "~/.wks/monitor.log"
  },

  "vault": {
    "type": "obsidian",
    "base_dir": "~/obsidian",
    "wks_dir": "WKS",
    "update_frequency_seconds": 10,
    "database": "wks.vault"
  },

  "db": {
    "type": "mongodb",
    "uri": "mongodb://localhost:27017/"
  },

  "extract": {
    "output_dir_rules": {
      "resolve_symlinks": true,
      "git_parent": true,
      "underscore_sibling": true
    },
    "engines": {
      "docling": {
        "enabled": true,
        "is_default": true,
        "ocr": false,
        "timeout_secs": 30,
        "write_extension": "md"
      },
      "builtin": {
        "enabled": true,
        "max_chars": 200000
      }
    },
    "_router": {
      "rules": [
        {"extensions": [".pdf", ".docx", ".pptx"], "engine": "docling"},
        {"extensions": [".txt", ".md", ".py"], "engine": "builtin"}
      ],
      "fallback": "builtin"
    }
  },

  "diff": {
    "engines": {
      "bdiff": {
        "enabled": true,
        "is_default": true,
        "algorithm": "bsdiff"
      },
      "text": {
        "enabled": true,
        "algorithm": "unified",
        "context_lines": 3
      }
    },
    "_router": {
      "rules": [
        {"extensions": [".txt", ".md", ".py", ".json"], "engine": "text"},
        {"mime_prefix": "text/", "engine": "text"}
      ],
      "fallback": "bdiff"
    }
  },

  "related": {
    "engines": {
      "embedding": {
        "enabled": true,
        "is_default": true,
        "model": "all-MiniLM-L6-v2",
        "min_chars": 10,
        "max_chars": 200000,
        "chunk_chars": 1500,
        "chunk_overlap": 200,
        "offline": true,
        "database": "wks_similarity",
        "collection": "file_embeddings"
      },
      "diff_based": {
        "enabled": false,
        "threshold": 0.7,
        "database": "wks_similarity",
        "collection": "diff_similarity"
      }
    },
    "_router": {
      "default": "embedding",
      "rules": [
        {"priority_min": 50, "engine": "embedding"}
      ]
    }
  },

  "index": {
    "indices": {
      "main": {
        "enabled": true,
        "type": "embedding",
        "include_extensions": [
          ".md", ".txt", ".py", ".ipynb", ".tex",
          ".docx", ".pptx", ".pdf", ".html",
          ".csv", ".xlsx"
        ],
        "respect_monitor_ignores": false,
        "respect_priority": true,
        "database": "wks_index_main",
        "collection": "documents"
      },
      "code": {
        "enabled": false,
        "type": "ast",
        "include_extensions": [".py", ".js", ".ts", ".cpp"],
        "database": "wks_index_code",
        "collection": "code_blocks"
      }
    }
  },

  "search": {
    "default_index": "main",
    "combine": {
      "enabled": false,
      "indices": ["main", "code"],
      "weights": {
        "main": 0.7,
        "code": 0.3
      }
    }
  },

  "display": {
    "timestamp_format": "%Y-%m-%d %H:%M:%S"
  }
}
```

## Layer Descriptions

### Monitor Layer

**Purpose**: Track filesystem state and calculate priorities

**Database**: `wks.monitor`

**Schema**:
- `path` ‚Äî absolute URI (e.g., `file:///Users/ww5/Documents/report.pdf`)
- `timestamp` ‚Äî ISO 8601 UTC string for last modification
- `checksum` ‚Äî SHA256 hash of file contents
- `bytes` ‚Äî file size in bytes
- `priority` ‚Äî calculated integer score (1-‚àû) based on path structure

**Priority Calculation**:
1. Match file to deepest `managed_directories` entry (e.g., `~/Documents` ‚Üí 100)
2. For each path component after base: multiply by `depth_multiplier` (0.9)
3. For each leading `_` in component name: divide by `underscore_divisor` (2)
4. If component is single `_`: divide by `single_underscore_divisor` (64)
5. Multiply by extension weight from `extension_weights`
6. Round to integer (minimum 1)

**Example**: `/Users/ww5/Documents/reports/_old/draft.pdf`
- Matches: `~/Documents` (base = 100)
- `reports`: 100 √ó 0.9 = 90
- `_old`: 90 √ó 0.9 √ó 0.5 = 40.5
- Extension `.pdf`: 40.5 √ó 1.1 = 44.55
- **Priority: 45**

**Commands**:
- `wks0 monitor status` ‚Äî show monitoring statistics (supports `--live` for auto-updating display)
- `wks0 monitor include_paths add <path>` ‚Äî add path to include_paths
- `wks0 monitor exclude_paths add <path>` ‚Äî add path to exclude_paths
- `wks0 db monitor` ‚Äî query filesystem database

### Vault Layer

**Purpose**: Maintain knowledge graph (links only)

**Database**: `wks_vault.links`

**Schema**:
- `from_path` ‚Äî source URI (typically vault-internal)
- `to_path` ‚Äî target URI (vault-internal or external `file:///`)
- `link_type` ‚Äî wikilink, markdown, embed
- `line_number` ‚Äî optional, for precise location
- `last_observed` ‚Äî timestamp when link was last seen

**Service Workflow**:
1. **Monitor ‚Üí Vault**: Watch `wks.monitor` for path changes
   - Query `wks_vault.links` for all references to changed path
   - Update Obsidian files with new paths
   - Update vault DB
2. **Vault ‚Üí DB**: Periodically scan Obsidian files
   - Parse all links
   - Sync to `wks_vault.links`
   - Obsidian handles internal vault moves, we observe and record

**Generated Files**:
- `WKS/Health.md` ‚Äî daemon metrics
- `WKS/Activity.md` ‚Äî recently modified files
- `WKS/FileOperations.md` ‚Äî move/rename log
- `WKS/Extractions/` ‚Äî extracted content snapshots (keep latest N)

**Commands**:
- `wks0 vault status` ‚Äî link health summary
- `wks0 vault links <file>` ‚Äî show inbound/outbound links for file
- `wks0 vault orphans` ‚Äî find files with no links
- `wks0 vault check` ‚Äî scan vault and sync links table
- `wks0 db vault` ‚Äî query links database

### Extract Layer

**Purpose**: Convert documents to plain text with pluggable engines

**Engines**:
- `docling` ‚Äî PDF, DOCX, PPTX via IBM Docling
- `builtin` ‚Äî Plain text files

**Router**: `_router` selects engine based on file extension

**Output Location Rules** (applied in order):
1. **Resolve symlinks**: Get real path before applying other rules
2. **Git parent**: If file is in git repo, place `.wkso/` above repo root
3. **Underscore sibling**: If any path component starts with `_`, place `.wkso/` as sibling to topmost `_`-prefixed directory
4. **Default**: Place `.wkso/` as sibling to source file

**Output Format**: `<.wkso_dir>/<checksum>.<extension>`

**Commands**:
- `wks0 extract <file>` ‚Äî extract file with default engine
- `wks0 extract <file> --engine builtin` ‚Äî force specific engine

**MCP Integration**: Extract engines can be exposed as MCP tools

### Diff Layer

**Purpose**: Calculate differences between file versions

**Engines**:
- `bdiff` ‚Äî Binary diff (bsdiff algorithm)
- `text` ‚Äî Unified text diff

**Router**: `_router` selects engine based on file type

**Commands**:
- `wks0 diff <file1> <file2>` ‚Äî diff with default engine
- `wks0 diff <file1> <file2> --engine text` ‚Äî force specific engine

**MCP Integration**: Diff engines can be exposed as MCP tools

### Related Layer

**Purpose**: Find semantically similar documents

**Engines**:
- `embedding` ‚Äî Sentence transformer embeddings (all-MiniLM-L6-v2)
- `diff_based` ‚Äî Similarity based on diff size

**Router**: `_router` selects engine based on context

**Database**: Each engine has its own database/collection

**Commands**:
- `wks0 related <file>` ‚Äî find similar files
- `wks0 related <file> --limit 10 --min-similarity 0.5`
- `wks0 related <file> --engine embedding`

**MCP Integration**: Related engines can be exposed as MCP tools

### Index Layer

**Purpose**: Multiple independent indices for different use cases

**Indices**:
- `main` ‚Äî General embedding-based index for documents
- `code` ‚Äî AST-based index for code (optional)

**No Router**: Indexing is a decision, not routed automatically

**Schema** (per index):
- `path` ‚Äî file URI
- `content_path` ‚Äî extracted text location
- `embedding` ‚Äî vector representation
- `angle` ‚Äî degrees from empty string embedding
- `metadata` ‚Äî index-specific metadata

**Commands**:
- `wks0 index <file>` ‚Äî index with default index
- `wks0 index <file> --index code` ‚Äî use specific index
- `wks0 index --untrack <file>` ‚Äî remove from all indices

### Search Layer

**Purpose**: Query indices with optional combination

**Features**:
- Single index search (default)
- Multi-index search with weighted combination

**Commands**:
- `wks0 search <query>` ‚Äî search default index
- `wks0 search <query> --index code` ‚Äî search specific index
- `wks0 search <query> --combine` ‚Äî search all indices with weights

## Database Commands

All layers store data in MongoDB:

```bash
# Query databases
wks0 db monitor              # Filesystem state
wks0 db vault                # Knowledge graph links
wks0 db related              # Similarity embeddings
wks0 db index                # Search indices

# Reset databases (destructive)
wks0 db reset monitor        # Clear filesystem state
wks0 db reset vault          # Clear link graph
wks0 db reset related        # Clear embeddings
wks0 db reset index          # Clear all indices
```

## Service Management

```bash
wks0 service install         # Install launchd service (macOS)
wks0 service uninstall       # Remove service
wks0 service start           # Start daemon
wks0 service stop            # Stop daemon
wks0 service restart         # Restart daemon
wks0 service status          # Show status and metrics (supports --live for auto-updating display)
```

## Config Management

```bash
wks0 config                  # Print effective config (table in CLI, JSON in MCP)
```

## Extraction Artefact Rules

Files extracted by the Extract layer are stored in `.wkso/` directories:

1. **Symlinks**: Resolve all symlinks before applying other rules
2. **Git repositories**: Place `.wkso/` one level above repository root (parent of `.git/`)
3. **Underscore-prefixed paths**: When any component starts with `_`, place `.wkso/` as sibling to topmost `_`-prefixed component
   - Example: `/Users/ww5/Documents/projects/_old/_previous/file.txt` ‚Üí `/Users/ww5/Documents/projects/.wkso/`
4. **Default**: Place `.wkso/` as sibling to source file

When file checksum changes or file moves, old extraction files are removed.

## Priority Scoring Details

### Managed Directory Mapping

| Directory | Priority | Purpose |
|-----------|----------|---------|
| `~/Desktop` | 150 | Current week's work (symlinks) |
| `~/deadlines` | 120 | Time-sensitive deliverables |
| `~` | 100 | Active year-scoped projects |
| `~/Documents` | 100 | Finalized materials and archives |
| `~/Pictures` | 80 | Visual assets (memes, figures) |
| `~/Downloads` | 50 | Temporary/unorganized staging |

### Calculation Examples

**Example 1**: `/Users/ww5/Documents/my/full/_path/__file.txt`
- Matches: `~/Documents` (base = 100)
- `my`: 100 √ó 0.9 = 90
- `full`: 90 √ó 0.9 = 81
- `_path`: 81 √ó 0.9 √ó 0.5 = 36.45 (one underscore)
- `__file`: 36.45 √ó 0.9 √ó 0.25 = 8.20 (two underscores)
- Extension `.txt`: 8.20 √ó 1.0 = 8.20
- **Priority: 8**

**Example 2**: `/Users/ww5/deadlines/2025_12_15-Proposal/draft.pdf`
- Matches: `~/deadlines` (base = 120)
- `2025_12_15-Proposal`: 120 √ó 0.9 = 108
- Extension `.pdf`: 108 √ó 1.1 = 118.8
- **Priority: 119**

**Example 3**: `/Users/ww5/Downloads/_archive/old.txt`
- Matches: `~/Downloads` (base = 50)
- `_archive`: 50 √ó 0.9 √ó 0.5 = 22.5
- Extension `.txt`: 22.5 √ó 1.0 = 22.5
- **Priority: 23**

### Auto-Indexing Rules

- Files with priority < `auto_index_min` (default 2) are NOT auto-indexed
- Files inside `_/` subdirectories typically have priority 1
- Manual indexing via `wks0 index <path>` always succeeds regardless of priority

## MCP Integration

WKS exposes semantic engines and monitor operations as MCP tools. Following SPEC principles: zero code duplication with business logic in controllers, view-agnostic structured data, and both CLI and MCP using the same controller methods.

### Monitor Tools

Complete parity between CLI and MCP for filesystem monitoring:

**Status and Validation**:
- `wks_monitor_status` ‚Äî Get monitoring status and configuration (`wks0 monitor status`)
- `wks_monitor_validate` ‚Äî Validate configuration for conflicts (`wks0 monitor validate`)
- `wks_monitor_check` ‚Äî Check if path would be monitored (`wks0 monitor check <path>`)

**List Management**:
- `wks_monitor_list` ‚Äî Get contents of configuration list (`wks0 monitor <list_name>`)
  - Parameters: `list_name` (include_paths, exclude_paths, ignore_dirnames, ignore_globs)
- `wks_monitor_add` ‚Äî Add value to list (`wks0 monitor <list_name> add <value>`)
  - Parameters: `list_name`, `value`
- `wks_monitor_remove` ‚Äî Remove value from list (`wks0 monitor <list_name> remove <value>`)
  - Parameters: `list_name`, `value`

**Managed Directories**:
- `wks_monitor_managed_list` ‚Äî List managed directories with priorities (`wks0 monitor managed`)
- `wks_monitor_managed_add` ‚Äî Add managed directory (`wks0 monitor managed add <path> --priority <N>`)
  - Parameters: `path`, `priority`
- `wks_monitor_managed_remove` ‚Äî Remove managed directory (`wks0 monitor managed remove <path>`)
  - Parameters: `path`
- `wks_monitor_managed_set_priority` ‚Äî Update directory priority (`wks0 monitor managed set-priority <path> <N>`)
  - Parameters: `path`, `priority`

All write operations save to config file and notify to restart service.

### Semantic Engine Tools

**Extract Tools**:
- `wks_extract` ‚Äî extract document to plain text
- Parameters: `path`, `engine` (optional)

**Diff Tools**:
- `wks_diff` ‚Äî compare two files
- Parameters: `path1`, `path2`, `engine` (optional)

**Related Tools**:
- `wks_related` ‚Äî find similar documents
- Parameters: `path`, `limit`, `min_similarity`, `engine` (optional)

**Search Tools**:
- `wks_search` ‚Äî semantic search across indices
- Parameters: `query`, `index` (optional), `limit`

### Architecture

**MonitorController Methods** (in `monitor_controller.py`):
- Read-only: `get_status()`, `get_list()`, `get_managed_directories()`, `validate_config()`, `check_path()`
- Write operations: `add_to_list()`, `remove_from_list()`, `add_managed_directory()`, `remove_managed_directory()`, `set_managed_priority()`

**MCP Server Responsibilities**:
1. JSON-RPC protocol (stdio transport)
2. Loading/saving configuration file
3. Routing tool calls to controllers
4. Formatting responses

**Implementation**: All functionality tested via unit tests (`test_monitor_controller.py`) and integration tests (`test_mcp_tools.py`).

## Patterns (CLAUDE.md)

Organizational patterns are documented separately in `CLAUDE.md`. They describe:
- Where to place files physically (which managed directories)
- How to name files (date formats, conventions)
- When to archive (`_old/YYYY/`)
- How to organize content types (presentations, emails, etc.)

**Patterns do NOT duplicate system mechanics like**:
- Priority calculation (defined in config)
- Indexing rules (defined in config)
- Extraction rules (defined in config)

**Patterns provide organizational guidance**:
- Use `~/deadlines/YYYY_MM_DD-Name/` for time-sensitive work
- Use `~/YYYY-ProjectName/` for active projects
- Use `_old/YYYY/` for hierarchical archiving
- Use `_drafts/` to deprioritize working documents

The system calculates priorities and handles indexing automatically based on where files are placed.
</file>

<file path="wks/obsidian.py">
"""
Obsidian vault integration for WKS.

Simple, single-file logs under a required base_dir (default: "WKS").
"""

import hashlib
from pathlib import Path
from typing import Optional, Dict
from datetime import datetime

from .config import mongo_settings, timestamp_format, load_config, get_config_path, DEFAULT_TIMESTAMP_FORMAT
from .constants import WKS_HOME_EXT, WKS_DOT_DIRS, WKS_HOME_DISPLAY
from .dbmeta import resolve_db_compatibility, ensure_db_compat, IncompatibleDatabase
from .utils import get_package_version


class ObsidianVault:
    """Interface to an Obsidian vault for WKS."""

    def __init__(self, vault_path: Path, *, base_dir: str, log_max_entries: int, active_files_max_rows: int, source_max_chars: int, destination_max_chars: int):
        """
        Initialize vault interface.

        Args:
            vault_path: Path to Obsidian vault (e.g., ~/obsidian)
        """
        self.vault_path = Path(vault_path)
        # Required base directory under the vault for WKS-managed logs and docs
        if not base_dir or not str(base_dir).strip():
            raise ValueError("obsidian.base_dir is required in configuration")
        self.base_dir = str(base_dir).strip("/")
        self._recompute_paths()
        # File log paths (single-file mode)
        self.file_log_path = self._base_path() / "FileOperations.md"
        self.activity_log_path = self._base_path() / "ActiveFiles.md"
        self.log_max_entries = int(log_max_entries)
        self.active_files_max_rows = int(active_files_max_rows)
        # Table width controls for path columns
        self.source_max_chars = int(source_max_chars)
        self.destination_max_chars = int(destination_max_chars)
        # Append-only JSONL ledger for file operations used to rebuild the log
        self.ops_ledger_path = Path.home() / WKS_HOME_EXT / "file_ops.jsonl"
        self.ops_ledger_max_lines = 20000
        self.ops_ledger_keep_lines = 10000
        try:
            cfg = load_config()
            self.timestamp_format = timestamp_format(cfg)
        except Exception:
            self.timestamp_format = DEFAULT_TIMESTAMP_FORMAT
        # Throttle state for markdown rewrites (seconds)
        self._write_throttle_secs = 2.0
        self._last_ops_write_ts = 0.0
        self._last_active_write_ts = 0.0
        # Health page throttle
        self._last_health_write_ts = 0.0
        try:
            self.ops_ledger_path.parent.mkdir(parents=True, exist_ok=True)
            if not self.ops_ledger_path.exists():
                self.ops_ledger_path.write_text("")
        except Exception:
            pass

    def _format_dt(self, dt: datetime) -> str:
        try:
            return dt.strftime(self.timestamp_format)
        except Exception:
            return dt.strftime(DEFAULT_TIMESTAMP_FORMAT)

    def _format_ts_value(self, value) -> str:
        if not value:
            return ""
        try:
            if isinstance(value, str):
                s = value.replace('Z', '+00:00') if value.endswith('Z') else value
                dt = datetime.fromisoformat(s)
            else:
                dt = datetime.fromtimestamp(float(value))
            return self._format_dt(dt)
        except Exception:
            return str(value)

    def _shorten_path(self, p: Path, max_chars: int) -> str:
        """Return a compact string for a path, using ~ for home and mid-ellipsis if too long."""
        try:
            rel = p.relative_to(Path.home())
            s = f"~/{rel}"
        except ValueError:
            s = str(p)
        if len(s) <= max_chars:
            return s
        if max_chars <= 3:
            return s[-max_chars:]
        head = max_chars // 2 - 1
        tail = max_chars - head - 1
        return s[:head] + "‚Ä¶" + s[-tail:]

    def _base_path(self) -> Path:
        return self.vault_path / self.base_dir

    def _recompute_paths(self):
        # Category folders always live at vault root
        # Use internal _links directory for embedded content management
        self.links_dir = self.vault_path / "_links"
        self.projects_dir = self.vault_path / "Projects"
        self.people_dir = self.vault_path / "People"
        self.topics_dir = self.vault_path / "Topics"
        self.ideas_dir = self.vault_path / "Ideas"
        self.orgs_dir = self.vault_path / "Organizations"
        self.records_dir = self.vault_path / "Records"

    def set_base_dir(self, base_dir: str):
        """Set the base subdirectory under the vault for WKS-managed content."""
        self.base_dir = base_dir.strip("/")
        self._recompute_paths()
        self.file_log_path = self._base_path() / "FileOperations.md"
        self.activity_log_path = self._base_path() / "ActiveFiles.md"

    def _get_file_log_path(self) -> Path:
        return self.file_log_path

    def _append_ops_ledger(self, record: dict):
        """Append a JSON line record to the operations ledger."""
        try:
            import json as _json
            with open(self.ops_ledger_path, 'a', encoding='utf-8') as f:
                f.write(_json.dumps(record, ensure_ascii=False) + "\n")
            # Compact/rotate if too large
            try:
                lines = self.ops_ledger_path.read_text(encoding='utf-8', errors='ignore').splitlines()
                if len(lines) > self.ops_ledger_max_lines:
                    keep = lines[-self.ops_ledger_keep_lines:]
                    self.ops_ledger_path.write_text("\n".join(keep) + "\n", encoding='utf-8')
            except Exception:
                pass
        except Exception:
            pass

    def _load_recent_ops(self, limit: int) -> list[dict]:
        """Load last N operation records from the JSONL ledger."""
        try:
            lines = self.ops_ledger_path.read_text(encoding='utf-8', errors='ignore').splitlines()
        except Exception:
            return []
        out = []
        import json as _json
        for line in lines[-max(0, int(limit)):] if lines else []:
            try:
                out.append(_json.loads(line))
            except Exception:
                continue
        # newest first
        out.reverse()
        return out

    def _rebuild_file_operations(self, tracked_files_count: Optional[int] = None):
        """Rewrite FileOperations.md from the JSONL ledger (newest first).

        Format: list entries with file:// links for Path(s), no tables.
        """
        import time as _time
        if _time.time() - self._last_ops_write_ts < self._write_throttle_secs:
            return
        file_path = self._get_file_log_path()
        ops = self._load_recent_ops(self.log_max_entries)
        timestamp = self._format_dt(datetime.now())
        cfg_path = get_config_path()
        cfg_url = f"file://{cfg_path}"
        # Intro + table header
        header = [
            f"Reverse chronological log of last {self.log_max_entries} file operations tracked by WKS. [config]({cfg_url})",
            "",
            "| Action | Time | Checksum | Size | Modified | Œ∏‚ÇÄ | Path |",
            "|:--|:--|--:|--:|:--|--:|:--|",
        ]
        rows = []
        def _icon_for(op:str) -> str:
            m = {
                'CREATED': '‚ûï',
                'MODIFIED': '‚úèÔ∏è',
                'DELETED': 'üóëÔ∏è',
                'MOVED': 'üîÄ',
                'RENAMED': 'üìù',
            }
            return m.get(op.upper(), 'üìÑ')
        home = Path.home().resolve()
        def _disp(p: Path) -> str:
            try:
                rel = p.resolve().relative_to(home)
                return f"~/{rel.as_posix()}"
            except Exception:
                return p.resolve().as_posix()

        home = Path.home().resolve()
        def _is_within(child: Path, base: Path) -> bool:
            try:
                child.resolve().relative_to(base.resolve())
                return True
            except Exception:
                return False

        def _is_temp_or_autosave(p: Path) -> bool:
            try:
                n = p.name
                nl = n.lower()
                if n.endswith('~'):
                    return True
                if (n.startswith('#') and n.endswith('#')) or n.startswith('.#'):
                    return True
                if n.startswith('~$'):
                    return True
                if n.startswith('._'):
                    return True
                if nl in {'.ds_store', 'icon\r'}:
                    return True
                if any(nl.endswith(ext) for ext in ('.swp', '.swo', '.tmp', '.part', '.crdownload')):
                    return True
                if '.tmp' in nl:
                    return True
            except Exception:
                pass
            return False

        def _skip_ops_path(p: Path) -> bool:
            try:
                pics = home/"Pictures"
                photos = home/"Photos"
                if _is_within(p, pics) or _is_within(p, photos):
                    return True
            except Exception:
                pass
            # Hide temp/autosave artifacts
            try:
                if _is_temp_or_autosave(p):
                    return True
            except Exception:
                pass
            return False

        # Helpers for size and modified
        def _hsize(n: int) -> str:
            try:
                units = ['B','KB','MB','GB','TB']
                i = 0; f = float(n)
                while f >= 1024.0 and i < len(units)-1:
                    f /= 1024.0; i += 1
                return f"{f:0.1f} {units[i]}"
            except Exception:
                return "-"

        # Optional similarity for angle-from-empty
        simdb = None
        try:
            from .similarity import build_similarity_from_config as _build_sim
            cfg = load_config()
            simdb, _ = _build_sim(cfg, require_enabled=False, compatibility_tag=resolve_db_compatibility(cfg)[0], product_version=get_package_version())
        except Exception:
            simdb = None

        for rec in ops:
            ts = rec.get('timestamp','')
            op = (str(rec.get('operation','')).upper() or 'OP')
            icon = _icon_for(op)
            src = rec.get('source')
            dst = rec.get('destination')
            checksum = rec.get('checksum') or 'N/A'
            a_path = Path(src) if src else None
            b_path = Path(dst) if dst else None
            a = a_path.resolve().as_uri() if a_path else None
            b = b_path.resolve().as_uri() if b_path else None
            # Skip ignored paths (e.g., Photos/Pictures)
            if a_path and _skip_ops_path(a_path):
                continue
            if b_path and _skip_ops_path(b_path):
                continue
            # Build rows; for MOVED, split into two rows: MOVE_FROM and MOVE_TO
            _fig = "\u2007"
            def make_action_cell(label: str) -> str:
                raw = f"{icon} {label}"
                pad = max(0, 14 - len(raw))
                return f"`{raw}{_fig*pad}`"
            time_cell = f"`{self._format_ts_value(ts)}{_fig*2}`"
            def _cells_for(p: Path) -> tuple[str,str,str]:
                # size, modified, angle0
                s_cell = "-"; m_cell = "-"; a0_cell = "-"
                try:
                    st = p.stat()
                    s_cell = _hsize(getattr(st,'st_size',0))
                    m_cell = self._format_dt(datetime.fromtimestamp(st.st_mtime))
                except Exception:
                    pass
                if simdb is not None:
                    try:
                        emb = simdb.get_file_embedding(p)
                        ang = simdb.angle_from_empty(emb) if emb else None
                        if ang is not None:
                            a0_cell = f"{ang:0.2f}¬∞"
                    except Exception:
                        pass
                return s_cell, m_cell, a0_cell

            if op == 'MOVED' and a_path and b_path:
                if _skip_ops_path(a_path) or _skip_ops_path(b_path):
                    continue
                s_cell, m_cell, a0_cell = _cells_for(a_path)
                from_cell = f"[{_disp(a_path)}]({a})"
                rows.append(f"| {make_action_cell('MOVE_FROM')} | {time_cell} | `{checksum}` | `{s_cell}` | `{m_cell}` | `{a0_cell}` | {from_cell} |")
                s_cell, m_cell, a0_cell = _cells_for(b_path)
                to_cell = f"[{_disp(b_path)}]({b})"
                rows.append(f"| {make_action_cell('MOVE_TO')} | {time_cell} | `{checksum}` | `{s_cell}` | `{m_cell}` | `{a0_cell}` | {to_cell} |")
            else:
                if a_path and b_path:
                    if _skip_ops_path(a_path) and _skip_ops_path(b_path):
                        continue
                    # Prefer destination for stats, but keep arrow for context
                    s_cell, m_cell, a0_cell = _cells_for(b_path)
                    path_cell = f"[{_disp(a_path)}]({a}) ‚Üí [{_disp(b_path)}]({b})"
                elif a_path:
                    if _skip_ops_path(a_path):
                        continue
                    s_cell, m_cell, a0_cell = _cells_for(a_path)
                    path_cell = f"[{_disp(a_path)}]({a})"
                elif b_path:
                    if _skip_ops_path(b_path):
                        continue
                    s_cell, m_cell, a0_cell = _cells_for(b_path)
                    path_cell = f"[{_disp(b_path)}]({b})"
                else:
                    s_cell, m_cell, a0_cell = ("-","-","-")
                    path_cell = ""
                action_cell = make_action_cell(op)
                rows.append(f"| {action_cell} | {time_cell} | `{checksum}` | `{s_cell}` | `{m_cell}` | `{a0_cell}` | {path_cell} |")
        # Do not filter out blank lines; keep a true blank line before the table
        out = "\n".join(header) + ("\n" + "\n".join(rows) + "\n" if rows else "\n")
        self._atomic_write(file_path, out)
        self._last_ops_write_ts = _time.time()

    def ensure_structure(self):
        """Create vault folder structure if it doesn't exist."""
        # Ensure base path (for logs) exists
        self._base_path().mkdir(parents=True, exist_ok=True)
        # Ensure Health landing page exists
        hp = self._base_path() / 'Health.md'
        if not hp.exists():
            try:
                self._atomic_write(hp, "# Health\n\n(Initialized)\n")
            except Exception:
                pass
        # Ensure root-level category folders exist
        for directory in [
            self.links_dir,
            self.projects_dir,
            self.people_dir,
            self.topics_dir,
            self.ideas_dir,
            self.orgs_dir,
            self.records_dir,
        ]:
            if directory is not None:
                directory.mkdir(parents=True, exist_ok=True)


    def create_project_note(
        self,
        project_path: Path,
        status: str = "Active",
        description: Optional[str] = None
    ) -> Path:
        """
        Create or update a project note in Projects/ folder.

        Args:
            project_path: Path to project directory (e.g., ~/2025-WKS)
            status: Project status
            description: Optional project description

        Returns:
            Path to created note
        """
        project_name = project_path.name
        note_path = self.projects_dir / f"{project_name}.md"

        # Extract date and name from project folder
        parts = project_name.split("-", 1)
        year = parts[0] if len(parts) > 0 else ""
        name = parts[1] if len(parts) > 1 else project_name

        content = f"""# {project_name}

**Status:** {status}
**Created:** {datetime.now().strftime('%Y-%m-%d')}
**Location:** `{project_path}`

## Overview

{description or f"Project: {name}"}

## Links

- Project directory: [[_links/{project_name}]]
- Related topics:

## Notes

"""

        note_path.write_text(content)
        return note_path

    def link_file(self, source_file: Path, preserve_structure: bool = True) -> Optional[Path]:
        """
        Create a symlink to a file in the _links/ directory.

        Args:
            source_file: File to link to
            preserve_structure: If True, mirror directory structure from home

        Returns:
            Path to created symlink, or None if failed
        """
        if not source_file.exists():
            return None

        if preserve_structure:
            # Mirror structure from home directory
            home = Path.home()
            try:
                relative = source_file.relative_to(home)
                link_path = self.links_dir / relative
            except ValueError:
                # File not under home, use project-based structure
                link_path = self.links_dir / source_file.name
        else:
            link_path = self.links_dir / source_file.name

        # Create parent directories
        link_path.parent.mkdir(parents=True, exist_ok=True)

        # Create symlink if it doesn't exist
        if not link_path.exists():
            link_path.symlink_to(source_file)

        return link_path

    def _link_rel_for_source(self, source_file: Path, preserve_structure: bool = True) -> str:
        """Compute the vault-internal wiki link target (relative) for a source file.

        Returns a path like '_links/<...>' suitable for use inside [[...]] links.
        """
        if preserve_structure:
            home = Path.home()
            try:
                relative = source_file.resolve().relative_to(home)
                return f"_links/{relative.as_posix()}"
            except Exception:
                return f"_links/{source_file.name}"
        else:
            return f"_links/{source_file.name}"

    def _iter_vault_markdown(self):
        for md in self.vault_path.rglob("*.md"):
            try:
                # Skip our log files
                if md.resolve() == self.file_log_path.resolve() or md.resolve() == self.activity_log_path.resolve():
                    continue
            except Exception:
                pass
            yield md

    def update_vault_links_on_move(self, old_path: Path, new_path: Path):
        """Update wiki links inside the vault when a file moves.

        Rewrites [[_links/<old>]] (and alias forms). Also updates legacy [[links/‚Ä¶]] to the new [[_links/‚Ä¶]] path.
        """
        old_rel = self._link_rel_for_source(old_path)
        new_rel = self._link_rel_for_source(new_path)
        if old_rel == new_rel:
            return
        # Prepare replacements for both _links and legacy links paths
        old_rel_legacy = old_rel.replace("_links/", "links/")
        new_rel_legacy = new_rel.replace("_links/", "links/")
        patterns = [
            (f"[[{old_rel}]]", f"[[{new_rel}]]"),
            (f"[[{old_rel}|", f"[[{new_rel}|"),
            (f"`{old_rel}`", f"`{new_rel}`"),
            (f"[[{old_rel_legacy}]]", f"[[{new_rel}]]"),
            (f"[[{old_rel_legacy}|", f"[[{new_rel}|"),
            (f"`{old_rel_legacy}`", f"`{new_rel}`"),
        ]
        for md in self._iter_vault_markdown():
            try:
                content = md.read_text(encoding="utf-8")
            except Exception:
                continue
            original = content
            for a, b in patterns:
                if a in content:
                    content = content.replace(a, b)
            if content != original:
                try:
                    md.write_text(content, encoding="utf-8")
                except Exception:
                    pass

    def mark_reference_deleted(self, path: Path):
        """Annotate vault notes that reference a deleted file with a callout.

        Adds a line '> üóëÔ∏è deleted: [[_links/...]]' near the top if not already present (supports legacy [[links/...]] references).
        """
        rel = self._link_rel_for_source(path)
        marker = f"üóëÔ∏è deleted: [[{rel}]]"
        legacy_rel = rel.replace("_links/", "links/")
        for md in self._iter_vault_markdown():
            try:
                content = md.read_text(encoding="utf-8")
            except Exception:
                continue
            if (
                f"[[{rel}]]" not in content and f"[[{rel}|" not in content and
                f"[[{legacy_rel}]]" not in content and f"[[{legacy_rel}|" not in content
            ):
                continue
            if marker in content:
                continue
            # Insert marker after first header or at top
            lines = content.splitlines()
            insert_idx = 0
            if lines and lines[0].startswith("# "):
                insert_idx = 1
            lines.insert(insert_idx, f"> {marker}")
            try:
                md.write_text("\n".join(lines) + "\n", encoding="utf-8")
            except Exception:
                pass

    def write_doc_text(self, content_hash: str, source_path: Path, text: str, keep: int = 99):
        """Write raw extracted text to WKS/Docs/<checksum>.md and keep only last N.

        Args:
            content_hash: SHA256 hex of file content
            source_path: Original file path
            text: Extracted raw text
            keep: Max number of docs to keep (delete oldest beyond this)
        """
        docs_dir = self._base_path() / 'Docs'
        docs_dir.mkdir(parents=True, exist_ok=True)
        doc_path = docs_dir / f"{content_hash}.md"
        header = (
            f"# {source_path.name}\n\n"
            f"`{source_path}`\n\n"
            f"*Checksum:* `{content_hash}`  *Updated:* {self._format_dt(datetime.now())}\n\n"
            "---\n\n"
        )
        try:
            doc_path.write_text(header + text, encoding='utf-8')
        except Exception:
            return
        # Rotate: keep only last N by mtime
        try:
            entries = sorted(docs_dir.glob('*.md'), key=lambda p: p.stat().st_mtime, reverse=True)
            for old in entries[keep:]:
                try:
                    old.unlink()
                except Exception:
                    pass
        except Exception:
            pass

    def link_project(self, project_path: Path) -> list[Path]:
        """
        Create symlinks for key files in a project.

        Args:
            project_path: Path to project directory

        Returns:
            List of created symlink paths
        """
        links_created = []

        # Common files to link
        common_files = [
            "README.md",
            "SPEC.md",
            "TODO.md",
            "NOTES.md",
        ]

        for filename in common_files:
            file_path = project_path / filename
            if file_path.exists():
                link = self.link_file(file_path)
                if link:
                    links_created.append(link)

        return links_created

    def update_link_on_move(self, old_path: Path, new_path: Path):
        """
        Update symlink when a file is moved.

        Args:
            old_path: Previous file location
            new_path: New file location
        """
        # Find existing symlink
        home = Path.home()
        try:
            relative_old = old_path.relative_to(home)
            old_link = self.links_dir / relative_old

            if old_link.exists() and old_link.is_symlink():
                # Remove old link
                old_link.unlink()

                # Create new link
                self.link_file(new_path)
        except (ValueError, OSError):
            pass  # File not tracked or operation failed

    def find_broken_links(self) -> list[Path]:
        """
        Find all broken symlinks in the vault.

        Returns:
            List of broken symlink paths
        """
        broken = []

        for link in self.links_dir.rglob("*"):
            if link.is_symlink() and not link.exists():
                broken.append(link)

        return broken

    def cleanup_broken_links(self) -> int:
        """
        Remove all broken symlinks from the vault.

        Returns:
            Number of links removed
        """
        broken = self.find_broken_links()
        for link in broken:
            link.unlink()
        return len(broken)

    def _get_file_checksum(self, path: Path) -> Optional[str]:
        """
        Calculate SHA256 checksum of a file.

        Args:
            path: Path to file

        Returns:
            Hex digest of checksum, or None if file doesn't exist/can't be read
        """
        try:
            if path.exists() and path.is_file():
                sha256 = hashlib.sha256()
                with open(path, 'rb') as f:
                    for block in iter(lambda: f.read(4096), b''):
                        sha256.update(block)
                return sha256.hexdigest()[:12]  # First 12 chars for brevity
        except (OSError, PermissionError):
            pass
        return None

    def log_file_operation(
        self,
        operation: str,
        path: Path,
        destination: Optional[Path] = None,
        details: Optional[str] = None,
        tracked_files_count: Optional[int] = None
    ):
        """
        Record a file operation by appending to a JSONL ledger and rewrite the
        FileOperations.md from that ledger (newest first). This avoids fragile
        in-place table editing.
        """
        # Build record
        ts_raw = self._format_dt(datetime.now())
        rec = {
            "timestamp": ts_raw,
            "operation": operation,
            "source": str(path) if path else None,
            "destination": str(destination) if destination else None,
            "details": details or None,
        }
        # Compute a representative checksum (source or destination)
        try:
            src_cs = self._get_file_checksum(path)
        except Exception:
            src_cs = None
        try:
            dst_cs = self._get_file_checksum(destination) if destination else None
        except Exception:
            dst_cs = None
        rec["checksum"] = dst_cs or src_cs or None
        # Append to ledger and rebuild file
        self._append_ops_ledger(rec)
        self._rebuild_file_operations(tracked_files_count=tracked_files_count)

    def get_recent_operations(self, limit: int = 50) -> str:
        """
        Get the most recent file operations.

        Args:
            limit: Maximum number of entries to return

        Returns:
            String containing recent operations
        """
        if not self.file_log_path.exists():
            return "No operations logged yet."

        content = self.file_log_path.read_text()
        lines = content.split('\n')

        # Find entries (lines starting with '- **')
        entries = [line for line in lines if line.strip().startswith('- **')]

        return '\n'.join(entries[:limit])

    def update_active_files(self, active_files: list[tuple[str, float, float]], tracker=None):
        """Single-line ActiveFiles entries with fixed-width header and path last.

        Format: `ANGLE¬∞ ¬∑ DELTA¬∞/min ¬∑ YYYY-MM-DD HH:MM:SS ¬∑` [~/path](file:///...)
        """
        # Compact table header with rate scales; numeric columns right-aligned
        lines = [
            "| ¬∞ | ¬∞/hr | ¬∞/day | ¬∞/wk | Modified | Name |",
            "|--:|--:|--:|--:|:--|:--|",
        ]
        from datetime import datetime
        home = Path.home().resolve()

        # Load ignore rules to filter out excluded/ignored paths
        try:
            from .cli import load_config as _load
            cfg = _load(); mon = cfg.get('monitor') or {}
            exclude_paths = [Path(p).expanduser().resolve() for p in (mon.get('exclude_paths') or [])]
            ignore_dirnames = set(mon.get('ignore_dirnames') or [])
            ignore_globs = list(mon.get('ignore_globs') or [])
        except Exception:
            exclude_paths = []
            ignore_dirnames = set()
            ignore_globs = []

        def _is_within(child: Path, base: Path) -> bool:
            try:
                child.resolve().relative_to(base.resolve())
                return True
            except Exception:
                return False

        def _skip_path(p: Path) -> bool:
            # Explicit skip for ~/Pictures hierarchy
            try:
                if p.resolve().is_relative_to(Path.home().resolve()/"Pictures"):
                    return True
            except Exception:
                try:
                    (p.resolve()).relative_to(Path.home().resolve()/"Pictures")
                    return True
                except Exception:
                    pass
            if any(_is_within(p, ex) for ex in exclude_paths):
                return True
            for part in p.parts:
                if part in WKS_DOT_DIRS:
                    return True
                if part.startswith('.'):
                    return True
                if part in ignore_dirnames:
                    return True
            try:
                import fnmatch as _fn
                pstr = p.as_posix()
                for g in ignore_globs:
                    if _fn.fnmatchcase(pstr, g) or _fn.fnmatchcase(p.name, g):
                        return True
            except Exception:
                pass
            return False

        def _disp(p: Path) -> str:
            try:
                rel = p.resolve().relative_to(home)
                return f"~/{rel.as_posix()}"
            except Exception:
                return p.resolve().as_posix()

        # Prepare Mongo changes querying for windowed averages
        use_changes = True
        changes_coll = None
        try:
            from .cli import load_config as _load
            cfg2 = _load()
            sim2 = cfg2.get('similarity') or {}
            if sim2.get('enabled'):
                from pymongo import MongoClient as _MC
                mongo_cfg = mongo_settings(cfg2)
                space_tag, _ = resolve_db_compatibility(cfg2)
                client = _MC(mongo_cfg['uri'])
                ensure_db_compat(client, mongo_cfg['space_database'], "space", space_tag)
                db = client[mongo_cfg['space_database']]
                changes_coll = db['embedding_changes']
            else:
                use_changes = False
        except IncompatibleDatabase:
            use_changes = False
        except Exception:
            use_changes = False

        import time as _time
        now_epoch = int(_time.time())
        def window_deg_per_sec(fp: str, seconds_window: int) -> float:
            if not changes_coll:
                return 0.0
            try:
                start = now_epoch - seconds_window
                total_deg = 0.0
                total_dt = 0.0
                for ev in changes_coll.find({"file_path": fp, "t_new_epoch": {"$gte": start}}):
                    d = float(ev.get('degrees') or 0.0)
                    dt = float(ev.get('seconds') or 0.0)
                    if dt > 0:
                        total_deg += d
                        total_dt += dt
                return (total_deg / total_dt) if total_dt > 0 else 0.0
            except Exception:
                return 0.0

        # Collect rows so we can sort by absolute hourly angle when embedding changes available
        rows_sorted: list[tuple[float, str]] = []

        for path_str, angle, _delta in (active_files[: self.active_files_max_rows] if isinstance(active_files, list) else active_files):
            p = Path(path_str)
            if _skip_path(p):
                continue
            # Last modified
            if tracker is not None:
                try:
                    last_mod_raw = tracker.get_last_modified(p)
                except Exception:
                    last_mod_raw = None
            else:
                last_mod_raw = None
            if not last_mod_raw:
                try:
                    last_mod = self._format_dt(datetime.fromtimestamp(p.stat().st_mtime))
                except Exception:
                    last_mod = "Unknown"
            else:
                last_mod = self._format_ts_value(last_mod_raw)
            # Windowed weighted averages from embedding_changes
            if use_changes and changes_coll is not None:
                # Strict windowed averages per your definition
                dps_1h = window_deg_per_sec(path_str, 3600)
                dps_1d = window_deg_per_sec(path_str, 86400)
                dps_1w = window_deg_per_sec(path_str, 604800)
                dph = dps_1h * 3600.0
                dpd = dps_1d * 86400.0
                dpw = dps_1w * 604800.0
                # recent angle from last event
                try:
                    ev = changes_coll.find({"file_path": path_str}).sort("t_new_epoch", -1).limit(1)
                    last_deg = None
                    for e in ev:
                        last_deg = float(e.get('degrees') or 0.0)
                    if last_deg is not None:
                        angle = last_deg
                except Exception:
                    pass
            else:
                # fallback: no changes available
                dph = 0.0; dpd = 0.0; dpw = 0.0
            angle_str = f"{angle:0.2f}"
            dph_str = f"{dph:+0.2f}"
            dpd_str = f"{dpd:+0.2f}"
            dpw_str = f"{dpw:+0.2f}"
            name_cell = f"üìÑ [{_disp(p)}]({p.resolve().as_uri()})"
            line = f"| `{angle_str}` | `{dph_str}` | `{dpd_str}` | `{dpw_str}` | `{last_mod}` | {name_cell} |"
            # Sort by absolute hourly angle when similarity changes are available; else keep order
            key = abs(float(dph)) if (use_changes and changes_coll is not None) else 0.0
            rows_sorted.append((key, line))
        # If we collected sortable rows, sort descending by absolute hourly angle
        if rows_sorted:
            rows_sorted.sort(key=lambda t: t[0], reverse=True)
            for _, ln in rows_sorted:
                lines.append(ln)
        lines.append("")
        lines.append(f"Updated: {self._format_dt(datetime.now())}")
        import time as _time
        if _time.time() - self._last_active_write_ts < self._write_throttle_secs:
            return
        self._atomic_write(self.activity_log_path, "\n".join(lines) + "\n")
        self._last_active_write_ts = _time.time()

    def _atomic_write(self, path: Path, content: str):
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            tmp = path.with_suffix(path.suffix + ".tmp")
            with open(tmp, 'w', encoding='utf-8') as f:
                f.write(content)
            tmp.replace(path)
        except Exception:
            try:
                path.write_text(content, encoding='utf-8')
            except Exception:
                pass

    def write_health_page(self):
        """Write Health.md landing page with key metrics and an embedded Spec.

        - Uses ~/.wks/health.json if present
        - Counts tracked files from monitor_state.json if present
        - Shows ledger size and recent ops count
        - Tries to include similarity stats if available
        """
        import time as _time
        if _time.time() - self._last_health_write_ts < self._write_throttle_secs:
            return
        base = self._base_path()
        hp = base / 'Health.md'
        # Metrics
        from datetime import datetime as _dt
        import json as _json
        health_path = Path.home()/WKS_HOME_EXT/'health.json'
        health = {}
        try:
            if health_path.exists():
                health = _json.load(open(health_path, 'r'))
        except Exception:
            health = {}
        # Tracked files (from monitor DB - not available here, will be None)
        tracked = None
        # Ledger lines
        ledger_lines = 0
        try:
            ledger_lines = len(self.ops_ledger_path.read_text(encoding='utf-8', errors='ignore').splitlines())
        except Exception:
            pass
        # Similarity stats
        sim_stats = None
        try:
            from .similarity import build_similarity_from_config as _build_sim
            from .cli import load_config as _load
            cfg = _load()
            db, _ = _build_sim(cfg, require_enabled=False, compatibility_tag=resolve_db_compatibility(cfg)[0], product_version=get_package_version())
            if db:
                sim_stats = db.get_stats()
        except Exception:
            sim_stats = None
        # Build compact metrics-only table
        def _age_str(secs):
            try:
                secs = int(secs)
                if secs < 60:
                    return f"{secs}s"
                mins = secs // 60
                if mins < 60:
                    return f"{mins}m"
                hrs = mins // 60
                return f"{hrs}h"
            except Exception:
                return "‚Äî"

        uptime_hms = health.get('uptime_hms', '‚Äî')
        bpm = health.get('avg_beats_per_min', '‚Äî')
        try:
            bpm_str = f"{float(bpm):.1f}"
        except Exception:
            bpm_str = str(bpm)
        pdel = health.get('pending_deletes', 0)
        pmod = health.get('pending_mods', 0)
        lerr = health.get('last_error')
        lerr_age = _age_str(health.get('last_error_age_secs') or 0)

        ops_path = (base/'FileOperations.md').as_posix()
        act_path = (base/'ActiveFiles.md').as_posix()

        pid = health.get('pid', '‚Äî')
        ok_flag = 'true' if not lerr else 'false'
        last_err_age = health.get('last_error_age_secs')
        lock_present = health.get('lock_present')
        lock_pid = health.get('lock_pid')
        lines = [
            '| Metric | Value | Info |',
            '|:--|:--|:--|',
            f"| üïí Uptime | `{uptime_hms}` | Since last restart |",
            f"| ü´Ä BPM | `{bpm_str}` | Average beats/min (ticks + ops) |",
            f"| üß© PID | `{pid}` | Daemon process ID |",
            f"| üß∞ Pending deletes | `{pdel}` | Pending coalesced ops |",
            f"| üß∞ Pending mods | `{pmod}` | Pending coalesced ops |",
            f"| ‚úÖ OK | `{ok_flag}` | Most recent error or OK |",
            f"| üìÅ Tracked files | `{tracked if tracked is not None else '‚Äî'}` | [Active](WKS/ActiveFiles) |",
            f"| üßæ Ledger entries | `{ledger_lines}` | [Ops](WKS/FileOperations) |",
            f"| üîí Lock present | `{str(bool(lock_present)).lower()}` | Lock file currently exists |",
            f"| üß∑ Lock pid | `{lock_pid if lock_pid is not None else '‚Äî'}` | PID recorded in lock file |",
        ]
        if sim_stats:
            total = sim_stats.get('total_files', 0)
            lines += [f"| üóÉÔ∏è Similarity files | `{total}` | Indexed files |"]
        # Divider and spec embed below, outside metrics
        lines += ['','---','![[SPEC]]']
        self._atomic_write(hp, "\n".join(lines) + "\n")
        self._last_health_write_ts = _time.time()


if __name__ == "__main__":
    # Example usage
    from rich.console import Console

    console = Console()

    vault = ObsidianVault(Path.home() / "obsidian")
    vault.ensure_structure()

    console.print("[green]Vault structure created![/green]")

    # Create a project note
    wks_project = Path.home() / "2025-WKS"
    if wks_project.exists():
        note = vault.create_project_note(
            wks_project,
            description="Wieselquist Knowledge System - AI-assisted file organization"
        )
        console.print(f"[blue]Created project note:[/blue] {note}")

        # Link project files
        links = vault.link_project(wks_project)
        console.print(f"[yellow]Created {len(links)} symlinks[/yellow]")
</file>

<file path="wks/daemon.py">
"""
WKS daemon for monitoring file system and updating Obsidian.

Adds support for ~/.wks/config.json with include/exclude path control.
"""

from .uri_utils import uri_to_path
from .priority import calculate_priority
from .utils import get_package_version, expand_path, file_checksum
from .config import load_config
from .config_validator import validate_and_raise, ConfigValidationError
from .mongoctl import MongoGuard, ensure_mongo_running
from .obsidian import ObsidianVault
from .monitor import start_monitoring
from .constants import WKS_HOME_EXT, WKS_DOT_DIRS, WKS_HOME_DISPLAY
from pymongo.collection import Collection
from typing import Optional, Set, List, Dict, Any
from pathlib import Path
from dataclasses import dataclass, field, asdict
import logging
import time
import json
import os
import threading
from collections import deque
from datetime import datetime

logger = logging.getLogger(__name__)
try:
    import fcntl  # POSIX file locking
except Exception:  # pragma: no cover
    fcntl = None


@dataclass
class HealthData:
    """Health data structure for daemon health.json file."""
    pending_deletes: int
    pending_mods: int
    last_error: Optional[str]
    pid: int
    last_error_at: Optional[int]
    last_error_at_iso: Optional[str]
    last_error_age_secs: Optional[int]
    started_at: int
    started_at_iso: str
    uptime_secs: int
    uptime_hms: str
    beats: int
    avg_beats_per_min: float
    lock_present: bool
    lock_pid: Optional[int]
    lock_path: str
    db_last_operation: Optional[str]
    db_last_operation_detail: Optional[str]
    db_last_operation_iso: Optional[str]
    db_ops_last_minute: int
    fs_rate_short: float
    fs_rate_long: float
    fs_rate_weighted: float
    fs_rate_short_window_secs: float
    fs_rate_long_window_secs: float
    fs_rate_short_weight: float
    fs_rate_long_weight: float

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dict for JSON serialization."""
        return asdict(self)


try:
    from .config import mongo_settings
    from .status import load_db_activity_summary, load_db_activity_history
except Exception:
    def load_db_activity_summary():  # type: ignore
        return {}

    def load_db_activity_history(max_age_secs: Optional[int] = None):  # type: ignore
        return []


class WKSDaemon:
    """Daemon that monitors filesystem and updates Obsidian vault."""

    def __init__(
        self,
        config: Dict[str, Any],
        vault_path: Path,
        base_dir: str,
        obsidian_log_max_entries: int,
        obsidian_active_files_max_rows: int,
        obsidian_source_max_chars: int,
        obsidian_destination_max_chars: int,
        obsidian_docs_keep: int,
        monitor_paths: list[Path],
        auto_project_notes: bool = False,
        ignore_dirnames: Optional[Set[str]] = None,
        exclude_paths: Optional[List[Path]] = None,
        ignore_patterns: Optional[Set[str]] = None,
        ignore_globs: Optional[List[str]] = None,
        fs_rate_short_window_secs: float = 10.0,
        fs_rate_long_window_secs: float = 600.0,
        fs_rate_short_weight: float = 0.8,
        fs_rate_long_weight: float = 0.2,
        mongo_uri: Optional[str] = None,
        monitor_collection: Optional[Collection] = None,
    ):
        """
        Initialize WKS daemon.

        Args:
            vault_path: Path to Obsidian vault
            monitor_paths: List of paths to monitor
        """
        self.config = config
        self.vault = ObsidianVault(
            vault_path,
            base_dir=base_dir,
            log_max_entries=obsidian_log_max_entries,
            active_files_max_rows=obsidian_active_files_max_rows,
            source_max_chars=obsidian_source_max_chars,
            destination_max_chars=obsidian_destination_max_chars,
        )
        self.docs_keep = int(obsidian_docs_keep)
        self.monitor_paths = monitor_paths
        # state_file removed - not needed
        self.ignore_dirnames = ignore_dirnames if ignore_dirnames is not None else set()
        self.exclude_paths = [Path(p).expanduser() for p in exclude_paths] if exclude_paths else []
        self.ignore_patterns = ignore_patterns if ignore_patterns is not None else set()
        self.observer = None
        self.auto_project_notes = auto_project_notes
        self.ignore_globs = ignore_globs if ignore_globs is not None else []
        # Single-instance lock
        self.lock_file = Path.home() / WKS_HOME_EXT / "daemon.lock"
        self._lock_fh = None
        # Maintenance (periodic tasks)
        self._last_prune_check = 0.0
        # Read prune interval from config, default to 5 minutes (300 seconds)
        monitor_cfg = config.get("monitor", {})
        self._prune_interval_secs = float(monitor_cfg.get("prune_interval_secs", 300.0))
        # Coalesce delete events to avoid temp-file save false positives
        self._pending_deletes: Dict[str, float] = {}
        self._delete_grace_secs = 2.0
        # Coalesce modify/create bursts
        self._pending_mods: Dict[str, Dict[str, Any]] = {}
        self._mod_coalesce_secs = 0.6
        # Health
        self.health_file = Path.home() / WKS_HOME_EXT / "health.json"
        self._last_error = None
        self._last_error_at = None
        self._health_started_at = time.time()
        self._beat_count = 0
        # FS operation rate tracking
        self.fs_rate_short_window = max(float(fs_rate_short_window_secs), 1.0)
        self.fs_rate_long_window = max(float(fs_rate_long_window_secs), self.fs_rate_short_window)
        self.fs_rate_short_weight = float(fs_rate_short_weight)
        self.fs_rate_long_weight = float(fs_rate_long_weight)
        self._fs_events_short: deque[float] = deque()
        self._fs_events_long: deque[float] = deque()
        self.mongo_uri = str(mongo_uri or "")
        self._mongo_guard: Optional[MongoGuard] = None
        self.monitor_collection = monitor_collection

    @staticmethod
    def _get_touch_weight(raw_weight: Any) -> float:
        min_weight = 0.001
        max_weight = 1.0

        if raw_weight is None:
            raise ValueError("monitor.touch_weight is required in config (found: missing, expected: float between 0.001 and 1.0)")

        try:
            weight = float(raw_weight)
        except (TypeError, ValueError):
            raise ValueError(f"monitor.touch_weight must be a number between 0.001 and 1 (found: {type(raw_weight).__name__} = {raw_weight!r}, expected: float between 0.001 and 1.0)")

        if weight < min_weight:
            logger.warning("monitor.touch_weight %.6f below %.3f; using %.3f", weight, min_weight, min_weight)
            return min_weight

        if weight > max_weight:
            logger.warning("monitor.touch_weight %.6f above %.3f; using %.3f", weight, max_weight, max_weight)
            return max_weight

        return weight

    def _compute_touches_per_day(self, doc: Optional[Dict[str, Any]], now: datetime, weight: float) -> float:
        if not doc:
            return 0.0

        timestamp = doc.get("timestamp")
        if not isinstance(timestamp, str):
            return 0.0

        try:
            last_modified_time = datetime.fromisoformat(timestamp)
        except Exception:
            return 0.0

        dt = max((now - last_modified_time).total_seconds(), 0.0)
        prev_rate = doc.get("touches_per_day")

        if not isinstance(prev_rate, (int, float)) or prev_rate <= 0.0:
            interval = dt
        else:
            prev_interval = 1.0 / (prev_rate / 86400.0)
            interval = weight * dt + (1.0 - weight) * prev_interval

        if interval <= 0.0:
            return 0.0

        return 86400.0 / interval

    def _update_monitor_db(self, path: Path):
        if self.monitor_collection is None:
            return

        # Skip if file should be ignored
        if self._should_ignore_by_rules(path):
            return

        try:
            stat = path.stat()
            checksum = file_checksum(path)
            now = datetime.now()
            monitor_config = self.config.get("monitor", {})
            managed_dirs = monitor_config.get("managed_directories", {})
            priority_config = monitor_config.get("priority", {})

            try:
                touch_weight = self._get_touch_weight(monitor_config.get("touch_weight"))
            except ValueError as exc:
                self._set_error(f"monitor_config_error: {exc}")
                return
            priority = calculate_priority(path, managed_dirs, priority_config)

            path_uri = path.as_uri()
            existing_doc = self.monitor_collection.find_one(
                {"path": path_uri}, {"timestamp": 1, "touches_per_day": 1}
            )
            touches_per_day = self._compute_touches_per_day(existing_doc, now, touch_weight)

            doc = {
                "path": path_uri,
                "checksum": checksum,
                "bytes": stat.st_size,
                "priority": priority,
                "timestamp": now.isoformat(),
                "touches_per_day": touches_per_day,
            }

            self.monitor_collection.update_one({"path": doc["path"]}, {"$set": doc}, upsert=True)
            self._enforce_monitor_db_limit()
        except Exception as e:
            self._set_error(f"monitor_db_update_error: {e}")

    def _remove_from_monitor_db(self, path: Path):
        if self.monitor_collection is None:
            return
        try:
            self.monitor_collection.delete_one({"path": path.as_uri()})
        except Exception as e:
            self._set_error(f"monitor_db_delete_error: {e}")

    def _enforce_monitor_db_limit(self):
        if self.monitor_collection is None:
            return

        try:
            monitor_config = self.config.get("monitor", {})
            max_docs = monitor_config.get("max_documents", 1000000)

            count = self.monitor_collection.count_documents({})
            if count <= max_docs:
                return

            extras = count - max_docs
            if extras > 0:
                # Find and remove the lowest priority documents
                lowest_priority_docs = self.monitor_collection.find(
                    {}, {"_id": 1, "priority": 1}
                ).sort("priority", 1).limit(extras)

                ids_to_delete = [doc["_id"] for doc in lowest_priority_docs]
                if ids_to_delete:
                    self.monitor_collection.delete_many({"_id": {"$in": ids_to_delete}})
        except Exception as e:
            self._set_error(f"monitor_db_limit_error: {e}")

    def _record_fs_event(self, timestamp: Optional[float] = None) -> None:
        """Track raw file-system event timing for rate calculations."""
        t = timestamp or time.time()
        self._fs_events_short.append(t)
        self._fs_events_long.append(t)
        cutoff_short = t - self.fs_rate_short_window
        cutoff_long = t - self.fs_rate_long_window
        while self._fs_events_short and self._fs_events_short[0] < cutoff_short:
            self._fs_events_short.popleft()
        while self._fs_events_long and self._fs_events_long[0] < cutoff_long:
            self._fs_events_long.popleft()

    def _is_probably_file(self, p: Path) -> bool:
        """Heuristic: log only file events; for deleted (nonexistent), use suffix-based guess."""
        try:
            if p.exists():
                return p.is_file()
        except Exception:
            pass
        name = p.name
        # Treat names with an extension as files (e.g., README.md); skip obvious directories
        return ('.' in name and not name.endswith('.') and name not in {'', '.', '..'})

    def _handle_move_event(self, src_path: str, dest_path: str):
        """Handle file move event."""
        src = Path(src_path)
        dest = Path(dest_path)

        # Cancel any pending delete for destination (temp-file replace pattern)
        try:
            self._pending_deletes.pop(dest.resolve().as_posix(), None)
        except Exception:
            pass

        # Only log file moves; skip pure directory moves to reduce noise
        try:
            if self._is_probably_file(src) or self._is_probably_file(dest):
                self.vault.log_file_operation("moved", src, dest, tracked_files_count=self._get_tracked_files_count())
                self._bump_beat()
        except Exception:
            pass

        # Update symlink target if tracked
        try:
            self.vault.update_link_on_move(src, dest)
        except Exception:
            pass

        # Update wiki links inside vault
        try:
            self.vault.update_vault_links_on_move(src, dest)
        except Exception:
            pass

        # Update monitor DB (only if not ignored)
        if not self._should_ignore_by_rules(src):
            self._remove_from_monitor_db(src)
        if not self._should_ignore_by_rules(dest):
            self._update_monitor_db(dest)

    def _handle_delete_event(self, path: Path):
        """Handle file delete event."""
        if self._is_probably_file(path):
            try:
                self._pending_deletes[path.resolve().as_posix()] = time.time()
            except Exception:
                pass

    def _handle_create_modify_event(self, path: Path, event_type: str):
        """Handle file create or modify event."""
        # Cancel any pending delete for same path
        try:
            self._pending_deletes.pop(path.resolve().as_posix(), None)
        except Exception:
            pass

        # Queue pending mod/create
        try:
            key = path.resolve().as_posix()
            rec = self._pending_mods.get(key) or {}
            rec["event_type"] = event_type
            rec["when"] = time.time()
            self._pending_mods[key] = rec
        except Exception:
            pass

    def _handle_new_directory(self, path: Path):
        """Handle new directory creation - check if it's a project."""
        if not self.auto_project_notes:
            return

        # Check if it looks like a project folder (YYYY-Name pattern)
        if path.parent == Path.home() and path.name.startswith("20"):
            try:
                self.vault.create_project_note(path, status="New")
                self.vault.log_file_operation(
                    "created",
                    path,
                    details="Auto-created project note in Obsidian",
                    tracked_files_count=self._get_tracked_files_count(),
                )
            except Exception as e:
                print(f"Error creating project note: {e}")

    def on_file_change(self, event_type: str, path_info):
        """
        Callback when a file changes.

        Args:
            event_type: Type of event (created, modified, moved, deleted)
            path_info: Path string for most events, or (src, dest) tuple for moves
        """
        self._record_fs_event()

        # Handle move events specially
        if event_type == "moved":
            src_path, dest_path = path_info
            self._handle_move_event(src_path, dest_path)
            return

        # Regular events
        path = Path(path_info)

        # Skip if file should be ignored
        if self._should_ignore_by_rules(path):
            return

        # Coalesce deletes: set pending and return; flush later
        if event_type == "deleted":
            self._handle_delete_event(path)
            return

        # Created/modified: cancel any pending delete for same path and coalesce modify
        if event_type in ["created", "modified"]:
            self._handle_create_modify_event(path, event_type)

            # Handle specific cases for new directories
            if event_type == "created" and path.is_dir():
                self._handle_new_directory(path)
            return

    def start(self):
        """Start monitoring."""
        self._start_mongo_guard()
        # Acquire single-instance lock
        self._acquire_lock()
        self.vault.ensure_structure()

        self.observer = start_monitoring(
            directories=self.monitor_paths,
            state_file=Path.home() / WKS_HOME_EXT / "monitor_state.json",  # Temporary default
            on_change=self.on_file_change,
            ignore_dirs=self.ignore_dirnames,
            ignore_patterns=self.ignore_patterns,
            include_paths=self.monitor_paths,
            exclude_paths=self.exclude_paths,
            ignore_globs=self.ignore_globs,
        )

        print(f"WKS daemon started, monitoring: {[str(p) for p in self.monitor_paths]}")

    def stop(self):
        """Stop monitoring."""
        if self.observer:
            self.observer.stop()
            self.observer.join()
            print("WKS daemon stopped")
        self._release_lock()
        self._stop_mongo_guard()

    def _start_mongo_guard(self):
        if not self.mongo_uri:
            return
        guard = self._mongo_guard
        if guard is None:
            guard = MongoGuard(self.mongo_uri, ping_interval=10.0)
            self._mongo_guard = guard
        guard.start(record_start=True)

    def _stop_mongo_guard(self):
        guard = self._mongo_guard
        if not guard:
            return
        try:
            guard.stop()
        except Exception:
            pass

    def run(self):
        """Run the daemon (blocking)."""
        try:
            self.start()
        except RuntimeError as e:
            print(str(e))
            return
        try:
            while True:
                # Flush pending deletes and periodic maintenance
                self._maybe_flush_pending_deletes()
                self._maybe_flush_pending_mods()
                self._write_health()
                time.sleep(1)
        except KeyboardInterrupt:
            self.stop()

    def _within_any(self, path: Path, bases: list[Path]) -> bool:
        for base in bases or []:
            try:
                path.resolve().relative_to(base.resolve())
                return True
            except Exception:
                continue
        return False

    def _should_ignore_by_rules(self, path: Path) -> bool:
        # Outside include paths
        if self.monitor_paths and not self._within_any(path, self.monitor_paths):
            return True
        # Inside exclude roots
        if self.exclude_paths and any(self._within_any(path, [ex]) for ex in self.exclude_paths):
            return True
        # Dotfile segments (including .wks/.wkso artefacts)
        for part in path.parts:
            if part in WKS_DOT_DIRS:
                return True
            if part.startswith('.'):
                return True
        # Named ignored directories
        if self.ignore_dirnames:
            for part in path.parts:
                if part in self.ignore_dirnames:
                    return True
        # Glob-based ignores (full path and basename)
        if self.ignore_globs:
            import fnmatch as _fn
            pstr = path.as_posix()
            for g in self.ignore_globs:
                try:
                    if _fn.fnmatchcase(pstr, g) or _fn.fnmatchcase(path.name, g):
                        return True
                except Exception:
                    continue
        return False

    def _maybe_prune_monitor_db(self):
        """Prune monitor database entries that are missing or match exclude rules."""
        if self.monitor_collection is None:
            return
        now = time.time()
        if now - self._last_prune_check < self._prune_interval_secs:
            return
        self._last_prune_check = now

        try:
            removed = 0
            # Iterate over all monitor entries
            cursor = self.monitor_collection.find({}, {"path": 1})
            for doc in cursor:
                uri = doc.get('path')
                if not uri:
                    continue

                # Convert URI to path for checking
                try:
                    p = uri_to_path(uri)
                except Exception:
                    continue

                # Check if file is missing
                try:
                    missing = not p.exists()
                except Exception:
                    missing = True

                # Check if file should be ignored by current rules
                ignored = False
                if not missing:
                    try:
                        ignored = self._should_ignore_by_rules(p)
                    except Exception:
                        ignored = False

                # Remove if missing or ignored
                if missing or ignored:
                    try:
                        self.monitor_collection.delete_one({"path": uri})
                        removed += 1
                    except Exception:
                        continue

            if removed:
                print(f"Monitor maintenance: pruned {removed} stale/excluded entries")
        except Exception as e:
            self._set_error(f"monitor_prune_error: {e}")

    def _maybe_flush_pending_deletes(self):
        """Log deletes after a short grace period to avoid temp-file saves showing as delete+recreate."""
        if not self._pending_deletes:
            return
        now = time.time()
        expired = []
        for pstr, ts in list(self._pending_deletes.items()):
            if now - ts >= self._delete_grace_secs:
                expired.append(pstr)
        for pstr in expired:
            try:
                p = Path(pstr)
                # If the path exists again, skip logging delete
                if p.exists():
                    self._pending_deletes.pop(pstr, None)
                    continue
                # Log deletion now
                try:
                    self.vault.log_file_operation("deleted", p, tracked_files_count=self._get_tracked_files_count())
                except Exception as e:
                    self._set_error(f"delete_log_error: {e}")
                    pass

                self._remove_from_monitor_db(p)

                try:
                    self.vault.mark_reference_deleted(p)
                except Exception as e:
                    self._set_error(f"mark_ref_error: {e}")
                    pass
                self._pending_deletes.pop(pstr, None)
            except Exception:
                # Best-effort
                try:
                    self._pending_deletes.pop(pstr, None)
                except Exception:
                    pass

    def _maybe_flush_pending_mods(self):
        if not self._pending_mods:
            return
        now = time.time()
        ready = []
        for pstr, rec in list(self._pending_mods.items()):
            if now - rec.get("when", 0) >= self._mod_coalesce_secs:
                ready.append((pstr, rec.get("event_type", "modified")))
        for pstr, etype in ready:
            try:
                p = Path(pstr)
                # Skip if file should be ignored
                if self._should_ignore_by_rules(p):
                    self._pending_mods.pop(pstr, None)
                    continue
                # Only log if still exists and is file
                if p.exists() and p.is_file():
                    try:
                        self.vault.log_file_operation(etype, p, tracked_files_count=self._get_tracked_files_count())
                        self._bump_beat()
                    except Exception as e:
                        self._set_error(f"ops_log_error: {e}")
                    self._update_monitor_db(p)
                self._pending_mods.pop(pstr, None)
            except Exception:
                try:
                    self._pending_mods.pop(pstr, None)
                except Exception:
                    pass

    def _get_db_activity_info(self, now: float) -> tuple[Optional[str], Optional[str], Optional[str], int]:
        """Get DB activity information from summary and history.

        Returns:
            Tuple of (last_operation, last_detail, last_iso, ops_last_minute)
        """
        db_summary = load_db_activity_summary()
        db_history_window = max(int(self.fs_rate_long_window), 600)
        db_history = load_db_activity_history(db_history_window)

        db_last_operation = None
        db_last_detail = None
        db_last_iso = None

        if db_summary:
            try:
                db_last_iso = db_summary.get("timestamp_iso") or None
                db_last_operation = db_summary.get("operation") or None
                db_last_detail = db_summary.get("detail") or None
            except Exception:
                pass

        if db_last_operation is None and db_history:
            try:
                db_last_iso = db_history[-1].get("timestamp_iso")
                db_last_operation = db_history[-1].get("operation")
                db_last_detail = db_history[-1].get("detail")
            except Exception:
                pass

        cutoff_minute = now - 60.0
        db_ops_last_minute = 0
        for item in db_history:
            try:
                ts_val = float(item.get("timestamp", 0))
            except Exception:
                continue
            if ts_val >= cutoff_minute:
                db_ops_last_minute += 1

        self._beat_count = len(db_history)
        return db_last_operation, db_last_detail, db_last_iso, db_ops_last_minute

    def _calculate_fs_rates(self) -> tuple[float, float, float]:
        """Calculate filesystem event rates.

        Returns:
            Tuple of (short_rate, long_rate, weighted_rate)
        """
        short_rate = len(self._fs_events_short) / self.fs_rate_short_window if self.fs_rate_short_window else 0.0
        long_rate = len(self._fs_events_long) / self.fs_rate_long_window if self.fs_rate_long_window else 0.0
        weighted_rate = (
            self.fs_rate_short_weight * short_rate
            + self.fs_rate_long_weight * long_rate
        )
        return short_rate, long_rate, weighted_rate

    def _get_lock_info(self) -> tuple[bool, Optional[int], str]:
        """Get lock file information.

        Returns:
            Tuple of (lock_present, lock_pid, lock_path)
        """
        lock_present = bool(self.lock_file.exists())
        lock_pid = None
        if lock_present:
            try:
                content = self.lock_file.read_text().strip()
                if content:
                    lock_pid = int(content.splitlines()[0])
            except Exception:
                pass
        return lock_present, lock_pid, str(self.lock_file)

    def _format_uptime(self, secs: int) -> str:
        """Format uptime seconds as HH:MM:SS."""
        h = secs // 3600
        m = (secs % 3600) // 60
        s = secs % 60
        return f"{h:02d}:{m:02d}:{s:02d}"

    def _write_health(self):
        """Write health data to health.json file."""
        try:
            now = time.time()
            uptime_secs = int(now - self._health_started_at)

            db_last_operation, db_last_detail, db_last_iso, db_ops_last_minute = self._get_db_activity_info(now)
            db_ops_per_min = round(db_ops_last_minute / 1.0, 2)

            short_rate, long_rate, weighted_rate = self._calculate_fs_rates()
            lock_present, lock_pid, lock_path = self._get_lock_info()

            health_data = HealthData(
                pending_deletes=len(self._pending_deletes),
                pending_mods=len(self._pending_mods),
                last_error=self._last_error,
                pid=os.getpid(),
                last_error_at=int(self._last_error_at) if self._last_error_at else None,
                last_error_at_iso=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self._last_error_at)) if self._last_error_at else None,
                last_error_age_secs=int(now - self._last_error_at) if self._last_error_at else None,
                started_at=int(self._health_started_at),
                started_at_iso=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self._health_started_at)),
                uptime_secs=uptime_secs,
                uptime_hms=self._format_uptime(uptime_secs),
                beats=int(self._beat_count),
                avg_beats_per_min=db_ops_per_min,
                lock_present=lock_present,
                lock_pid=lock_pid,
                lock_path=lock_path,
                db_last_operation=db_last_operation,
                db_last_operation_detail=db_last_detail,
                db_last_operation_iso=db_last_iso,
                db_ops_last_minute=db_ops_last_minute,
                fs_rate_short=short_rate,
                fs_rate_long=long_rate,
                fs_rate_weighted=weighted_rate,
                fs_rate_short_window_secs=self.fs_rate_short_window,
                fs_rate_long_window_secs=self.fs_rate_long_window,
                fs_rate_short_weight=self.fs_rate_short_weight,
                fs_rate_long_weight=self.fs_rate_long_weight,
            )

            self.health_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.health_file, 'w') as f:
                json.dump(health_data.to_dict(), f)

            # Update Health landing page
            try:
                self.vault.write_health_page()
            except Exception:
                pass
        except Exception:
            pass

    def _bump_beat(self):
        try:
            self._beat_count += 1
        except Exception:
            pass

    def _set_error(self, msg: str):
        try:
            self._last_error = str(msg)
            self._last_error_at = time.time()
        except Exception:
            pass

    def _acquire_lock(self):
        """Acquire an exclusive file lock to ensure a single daemon instance."""
        # Ensure directory exists
        self.lock_file.parent.mkdir(parents=True, exist_ok=True)
        # Stale lock auto-clean: if lock exists but PID is not running, remove it
        try:
            if self.lock_file.exists():
                try:
                    raw = self.lock_file.read_text().strip().splitlines()
                    stale_pid = int(raw[0]) if raw else None
                except Exception:
                    stale_pid = None
                if stale_pid and not self._pid_running(stale_pid):
                    try:
                        self.lock_file.unlink()
                    except Exception:
                        pass
        except Exception:
            pass
        # If fcntl not available, fall back to a coarse PID file check
        if fcntl is None:
            if self.lock_file.exists():
                # Read PID and check if running
                try:
                    pid = int(self.lock_file.read_text().strip().splitlines()[0])
                except Exception:
                    pid = None
                if pid and pid > 0 and self._pid_running(pid):
                    raise RuntimeError(f"Another WKS daemon is already running (PID {pid}).")
            # Write current PID
            self.lock_file.write_text(str(os.getpid()))
            return
        # POSIX advisory lock
        try:
            self._lock_fh = open(self.lock_file, 'w')
            fcntl.flock(self._lock_fh.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            # Write PID and timestamp
            self._lock_fh.seek(0)
            self._lock_fh.truncate()
            self._lock_fh.write(f"{os.getpid()}\n")
            self._lock_fh.flush()
        except BlockingIOError:
            # Another process holds the lock
            raise RuntimeError("Another WKS daemon instance is already running.")
        except Exception as e:
            raise RuntimeError(f"Failed to acquire daemon lock: {e}")

    def _release_lock(self):
        """Release the single-instance lock."""
        try:
            if self._lock_fh and fcntl is not None:
                fcntl.flock(self._lock_fh.fileno(), fcntl.LOCK_UN)
                self._lock_fh.close()
                self._lock_fh = None
            # Best-effort cleanup
            if self.lock_file.exists():
                try:
                    self.lock_file.unlink()
                except Exception:
                    pass
        except Exception:
            pass

    def _pid_running(self, pid: int) -> bool:
        try:
            os.kill(pid, 0)
            return True
        except Exception:
            return False

    def _get_tracked_files_count(self) -> int:
        """Return number of unique files tracked in monitor DB."""
        if self.monitor_collection is not None:
            try:
                return self.monitor_collection.count_documents({})
            except Exception:
                return 0
        return 0


if __name__ == "__main__":
    import sys
    from pymongo import MongoClient

    # Load and validate config
    config = load_config()
    try:
        validate_and_raise(config)
    except ConfigValidationError as e:
        print(str(e))
        raise SystemExit(2)

    # Vault config (new structure)
    vault_cfg = config.get("vault")
    if not vault_cfg:
        raise ValueError("vault section is required in config (found: missing, expected: vault section with base_dir, wks_dir, etc.)")
    vault_path_str = vault_cfg.get("base_dir")
    if not vault_path_str:
        raise ValueError("vault.base_dir is required in config (found: missing, expected: path to Obsidian vault directory)")
    vault_path = expand_path(vault_path_str)
    base_dir = vault_cfg.get("wks_dir")
    if not base_dir:
        raise ValueError("vault.wks_dir is required in config (found: missing, expected: subdirectory name within vault, e.g., 'WKS')")

    # Monitor config
    monitor_cfg = config.get("monitor")
    if not monitor_cfg:
        raise ValueError("monitor section is required in config (found: missing, expected: monitor section with include_paths, exclude_paths, etc.)")
    include_paths_list = monitor_cfg.get("include_paths")
    if not include_paths_list:
        raise ValueError("monitor.include_paths is required in config (found: missing, expected: list of directory paths to monitor)")
    include_paths = [expand_path(p) for p in include_paths_list]
    exclude_paths_list = monitor_cfg.get("exclude_paths")
    if exclude_paths_list is None:
        raise ValueError("monitor.exclude_paths is required in config (found: missing, expected: list of directory paths to exclude, can be empty [])")
    exclude_paths = [expand_path(p) for p in exclude_paths_list]
    ignore_dirnames_list = monitor_cfg.get("ignore_dirnames")
    if ignore_dirnames_list is None:
        raise ValueError("monitor.ignore_dirnames is required in config (found: missing, expected: list of directory names to ignore, can be empty [])")
    ignore_dirnames = set(ignore_dirnames_list)
    ignore_patterns = set()  # deprecated
    ignore_globs_list = monitor_cfg.get("ignore_globs")
    if ignore_globs_list is None:
        raise ValueError("monitor.ignore_globs is required in config (found: missing, expected: list of glob patterns to ignore, can be empty [])")
    ignore_globs = list(ignore_globs_list)

    # DB config
    db_cfg = config.get("db")
    if not db_cfg:
        raise ValueError("db section is required in config (found: missing, expected: db section with type and uri)")
    mongo_uri = db_cfg.get("uri")
    if not mongo_uri:
        raise ValueError("db.uri is required in config (found: missing, expected: MongoDB connection URI string)")
    mongo_uri = str(mongo_uri)
    ensure_mongo_running(mongo_uri, record_start=True)

    client = MongoClient(mongo_uri)
    monitor_db_key = monitor_cfg.get("database")
    if not monitor_db_key:
        raise ValueError("monitor.database is required in config (found: missing, expected: 'database.collection' format, e.g., 'wks.monitor')")
    if "." not in monitor_db_key:
        raise ValueError(f"monitor.database must be in format 'database.collection' (found: {monitor_db_key!r}, expected: format like 'wks.monitor')")
    parts = monitor_db_key.split(".", 1)
    if len(parts) != 2 or not parts[0] or not parts[1]:
        raise ValueError(f"monitor.database must be in format 'database.collection' (found: {monitor_db_key!r}, expected: format like 'wks.monitor' with both parts non-empty)")
    monitor_db_name, monitor_coll_name = parts
    monitor_collection = client[monitor_db_name][monitor_coll_name]

    # Vault settings from vault section (SPEC: vault.type="obsidian")
    # Simplified: only base_dir, wks_dir, update_frequency_seconds, type, database
    obsidian_log_max_entries = 500  # Default, not in vault section
    obsidian_active_files_max_rows = 100  # Default, not in vault section
    obsidian_source_max_chars = 40  # Default, not in vault section
    obsidian_destination_max_chars = 40  # Default, not in vault section
    obsidian_docs_keep = 50  # Default, not in vault section
    auto_project_notes = False  # Default, not in vault section

    daemon = WKSDaemon(
        config=config,
        vault_path=vault_path,
        base_dir=base_dir,
        obsidian_log_max_entries=obsidian_log_max_entries,
        obsidian_active_files_max_rows=obsidian_active_files_max_rows,
        obsidian_source_max_chars=obsidian_source_max_chars,
        obsidian_destination_max_chars=obsidian_destination_max_chars,
        obsidian_docs_keep=obsidian_docs_keep,
        auto_project_notes=auto_project_notes,
        monitor_paths=include_paths,
        ignore_dirnames=ignore_dirnames,
        exclude_paths=exclude_paths,
        ignore_patterns=ignore_patterns,
        ignore_globs=ignore_globs,
        mongo_uri=mongo_uri,
        monitor_collection=monitor_collection,
    )

    print("Starting WKS daemon...")
    print("Press Ctrl+C to stop")

    try:
        daemon.run()
    except KeyboardInterrupt:
        print("\nStopping...")
        sys.exit(0)
</file>

<file path="wks/cli.py">
"""
WKS command-line interface.

This module maintains backward compatibility by re-exporting from the new modular structure.
The actual implementation is in wks.cli.main and wks.cli.commands.*
"""

from .cli.main import main
from .cli.commands.config import show_config
from .config import load_config

# Re-export for backward compatibility
__all__ = ["main", "show_config", "load_config"]
</file>

</files>
